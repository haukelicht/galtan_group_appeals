---
title: "Group appeal PRRPs and Greens"
format: html
editor: visual
Authors: Hauke Licht / Leonce Röth
---

Load packages

```{r}
#| echo: false
# load pkgs 
library(dplyr)
library(tidyr)
library(purrr)
library(stringi)
library(stringr)
library(officer)
library(quarto)

```

Set directories

```{r}
#| echo: false
data_path <- "C:/Users/wmb917/Dropbox/Group appeal PPPR and Greens/data/"
manifestos_path <- file.path(data_path, "manifestos/annotated/")
```

Reading the data

```{r}
#| echo: false
docx_files <- list.files(manifestos_path, pattern = "^[^~].+\\.docx$", full.names = TRUE)

docs <- map(docx_files, read_docx)

doc <- docs[[1]]

cont <- as_tibble(docx_summary(doc))

table(unlist(str_extract_all(cont$text, "\\[[^s:\\]]+")))
```

Cleaning the text and sentence splitting

```{r}
#| echo: false
s <- grep("^\\[Here does the program start", cont$text)

cont <- cont[(s+1):nrow(cont), ]

cont <- cont[-grep("^\\h*$", cont$text), ]

(par <- cont$text[8])

sents <- stri_split_boundaries(par, type = "sentence", locale = "en_EN")[[1]]

(sent <- sents[1])
```

Parsing

```{r}

parse_sentence <- function(sent) {
  # # test
  # sent <- "abcd [s]def [label]ghij [s]klmn [label]nop"
  # # idx:   1       4 6        7       11 14
  
  if (!grepl("[s]", sent, fixed = TRUE))
    return(tibble(text = sent, annotations = list(NULL)))
  
  # identify starts of spans
  s <- str_locate_all(sent, "\\[s\\]")[[1]][ ,"start"]
  
  # segment sentence into subsets with spans at beginning
  chars <- strsplit(sent, "")[[1]]
  segs <- map2(c(1, s), c(s-1, nchar(sent)), ~paste(chars[.x:.y], collapse = ""))
  
  # iterate over segments
  segs <- map_dfr(segs, function(seg) { # seg <- segs[[2]]
    
    # remove the span start special token
    seg <- str_remove(seg, "^\\[s\\]\\h*")
    
    # extract the annotation note
    label <- str_extract(seg, "\\[[^\\]]+\\]") # test: should be "[label]"
    
    # get the extend of the annotation note in the text
    ext <- str_locate(seg, "\\h+\\[[^\\]]+\\]")
    
    # compute the span end
    e <- unname(ext[1, "start"]-1) # test: should be 3
    (span <- substr(seg, 1, e))
    
    # remove the annotation note
    seg <- str_remove(seg, "\\h*\\[[^\\]]+\\]")
    
    # return the relevant data
    tibble(text = seg, s = 0, e, label, span)
  })
  
  
  
  # compute the number of characters in segments
  segs$nchar <- c(0, nchar(segs$text[-nrow(segs)]))
  
  # increment span starts by previous segment lengths
  segs$s <- segs$s + segs$nchar
  segs$s <- cumsum(segs$s)+1L
  # increment span ends by previous segment lengths
  segs$e <- segs$s+segs$e-1L
  
  # extract annotations
  annotations <- segs[!is.na(segs$label), c("s", "e", "label", "span")]
  # test: should be
  # # A tibble: 2 × 3
  #       s     e label     
  #   <dbl> <dbl> <chr>     
  # 1     4     6 " [label]"
  # 2    11    14 " [label]"
  
  # return
  return(
    tibble(
      text = paste(segs$text, collapse = "") # test: should be "abcdefghijklmnop"
      , annotations = list(annotations)
    )
  )
}

parse_paragraph <- function(par) {
  sents <- stri_split_boundaries(par, type = "sentence", locale = "en_EN")[[1]]
  out <- map_dfr(sents, parse_sentence, .id = "sentence_nr")
  out$sentence_nr <- as.integer(out$sentence_nr)
  return(out)
}

# View(map_dfr(cont$text[1:10], parse_paragraph, .id = "par_nr"))

```

``` python
```

```{r}
library(reticulate)
  
  # Create a new environment
  version <- "3.11.4"

  virtualenv_create("my-python", python_version = version)
 
  virtualenv_install(envname = "my-python", "nltk", ignore_istalled = FALSE, 
  pip_options = character()  )
    
  virtualenv_install(envname = "my-python", "python-docx", ignore_istalled =
    FALSE, pip_options = character()  )
```

```{r}
library(reticulate)
# Path to your Python script
python_script <- "C:/Users/wmb917/Dropbox/Group appeal PPPR and Greens/code/preproc/splitting.py"

# Call the Python script
reticulate::source_python(python_script)

# Call the Python function to split the DOCX document
file_path <- "C:/Users/wmb917/Dropbox/Group appeal PPPR and Greens/data/manifestos/annotated/Manifesto 1992 Greater Romania Party - Romania_Leonce_corrected.docx"
text <- read_docx(file_path)
sentences <- split_into_sentences(text)

# Print the sentences (or perform any further processing)
for (sentence in sentences) {
  print(sentence)
}


```

```{python}

```

```{python}


import os
import docx 
from nltk.tokenize import sent_tokenize

def read_docx(file_path):
    doc = docx.Document(file_path)
    full_text = []
    for para in doc.paragraphs:
        full_text.append(para.text)
    return "\n".join(full_text)

def split_into_sentences(text):
    sentences = sent_tokenize(text)
    return sentences

def main():
    # Replace with the path to your DOCX file
    file_path = "C:\Users\wmb917\Dropbox\Group appeal PPPR and Greens\data\manifestos\annotated\Manifesto 1992 Greater Romania Party - Romania_Leonce_corrected.docx"

    # Read the DOCX file and extract the text
    text = read_docx(file_path)

    # Split the text into sentences
    sentences = split_into_sentences(text)

    # Print the sentences (or perform any further processing)
    for sentence in sentences:
        print(sentence)

if __name__ == "__main__":
    main()


```
