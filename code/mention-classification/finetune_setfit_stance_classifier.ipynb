{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "552ec552",
   "metadata": {},
   "source": [
    "# Social group mention stance classification\n",
    "\n",
    "\n",
    "In this notebook, we fine-tune a pre-trained sentence transformer model for multilabel classifiers using the `setfit` library to categorize into which attribute dimensions social group mentions belong."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af7f258-5aaf-47c2-b81e-2f10fc349812",
   "metadata": {},
   "source": [
    "notebook based on https://github.com/huggingface/setfit/blob/main/notebooks/text-classification_multilabel.ipynb\n",
    "\n",
    "See also:\n",
    "\n",
    "- https://huggingface.co/docs/setfit/en/how_to/multilabel\n",
    "- https://github.com/huggingface/setfit/issues/413#issuecomment-1697751329"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5604f73-f395-42cb-8082-9974a87ef9e9",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50cfb2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../code/mention-classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70a859ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hauke-licht/miniforge/envs/galtan_group_appeals/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import regex\n",
    "\n",
    "import torch\n",
    "import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from utils.setfit import get_class_weights, model_init, TrainerForSpanClassification\n",
    "\n",
    "from transformers import AutoTokenizer, set_seed\n",
    "from setfit import TrainingArguments, Trainer\n",
    "\n",
    "from utils.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0cce1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas\n",
    "# numpy\n",
    "# regex\n",
    "# torch\n",
    "# accelerate\n",
    "# tokenizers\n",
    "# sentencepiece\n",
    "# datasets\n",
    "# transformers\n",
    "# setfit[absa]\n",
    "# scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "101e20b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c003a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '../../models'\n",
    "# base_model = os.path.join(model_path, 'paraphrase-mpnet-base-v2-social-group-mention-attributes-embedding')\n",
    "base_model = \"sentence-transformers/paraphrase-mpnet-base-v2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e756be8-3b60-4c86-aa1b-7ef78289b8e2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0095eab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../../data/annotations/group_mention_categorization'\n",
    "fp = os.path.join(data_path, 'consolidated_annotations.tsv')\n",
    "df = pd.read_csv(fp, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d39cc7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = df[df.attribute==\"stance\"]\n",
    "tmp = tmp[['mention_id', 'text', 'mention', 'label']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40974d39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "Positive    242\n",
       "Negative     34\n",
       "Neutral      23\n",
       "Unsure        1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp['label'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d77e4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = tmp[tmp.label != 'Unsure']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57d90a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp = tmp[tmp.label != 'Neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "323e7173",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = dict(enumerate(tmp.label.unique()))\n",
    "label2id = {l: i for i, l in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74f48f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp.loc[:,'labels'] = tmp.label.map(label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb0fddde",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a609fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using concat strategy\n",
    "tmp['input'] = tmp.text + tokenizer.sep_token + tmp.mention \n",
    "max_length_ = max(tokenizer(tmp.input.to_list(), truncation=False, padding=False, return_length=True).length)\n",
    "cols = ['input', 'labels']\n",
    "cols_mapping = {\"input\": \"text\", \"labels\": \"label\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be40617",
   "metadata": {},
   "source": [
    "### split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c11471c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trn, tst = train_test_split(range(len(tmp)), test_size=0.25, random_state=SEED, stratify=tmp.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a83c9560",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_train = tmp.iloc[trn][cols]\n",
    "# print(tmp_train.labels.value_counts(dropna=False))\n",
    "# # downsample the training set\n",
    "# tmp_train = tmp_train.groupby('labels').sample(50, random_state=SEED, replace=True).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa4c4839",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.DatasetDict({\n",
    "    'train': datasets.Dataset.from_pandas(tmp_train, preserve_index=False),\n",
    "    'test': datasets.Dataset.from_pandas(tmp.iloc[tst][cols], preserve_index=False)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e43845",
   "metadata": {},
   "source": [
    "## Prepare setfit fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7d3c791",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0537386 , 0.57215805, 0.37410334])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = np.array(dataset['train']['labels'])\n",
    "class_weights = get_class_weights(y_train)\n",
    "class_weights = class_weights.astype(float)\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9e971bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'social-group-mention-stance-classifier'\n",
    "model_dir = os.path.join(model_path, model_id)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_dir,\n",
    "    batch_size=(32, 8),\n",
    "    max_length=max_length_,\n",
    "    num_epochs=(1, 15),\n",
    "    max_steps=50,\n",
    "    end_to_end=True,\n",
    "    # loss=CosineSimilarityLoss,\n",
    "    # samples_per_label=2,\n",
    "    # use_amp=True,\n",
    "    #report_to='none',\n",
    "    eval_strategy='steps',\n",
    "    eval_steps=25,\n",
    "    #eval_strategy = 'epoch',\n",
    "    #save_strategy = 'epoch',\n",
    "    #save_total_limit=2,\n",
    "    #load_best_model_at_end=True,\n",
    "    ## metric_for_best_model='balanced_accuracy',\n",
    "    seed=SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9d22a4b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying column mapping to the training dataset\n",
      "Applying column mapping to the evaluation dataset\n",
      "Map: 100%|██████████| 224/224 [00:00<00:00, 31068.92 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from utils.metrics import compute_metrics_multiclass\n",
    "\n",
    "# trainer = TrainerForSpanClassification(\n",
    "trainer = Trainer(\n",
    "    model_init=lambda: model_init(\n",
    "        model_name=base_model,\n",
    "        id2label=id2label,\n",
    "        # multitarget_strategy='one-vs-rest',\n",
    "        class_weights=class_weights,\n",
    "        use_span_embedding=False,#True,\n",
    "        # device='mps'\n",
    "    ),\n",
    "    metric=lambda p, t: compute_metrics_multiclass(p, t, id2label),\n",
    "    args=training_args,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['test'],\n",
    "    column_mapping=cols_mapping\n",
    ")\n",
    "\n",
    "# for deterministic results\n",
    "trainer._args.seed = SEED\n",
    "trainer.st_trainer.args.seed = SEED\n",
    "trainer.st_trainer.args.data_seed = SEED\n",
    "trainer.st_trainer.args.full_determinism = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c06bd9",
   "metadata": {},
   "source": [
    "### Fine-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8ad7b804",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num unique pairs = 1600\n",
      "  Batch size = 32\n",
      "  Num epochs = 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:13, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.298500</td>\n",
       "      <td>0.236249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.208400</td>\n",
       "      <td>0.217134</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 15/15 [00:15<00:00,  1.03s/it]               \n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf4519f",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e8f56430",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running evaluation *****\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8533333333333334,\n",
       " 'balanced_accuracy': 0.5669398907103825,\n",
       " 'f1_macro': 0.5544619422572179,\n",
       " 'precision_macro': 0.5429292929292929,\n",
       " 'recall_macro': 0.5669398907103825,\n",
       " 'precision_Positive': 0.8787878787878788,\n",
       " 'recall_Positive': 0.9508196721311475,\n",
       " 'f1_Positive': 0.9133858267716536,\n",
       " 'precision_Neutral': 0.0,\n",
       " 'recall_Neutral': 0.0,\n",
       " 'f1_Neutral': 0.0,\n",
       " 'precision_Negative': 0.75,\n",
       " 'recall_Negative': 0.75,\n",
       " 'f1_Negative': 0.75}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = trainer.evaluate()\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a60370",
   "metadata": {},
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0a055f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# shutil.rmtree(model_dir)\n",
    "# trainer.model.save_pretrained(model_dir)\n",
    "# tokenizer.save_pretrained(model_dir)\n",
    "# trainer.model.to('cpu');\n",
    "# del trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9709c782",
   "metadata": {},
   "source": [
    "## Fine-tune with setfitABSA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877a3a01",
   "metadata": {},
   "source": [
    "### Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0df948ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp[['manifesto_id', 'sentence_nr', 'mention_nr']] = tmp.mention_id.str.split('-', expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b91b7d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex\n",
    "tmp['span'] = tmp.apply(lambda x: regex.search(regex.escape(x.mention), x.text).span(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4bca77ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_train = tmp.iloc[trn]\n",
    "tmp_test = tmp.iloc[tst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8ef7f898",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_666936/2278076560.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tmp_train.loc[:, 'ordinal'] = tmp_train.groupby(['manifesto_id', 'sentence_nr', 'mention']).cumcount()\n",
      "/tmp/ipykernel_666936/2278076560.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tmp_test.loc[:, 'ordinal'] = tmp_test.groupby(['manifesto_id', 'sentence_nr', 'mention']).cumcount()\n"
     ]
    }
   ],
   "source": [
    "# rank spans within sentence\n",
    "tmp_train.loc[:, 'ordinal'] = tmp_train.groupby(['manifesto_id', 'sentence_nr', 'mention']).cumcount() \n",
    "tmp_test.loc[:, 'ordinal'] = tmp_test.groupby(['manifesto_id', 'sentence_nr', 'mention']).cumcount() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0f071dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['text', 'mention', 'label', 'ordinal']\n",
    "dataset = datasets.DatasetDict({\n",
    "    'train': datasets.Dataset.from_pandas(tmp_train[cols], preserve_index=False),\n",
    "    'test': datasets.Dataset.from_pandas(tmp_test[cols], preserve_index=False)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "900dd88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy==3.8.5\n",
    "# !python -m spacy download en_core_web_lg==3.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3e7dde21",
   "metadata": {},
   "outputs": [],
   "source": [
    "del trainer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "68310fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n",
      "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from setfit import AbsaTrainer, TrainingArguments, AbsaModel\n",
    "\n",
    "model = AbsaModel.from_pretrained(\n",
    "    \"sentence-transformers/paraphrase-mpnet-base-v2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d771a2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInit signature:\u001b[39m\n",
      "TrainingArguments(\n",
      "    output_dir: \u001b[33m'str'\u001b[39m = \u001b[33m'checkpoints'\u001b[39m,\n",
      "    batch_size: \u001b[33m'Union[int, Tuple[int, int]]'\u001b[39m = (\u001b[32m16\u001b[39m, \u001b[32m2\u001b[39m),\n",
      "    num_epochs: \u001b[33m'Union[int, Tuple[int, int]]'\u001b[39m = (\u001b[32m1\u001b[39m, \u001b[32m16\u001b[39m),\n",
      "    max_steps: \u001b[33m'int'\u001b[39m = -\u001b[32m1\u001b[39m,\n",
      "    sampling_strategy: \u001b[33m'str'\u001b[39m = \u001b[33m'oversampling'\u001b[39m,\n",
      "    num_iterations: \u001b[33m'Optional[int]'\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    body_learning_rate: \u001b[33m'Union[float, Tuple[float, float]]'\u001b[39m = (\u001b[32m2e-05\u001b[39m, \u001b[32m1e-05\u001b[39m),\n",
      "    head_learning_rate: \u001b[33m'float'\u001b[39m = \u001b[32m0.01\u001b[39m,\n",
      "    loss: \u001b[33m'Callable'\u001b[39m = <\u001b[38;5;28;01mclass\u001b[39;00m \u001b[33m'sentence_transformers.losses.CosineSimilarityLoss.CosineSimilarityLoss'\u001b[39m>,\n",
      "    distance_metric: \u001b[33m'Callable'\u001b[39m = <function BatchHardTripletLossDistanceFunction.cosine_distance at \u001b[32m0x7c571754c860\u001b[39m>,\n",
      "    margin: \u001b[33m'float'\u001b[39m = \u001b[32m0.25\u001b[39m,\n",
      "    end_to_end: \u001b[33m'bool'\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
      "    use_amp: \u001b[33m'bool'\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
      "    warmup_proportion: \u001b[33m'float'\u001b[39m = \u001b[32m0.1\u001b[39m,\n",
      "    l2_weight: \u001b[33m'Optional[float]'\u001b[39m = \u001b[32m0.01\u001b[39m,\n",
      "    max_length: \u001b[33m'Optional[int]'\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    samples_per_label: \u001b[33m'int'\u001b[39m = \u001b[32m2\u001b[39m,\n",
      "    show_progress_bar: \u001b[33m'bool'\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m,\n",
      "    seed: \u001b[33m'int'\u001b[39m = \u001b[32m42\u001b[39m,\n",
      "    report_to: \u001b[33m'str'\u001b[39m = \u001b[33m'all'\u001b[39m,\n",
      "    run_name: \u001b[33m'Optional[str]'\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    logging_dir: \u001b[33m'Optional[str]'\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    logging_strategy: \u001b[33m'str'\u001b[39m = \u001b[33m'steps'\u001b[39m,\n",
      "    logging_first_step: \u001b[33m'bool'\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m,\n",
      "    logging_steps: \u001b[33m'int'\u001b[39m = \u001b[32m50\u001b[39m,\n",
      "    eval_strategy: \u001b[33m'str'\u001b[39m = \u001b[33m'no'\u001b[39m,\n",
      "    evaluation_strategy: \u001b[33m'Optional[str]'\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    eval_steps: \u001b[33m'Optional[int]'\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    eval_delay: \u001b[33m'int'\u001b[39m = \u001b[32m0\u001b[39m,\n",
      "    eval_max_steps: \u001b[33m'int'\u001b[39m = -\u001b[32m1\u001b[39m,\n",
      "    save_strategy: \u001b[33m'str'\u001b[39m = \u001b[33m'steps'\u001b[39m,\n",
      "    save_steps: \u001b[33m'int'\u001b[39m = \u001b[32m500\u001b[39m,\n",
      "    save_total_limit: \u001b[33m'Optional[int]'\u001b[39m = \u001b[32m1\u001b[39m,\n",
      "    load_best_model_at_end: \u001b[33m'bool'\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
      "    metric_for_best_model: \u001b[33m'Optional[str]'\u001b[39m = \u001b[33m'embedding_loss'\u001b[39m,\n",
      "    greater_is_better: \u001b[33m'bool'\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
      ") -> \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mDocstring:\u001b[39m     \n",
      "TrainingArguments is the subset of the arguments which relate to the training loop itself.\n",
      "Note that training with SetFit consists of two phases behind the scenes: **finetuning embeddings** and\n",
      "**training a classification head**. As a result, some of the training arguments can be tuples,\n",
      "where the two values are used for each of the two phases, respectively. The second value is often only\n",
      "used when training the model was loaded using `use_differentiable_head=True`.\n",
      "\n",
      "Parameters:\n",
      "    output_dir (`str`, defaults to `\"checkpoints\"`):\n",
      "        The output directory where the model predictions and checkpoints will be written.\n",
      "    batch_size (`Union[int, Tuple[int, int]]`, defaults to `(16, 2)`):\n",
      "        Set the batch sizes for the embedding and classifier training phases respectively,\n",
      "        or set both if an integer is provided.\n",
      "        Note that the batch size for the classifier is only used with a differentiable PyTorch head.\n",
      "    num_epochs (`Union[int, Tuple[int, int]]`, defaults to `(1, 16)`):\n",
      "        Set the number of epochs the embedding and classifier training phases respectively,\n",
      "        or set both if an integer is provided.\n",
      "        Note that the number of epochs for the classifier is only used with a differentiable PyTorch head.\n",
      "    max_steps (`int`, defaults to `-1`):\n",
      "        If set to a positive number, the total number of training steps to perform. Overrides `num_epochs`.\n",
      "        The training may stop before reaching the set number of steps when all data is exhausted.\n",
      "    sampling_strategy (`str`, defaults to `\"oversampling\"`):\n",
      "        The sampling strategy of how to draw pairs in training. Possible values are:\n",
      "\n",
      "            - `\"oversampling\"`: Draws even number of positive/ negative sentence pairs until every\n",
      "                sentence pair has been drawn.\n",
      "            - `\"undersampling\"`: Draws the minimum number of positive/ negative sentence pairs until\n",
      "                every sentence pair in the minority class has been drawn.\n",
      "            - `\"unique\"`: Draws every sentence pair combination (likely resulting in unbalanced\n",
      "                number of positive/ negative sentence pairs).\n",
      "\n",
      "        The default is set to `\"oversampling\"`, ensuring all sentence pairs are drawn at least once.\n",
      "        Alternatively, setting `num_iterations` will override this argument and determine the number\n",
      "        of generated sentence pairs.\n",
      "    num_iterations (`int`, *optional*):\n",
      "        If not set the `sampling_strategy` will determine the number of sentence pairs to generate.\n",
      "        This argument sets the number of iterations to generate sentence pairs for\n",
      "        and provides compatability with Setfit <v1.0.0.\n",
      "        This argument is ignored if triplet loss is used.\n",
      "        It is only used in conjunction with `CosineSimilarityLoss`.\n",
      "    body_learning_rate (`Union[float, Tuple[float, float]]`, defaults to `(2e-5, 1e-5)`):\n",
      "        Set the learning rate for the `SentenceTransformer` body for the embedding and classifier\n",
      "        training phases respectively, or set both if a float is provided.\n",
      "        Note that the body learning rate for the classifier is only used with a differentiable PyTorch\n",
      "        head *and* if `end_to_end=True`.\n",
      "    head_learning_rate (`float`, defaults to `1e-2`):\n",
      "        Set the learning rate for the head for the classifier training phase. Only used with a\n",
      "        differentiable PyTorch head.\n",
      "    loss (`nn.Module`, defaults to `CosineSimilarityLoss`):\n",
      "        The loss function to use for contrastive training of the embedding training phase.\n",
      "    distance_metric (`Callable`, defaults to `BatchHardTripletLossDistanceFunction.cosine_distance`):\n",
      "        Function that returns a distance between two embeddings.\n",
      "        It is set for the triplet loss and ignored for `CosineSimilarityLoss` and `SupConLoss`.\n",
      "    margin (`float`, defaults to `0.25`):\n",
      "        Margin for the triplet loss.\n",
      "        Negative samples should be at least margin further apart from the anchor than the positive.\n",
      "        It is ignored for `CosineSimilarityLoss`, `BatchHardSoftMarginTripletLoss` and `SupConLoss`.\n",
      "    end_to_end (`bool`, defaults to `False`):\n",
      "        If True, train the entire model end-to-end during the classifier training phase.\n",
      "        Otherwise, freeze the `SentenceTransformer` body and only train the head.\n",
      "        Only used with a differentiable PyTorch head.\n",
      "    use_amp (`bool`, defaults to `False`):\n",
      "        Whether to use Automatic Mixed Precision (AMP) during the embedding training phase.\n",
      "        Only for Pytorch >= 1.6.0\n",
      "    warmup_proportion (`float`, defaults to `0.1`):\n",
      "        Proportion of the warmup in the total training steps.\n",
      "        Must be greater than or equal to 0.0 and less than or equal to 1.0.\n",
      "    l2_weight (`float`, *optional*):\n",
      "        Optional l2 weight for both the model body and head, passed to the `AdamW` optimizer in the\n",
      "        classifier training phase if a differentiable PyTorch head is used.\n",
      "    max_length (`int`, *optional*):\n",
      "        The maximum token length a tokenizer can generate. If not provided, the maximum length for\n",
      "        the `SentenceTransformer` body is used.\n",
      "    samples_per_label (`int`, defaults to `2`): Number of consecutive, random and unique samples drawn per label.\n",
      "        This is only relevant for triplet loss and ignored for `CosineSimilarityLoss`.\n",
      "        Batch size should be a multiple of samples_per_label.\n",
      "    show_progress_bar (`bool`, defaults to `True`):\n",
      "        Whether to display a progress bar for the training epochs and iterations.\n",
      "    seed (`int`, defaults to `42`):\n",
      "        Random seed that will be set at the beginning of training. To ensure reproducibility across\n",
      "        runs, use the `model_init` argument to [`Trainer`] to instantiate the model if it has some\n",
      "        randomly initialized parameters.\n",
      "    report_to (`str` or `List[str]`, *optional*, defaults to `\"all\"`):\n",
      "        The list of integrations to report the results and logs to. Supported platforms are `\"azure_ml\"`,\n",
      "        `\"comet_ml\"`, `\"mlflow\"`, `\"neptune\"`, `\"tensorboard\"`,`\"clearml\"` and `\"wandb\"`. Use `\"all\"` to report to\n",
      "        all integrations installed, `\"none\"` for no integrations.\n",
      "    run_name (`str`, *optional*):\n",
      "        A descriptor for the run. Typically used for [wandb](https://www.wandb.com/) and\n",
      "        [mlflow](https://www.mlflow.org/) logging.\n",
      "    logging_dir (`str`, *optional*):\n",
      "        [TensorBoard](https://www.tensorflow.org/tensorboard) log directory. Will default to\n",
      "        *runs/**CURRENT_DATETIME_HOSTNAME***.\n",
      "    logging_strategy (`str` or [`~transformers.trainer_utils.IntervalStrategy`], *optional*, defaults to `\"steps\"`):\n",
      "        The logging strategy to adopt during training. Possible values are:\n",
      "\n",
      "            - `\"no\"`: No logging is done during training.\n",
      "            - `\"epoch\"`: Logging is done at the end of each epoch.\n",
      "            - `\"steps\"`: Logging is done every `logging_steps`.\n",
      "\n",
      "    logging_first_step (`bool`, *optional*, defaults to `False`):\n",
      "        Whether to log and evaluate the first `global_step` or not.\n",
      "    logging_steps (`int`, defaults to 50):\n",
      "        Number of update steps between two logs if `logging_strategy=\"steps\"`.\n",
      "    eval_strategy (`str` or [`~transformers.trainer_utils.IntervalStrategy`], *optional*, defaults to `\"no\"`):\n",
      "        The evaluation strategy to adopt during training. Possible values are:\n",
      "\n",
      "            - `\"no\"`: No evaluation is done during training.\n",
      "            - `\"steps\"`: Evaluation is done (and logged) every `eval_steps`.\n",
      "            - `\"epoch\"`: Evaluation is done at the end of each epoch.\n",
      "\n",
      "    eval_steps (`int`, *optional*):\n",
      "        Number of update steps between two evaluations if `eval_strategy=\"steps\"`. Will default to the same\n",
      "        value as `logging_steps` if not set.\n",
      "    eval_delay (`float`, *optional*):\n",
      "        Number of epochs or steps to wait for before the first evaluation can be performed, depending on the\n",
      "        eval_strategy.\n",
      "    eval_max_steps (`int`, defaults to `-1`):\n",
      "        If set to a positive number, the total number of evaluation steps to perform. The evaluation may stop\n",
      "        before reaching the set number of steps when all data is exhausted.\n",
      "\n",
      "    save_strategy (`str` or [`~transformers.trainer_utils.IntervalStrategy`], *optional*, defaults to `\"steps\"`):\n",
      "        The checkpoint save strategy to adopt during training. Possible values are:\n",
      "\n",
      "            - `\"no\"`: No save is done during training.\n",
      "            - `\"epoch\"`: Save is done at the end of each epoch.\n",
      "            - `\"steps\"`: Save is done every `save_steps`.\n",
      "    save_steps (`int`, *optional*, defaults to 500):\n",
      "        Number of updates steps before two checkpoint saves if `save_strategy=\"steps\"`.\n",
      "    save_total_limit (`int`, *optional*, defaults to `1`):\n",
      "        If a value is passed, will limit the total amount of checkpoints. Deletes the older checkpoints in\n",
      "        `output_dir`. Note, the best model is always preserved if the `eval_strategy` is not `\"no\"`.\n",
      "    load_best_model_at_end (`bool`, *optional*, defaults to `False`):\n",
      "        Whether or not to load the best model found during training at the end of training.\n",
      "\n",
      "        <Tip>\n",
      "\n",
      "        When set to `True`, the parameters `save_strategy` needs to be the same as `eval_strategy`, and in\n",
      "        the case it is \"steps\", `save_steps` must be a round multiple of `eval_steps`.\n",
      "\n",
      "        </Tip>\n",
      "\u001b[31mFile:\u001b[39m           ~/miniforge/envs/galtan_group_appeals/lib/python3.11/site-packages/setfit/training_args.py\n",
      "\u001b[31mType:\u001b[39m           type\n",
      "\u001b[31mSubclasses:\u001b[39m     "
     ]
    }
   ],
   "source": [
    "TrainingArguments?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8b7a338a",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    num_epochs=1,\n",
    "    max_steps=50,\n",
    "    batch_size=4,\n",
    "    num_iterations=20,\n",
    "    save_strategy=\"no\",\n",
    "    report_to=\"none\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9d8fedfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1094/1094 [00:00<00:00, 47094.60 examples/s]\n",
      "Map: 100%|██████████| 221/221 [00:00<00:00, 22861.47 examples/s]\n"
     ]
    }
   ],
   "source": [
    "trainer = AbsaTrainer(\n",
    "    model,\n",
    "    args=args,\n",
    "    train_dataset=dataset['train'],  # if you want to train over the entire train set change experiment_ds to train_ds\n",
    "    column_mapping={\n",
    "        \"text\": \"text\",\n",
    "        \"mention\": \"span\",\n",
    "        \"label\": \"label\",\n",
    "        \"ordinal\": \"ordinal\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "941a09df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num unique pairs = 43760\n",
      "  Batch size = 4\n",
      "  Num epochs = 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:02, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num unique pairs = 8840\n",
      "  Batch size = 4\n",
      "  Num epochs = 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:01, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ad030573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mSignature:\u001b[39m\n",
      "ds.rename_column(\n",
      "    original_column_name: str,\n",
      "    new_column_name: str,\n",
      "    new_fingerprint: Optional[str] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      ") -> \u001b[33m'Dataset'\u001b[39m\n",
      "\u001b[31mDocstring:\u001b[39m\n",
      "Rename a column in the dataset, and move the features associated to the original column under the new column\n",
      "name.\n",
      "\n",
      "Args:\n",
      "    original_column_name (`str`):\n",
      "        Name of the column to rename.\n",
      "    new_column_name (`str`):\n",
      "        New name for the column.\n",
      "    new_fingerprint (`str`, *optional*):\n",
      "        The new fingerprint of the dataset after transform.\n",
      "        If `None`, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments.\n",
      "\n",
      "Returns:\n",
      "    [`Dataset`]: A copy of the dataset with a renamed column.\n",
      "\n",
      "Example:\n",
      "\n",
      "```py\n",
      ">>> from datasets import load_dataset\n",
      ">>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
      ">>> ds = ds.rename_column('label', 'label_new')\n",
      "Dataset({\n",
      "    features: ['text', 'label_new'],\n",
      "    num_rows: 1066\n",
      "})\n",
      "```\n",
      "\u001b[31mFile:\u001b[39m      ~/miniforge/envs/galtan_group_appeals/lib/python3.11/site-packages/datasets/arrow_dataset.py\n",
      "\u001b[31mType:\u001b[39m      method"
     ]
    }
   ],
   "source": [
    "ds.rename_column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4655ea62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'span', 'ordinal', 'pred_polarity'],\n",
       "    num_rows: 75\n",
       "})"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = dataset['test'].remove_columns(['label'])\n",
    "ds = ds.rename_column('mention', 'span')\n",
    "output  = model.predict(ds) # a new column which holds the predicted polarity, \"pred_polarity\", is added to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "da0db772",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hauke-licht/miniforge/envs/galtan_group_appeals/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8266666666666667,\n",
       " 'balanced_accuracy': 0.5198087431693988,\n",
       " 'f1_macro': 0.4867724867724868,\n",
       " 'precision_macro': 0.45897435897435895,\n",
       " 'recall_macro': 0.5198087431693988,\n",
       " 'precision_Positive': 0.8769230769230769,\n",
       " 'recall_Positive': 0.9344262295081968,\n",
       " 'f1_Positive': 0.9047619047619048,\n",
       " 'precision_Neutral': 0.0,\n",
       " 'recall_Neutral': 0.0,\n",
       " 'f1_Neutral': 0.0,\n",
       " 'precision_Negative': 0.5,\n",
       " 'recall_Negative': 0.625,\n",
       " 'f1_Negative': 0.5555555555555556}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = [label2id[l] for l in output['pred_polarity']]\n",
    "y_test = [label2id[l] for l in dataset['test']['label']]\n",
    "compute_metrics_multiclass(y_pred, y_test, id2label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
