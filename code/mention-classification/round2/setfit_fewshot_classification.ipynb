{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "552ec552",
   "metadata": {},
   "source": [
    "# SetFit for Multilabel Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af7f258-5aaf-47c2-b81e-2f10fc349812",
   "metadata": {},
   "source": [
    "notebook based on https://github.com/huggingface/setfit/blob/main/notebooks/text-classification_multilabel.ipynb\n",
    "\n",
    "See also:\n",
    "\n",
    "- https://huggingface.co/docs/setfit/en/how_to/multilabel\n",
    "- https://github.com/huggingface/setfit/issues/413#issuecomment-1697751329"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5604f73-f395-42cb-8082-9974a87ef9e9",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50cfb2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../../code/mention-classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70a859ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hlicht/miniforge3/envs/galtan_group_appeals/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from utils.setfit import get_class_weights, model_init\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from setfit import TrainingArguments, Trainer\n",
    "from sentence_transformers.losses import CosineSimilarityLoss\n",
    "\n",
    "from utils.metrics import *\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c003a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"sentence-transformers/paraphrase-mpnet-base-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc551c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "#  - prepare dataset function\n",
    "#  - prepare model function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e756be8-3b60-4c86-aa1b-7ef78289b8e2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0095eab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['economic_attributes', 'non-economic_attributes', 'stance',\n",
       "       'universal_attributes'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "data_path = '../../../data/annotations/group_mention_categorization'\n",
    "\n",
    "dirs = ['social-group-mention-categorization-coder-training', 'social-group-mention-categorization-round02']\n",
    "\n",
    "fps = [os.path.join(data_path, d, 'parsed', 'consolidated_annotations.tsv') for d in dirs]\n",
    "\n",
    "df_all = pd.concat([pd.read_csv(fp, sep='\\t') for fp in fps], axis=0, ignore_index=True)\n",
    "df_all.q_id.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf39910",
   "metadata": {},
   "source": [
    "## Universal attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93d2e593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: let's focus on universal attributes for now\n",
    "df = df_all.loc[df_all.q_id == 'universal_attributes', ['text', 'mention', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "753c2beb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "No       394\n",
       "Yes       56\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['label']].value_counts(sort=False)\n",
    "# NOTE: extreme label class imbalance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cccee24",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {'No': 0, 'Yes': 1}\n",
    "id2label = {0: 'No', 1: 'Yes'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72de8c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.label = df.label.map(label2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dadd92c",
   "metadata": {},
   "source": [
    "### split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1bf09790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: consider increasing train size\n",
    "trn, tst = train_test_split(range(len(df)), test_size=0.5, stratify=df.label, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "828c6ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bde08023",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['input'] = df.text + tokenizer.sep_token + df.mention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea42967b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['input', 'label']\n",
    "dataset = datasets.DatasetDict({\n",
    "    'train': datasets.Dataset.from_pandas(df.iloc[trn][cols], preserve_index=False),\n",
    "    'test': datasets.Dataset.from_pandas(df.iloc[tst][cols], preserve_index=False)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00cbe3c",
   "metadata": {},
   "source": [
    "### fine-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10685e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir='setfit',\n",
    "    batch_size=(32, 4),\n",
    "    num_epochs=(1, 1),\n",
    "    max_steps=10,\n",
    "    body_learning_rate=(2e-5, 1e-5),\n",
    "    head_learning_rate=1e-2,\n",
    "    end_to_end=True,\n",
    "    samples_per_label=2, # default but can be increased for TripletLoss\n",
    "    loss=CosineSimilarityLoss, # note: could use TripletLoss\n",
    "    use_amp=True,\n",
    "    report_to='none'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40f9c188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'No': 0.12444444444444443, 'Yes': 0.8755555555555555}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights = get_class_weights(np.array(dataset['train']['label']))\n",
    "dict(zip(id2label.values(), class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "93103d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying column mapping to the training dataset\n",
      "Applying column mapping to the evaluation dataset\n",
      "Map: 100%|██████████| 225/225 [00:00<00:00, 30144.97 examples/s]\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model_init=lambda: model_init(\n",
    "        model_name=MODEL,\n",
    "        id2label=id2label,\n",
    "        # class_weights=class_weights,\n",
    "        device='mps'\n",
    "    ),\n",
    "    args=args,\n",
    "    metric=compute_metrics_binary,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['test'],\n",
    "    column_mapping={'input': 'text', 'label': 'label'},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ce8f491",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num unique pairs = 320\n",
      "  Batch size = 32\n",
      "  Num epochs = 1\n",
      " 10%|█         | 1/10 [00:02<00:19,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.2752, 'grad_norm': 1.1571369171142578, 'learning_rate': 2e-05, 'epoch': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:17<00:00,  1.79s/it]\n",
      "The `max_length` is `None`. Using the maximum acceptable length according to the current model body: 512.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 17.9318, 'train_samples_per_second': 17.845, 'train_steps_per_second': 0.558, 'train_loss': 0.2602002501487732, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab6199d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7b8285",
   "metadata": {},
   "source": [
    "### evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c805e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array(dataset['test']['label'])\n",
    "y_pred = trainer.model.predict(dataset['test']['input'], use_labels=False).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef697e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_true, y_pred, target_names=id2label.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cfbabeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs = trainer.model.model_body.tokenizer(dataset['test']['input'][:16], return_tensors='pt', padding=True)\n",
    "# with torch.no_grad():\n",
    "#     embeddings = trainer.model.model_body(inputs.to('mps'))\n",
    "#     # outputs = trainer.model.model_head.linear(embeddings['sentence_embedding']).cpu().numpy()\n",
    "#     logits, probs = trainer.model.model_head(embeddings['sentence_embedding'], temperature=1.0)\n",
    "#     logits = logits.cpu().numpy()\n",
    "#     probs = probs.cpu().numpy()\n",
    "\n",
    "# probs.round(3) # overconfidence, need to apply early stopping?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adde9291",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = trainer.model.predict_proba(dataset['test']['input'], as_numpy=True)\n",
    "probs.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0321d7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "labs = np.array(dataset['test']['label'])\n",
    "pred_probs = probs[range(len(labs)), labs]\n",
    "\n",
    "# plot boxplot of predicted probabilities by true label\n",
    "plt.boxplot([pred_probs[labs==0], pred_probs[labs==1]], labels=['No', 'Yes'])\n",
    "plt.ylim(0, 1)\n",
    "# draw a line at 0.5\n",
    "plt.axhline(0.5, color='r', linestyle='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29159833",
   "metadata": {},
   "source": [
    "## Universal/econ/non-econ as three-way multilabel problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa75eb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(fp, sep='\\t')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "44b1d72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack by category\n",
    "tmp = pd.concat([\n",
    "    df[df.q_id == 'universal_attributes'].drop(columns=['category']),\n",
    "    df[df.q_id == 'economic_attributes'].groupby(['mention_id', 'text', 'mention', 'q_id']).agg({'label': lambda x: 'Yes' if (x=='Yes').any() else 'No'}).reset_index(),\n",
    "    df[df.q_id == 'non-economic_attributes'].groupby(['mention_id', 'text', 'mention', 'q_id']).agg({'label': lambda x: 'Yes' if (x=='Yes').any() else 'No'}).reset_index()\n",
    "])\n",
    "tmp.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# get dimensions\n",
    "tmp.q_id = tmp.q_id.str.removesuffix('_attributes')\n",
    "features = tmp.q_id.unique().tolist()\n",
    "\n",
    "# reshape to wide format\n",
    "tmp = tmp.pivot(index=['mention_id', 'text', 'mention'], columns='q_id', values='label').reset_index()\n",
    "tmp = tmp.rename_axis(None, axis=1)\n",
    "\n",
    "# keep only fully gold-labeled examples\n",
    "tmp = tmp[tmp[features].isna().sum(axis=1) == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602401ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp[features].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "abe59829",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {'No': 0, 'Yes': 1}\n",
    "id2label = {0: 'No', 1: 'Yes'}\n",
    "tmp.loc[:,features] = tmp.loc[:,features].apply(lambda x: x.map(label2id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9490dab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp['labels'] = tmp.loc[:,features].apply(list, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c7de5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp[features].mean(axis=0)\n",
    "# strong label class imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4936057f",
   "metadata": {},
   "source": [
    "## split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7faca004",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "69912139",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp['input'] = tmp.text + tokenizer.sep_token + tmp.mention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "703707aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length_ = max(tokenizer(tmp.input.to_list(), truncation=False, padding=False, return_length=True).length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0b53c639",
   "metadata": {},
   "outputs": [],
   "source": [
    "trn, tst = train_test_split(range(len(tmp)), test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c3227e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['input', 'labels']\n",
    "dataset = datasets.DatasetDict({\n",
    "    'train': datasets.Dataset.from_pandas(tmp.iloc[trn][cols], preserve_index=False),\n",
    "    'test': datasets.Dataset.from_pandas(tmp.iloc[tst][cols], preserve_index=False)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130f6664",
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = tmp.iloc[tst][features].to_numpy()\n",
    "class_weights = get_class_weights(feats, multitarget=True)\n",
    "class_weights = class_weights.astype(float)\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1702b55",
   "metadata": {},
   "source": [
    "## Fine-tuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2242313",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {i: l for i, l in enumerate(features)}\n",
    "label2id = {l: i for i, l in enumerate(features)}\n",
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e6f52b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from setfit import SetFitModel\n",
    "# \n",
    "# device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "# def model_init():\n",
    "#     return SetFitModel.from_pretrained(\n",
    "#         model_name, \n",
    "#         use_differentiable_head=True, \n",
    "#         head_params={\"out_features\": len(features)},\n",
    "#         multi_target_strategy='one-vs-rest',\n",
    "#         labels=features,\n",
    "#         id2label=id2label\n",
    "#     ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f60340a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.losses import CosineSimilarityLoss\n",
    "from setfit import TrainingArguments, Trainer\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir='setfit',\n",
    "    batch_size=(32, 4),\n",
    "    max_length=max_length_,\n",
    "    num_epochs=(1, 8),\n",
    "    max_steps=100,\n",
    "    end_to_end=False,\n",
    "    samples_per_label=2,\n",
    "    loss=CosineSimilarityLoss,\n",
    "    use_amp=True,\n",
    "    report_to='none'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f800dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.metrics import compute_metrics_multilabel\n",
    "trainer = Trainer(\n",
    "    model_init=lambda: model_init(\n",
    "        model_name=MODEL,\n",
    "        id2label=id2label,\n",
    "        multitarget_strategy='one-vs-rest',\n",
    "        class_weights=class_weights,\n",
    "        device='mps'\n",
    "    ),\n",
    "    metric=lambda p, t: compute_metrics_multilabel(p, t, id2label),\n",
    "    args=args,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['test'],\n",
    "    column_mapping={\"input\": \"text\", \"labels\": \"label\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a169b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661f37b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c839e9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame(metrics, index=[0]).T.reset_index().rename(columns={'index': 'metric', 0: 'value'})\n",
    "res[['metric', 'category']] = res.metric.str.split('_', expand=True)\n",
    "res = res.pivot(index='category', columns='metric', values='value')\n",
    "# remove index names\n",
    "res.columns.name = None\n",
    "res.index.name = None\n",
    "res.loc[['macro']+features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7ab4766c",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = trainer.model.predict_proba(dataset['test']['input'], as_numpy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bc5470",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49168dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_df = pd.DataFrame(probs, columns=['pred: '+f for f in features]).round(3)\n",
    "# compute loss \n",
    "losses = np.array(dataset['test']['labels']) - probs\n",
    "probs_df['loss'] = np.abs(losses).sum(axis=1).round(3)\n",
    "probs_df[features] = tmp.iloc[tst, :][features].reset_index(drop=True)\n",
    "probs_df[['text', 'mention']] = pd.Series(dataset['test']['input']).str.split(tokenizer.sep_token, expand=True)\n",
    "probs_df.sort_values('loss', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c018441f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IDEA: measure uncertainty by computing closeness to classification threshold\n",
    "threshold = np.ones(probs.shape)/2\n",
    "cuts = probs - threshold\n",
    "vals = np.abs(cuts).min(axis=1)\n",
    "idxs = vals.argsort()[::1]\n",
    "\n",
    "probs_df.iloc[idxs, :].head(16)#.text.to_list()\n",
    "# TODO: compute share of misclassification as indicator of informativeness of ranking criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c909a56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference\n",
    "\n",
    "text_data_file = '../../data/intermediate/social_group_mentions_ranked.tsv'\n",
    "texts = pd.read_csv(text_data_file, sep='\\t', nrows=32*200) # has 13748 rows\n",
    "\n",
    "probs = trainer.model.predict_proba(texts.text.to_list(), as_numpy=True, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3c1d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = np.ones(probs.shape)/2\n",
    "cuts = probs - threshold\n",
    "vals = np.abs(cuts).min(axis=1)\n",
    "# plot histogram of vals\n",
    "plt.hist(vals, bins=20)\n",
    "plt.show()\n",
    "# NOTE: shows overall high \"confidence\" (maybe overconfidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9b3c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ = 100\n",
    "idxs = vals.argsort()[:n_]\n",
    "\n",
    "pd.concat([\n",
    "    texts.iloc[idxs, :][['text', 'mention']].reset_index(drop=True),\n",
    "    pd.DataFrame(probs[idxs, :].round(3), columns=features),\n",
    "\n",
    "], axis=1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d87d31",
   "metadata": {},
   "source": [
    "## Non-econ attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c54471f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: let's focus on non-economic attributes for now\n",
    "df = pd.read_csv(fp, sep='\\t')\n",
    "df = df[df.q_id == 'non-economic_attributes']\n",
    "df = df[~df.category.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572c77d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['category', 'label']].value_counts(sort=False)\n",
    "# NOTE: extreme label class imbalance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbdc515",
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_cats = df[df.label == 'Yes'].category.unique().tolist()\n",
    "\n",
    "df = df[df.category.isin(keep_cats)]\n",
    "df[['category', 'label']].value_counts(sort=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd7d2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove any instances where some are Unsure\n",
    "discard = df.groupby('mention_id').agg({'label': lambda l: (l == 'Unsure').any()})\n",
    "discard = discard[discard.label].index.to_list()\n",
    "\n",
    "df = df[~df.mention_id.isin(discard)]\n",
    "\n",
    "df[['category', 'label']].value_counts(sort=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc88c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['text', 'mention', 'category', 'label']]\n",
    "\n",
    "# pivot wider\n",
    "df = df.pivot(index=['text', 'mention'], columns='category', values='label').reset_index()\n",
    "\n",
    "features = df.columns[2:].to_list()\n",
    "\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8050026",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.iloc[:,3:].isna().sum(axis=1) == 0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61666ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {'No': 0, 'Yes': 1}\n",
    "id2label = {0: 'No', 1: 'Yes'}\n",
    "df.loc[:,features] = df.loc[:,features].apply(lambda x: x.map(label2id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e89621a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['labels'] = df.loc[:,features].apply(list, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4124687d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[features].mean(axis=0)\n",
    "# still crazy label class imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64740887",
   "metadata": {},
   "source": [
    "### split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2e9267",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[features].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0eb9bc83-bf99-422c-b123-4fcaed168bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from skmultilearn.model_selection import IterativeStratification\n",
    "# \n",
    "# X = np.zeros((len(df), 1))# df[['text', 'mention', 'labels']]\n",
    "# y = df[features].reset_index(drop=True)\n",
    "# test_size = 0.4\n",
    "# stratifier = IterativeStratification(n_splits=2, order=2, sample_distribution_per_fold=[test_size, 1.0-test_size])\n",
    "# stratifier = IterativeStratification(n_splits=2, order=2, sample_distribution_per_fold=[test_size, 1.0-test_size])\n",
    "# train_indexes, test_indexes = next(stratifier.split(X, y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d23f980c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "trn, tst = train_test_split(range(len(df)), test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "517efad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"sentence-transformers/paraphrase-mpnet-base-v2\"\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3cb4e1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['input'] = df.text + tokenizer.sep_token + df.mention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "239c91f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['input', 'labels']\n",
    "dataset = datasets.DatasetDict({\n",
    "    'train': datasets.Dataset.from_pandas(df.iloc[trn][cols], preserve_index=False),\n",
    "    'test': datasets.Dataset.from_pandas(df.iloc[tst][cols], preserve_index=False)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e7c839-1f06-4d35-aa34-6e13659db814",
   "metadata": {},
   "source": [
    "### Fine-tuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4181cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "embedder = SentenceTransformer(model_name, device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e8c41a",
   "metadata": {},
   "source": [
    "To train a SetFit model, the first thing to do is download a pretrained checkpoint from the Hub. We can do so by using the `from_pretrained()` method associated with the `SetFitModel` class.\n",
    "\n",
    "**Note that the `multi_target_strategy` parameter here signals to both the model and the trainer to expect a multi-labelled dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af2d215e",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {i: l for i, l in enumerate(features)}\n",
    "label2id = {l: i for i, l in enumerate(features)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ceb0abe",
   "metadata": {},
   "source": [
    "### Non-diff head (regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33661c9d-46d3-42eb-9b15-8a2bc49d7f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from setfit import SetFitModel\n",
    "\n",
    "model = SetFitModel.from_pretrained(\n",
    "    model_name, \n",
    "    multi_target_strategy='one-vs-rest',\n",
    "    labels=features\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92bbfb6",
   "metadata": {},
   "source": [
    "Alternative is to init explicitly (see [here](https://github.com/huggingface/setfit/issues/413#issuecomment-1697751329))\n",
    "\n",
    "```python\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "model = SetFitModel(\n",
    "    model_body=embedder, \n",
    "    model_head=OneVsRestClassifier(LogisticRegression(class_weight=\"balanced\")),\n",
    "    multi_target_strategy=\"one-vs-rest\"\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "f45f0f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "def model_init():\n",
    "    return SetFitModel.from_pretrained(\n",
    "        model_name, \n",
    "        use_differentiable_head=True, \n",
    "        head_params={\"out_features\": len(features)},\n",
    "        multi_target_strategy='one-vs-rest',\n",
    "        labels=features,\n",
    "    ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "fd7c4855",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.losses import CosineSimilarityLoss\n",
    "from setfit import TrainingArguments, Trainer\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir='setfit',\n",
    "    batch_size=(16, 4),\n",
    "    num_epochs=(2, 8),\n",
    "    samples_per_label=2, # default but can be increased for TripletLoss\n",
    "    loss=CosineSimilarityLoss, # note: could use TripletLoss\n",
    "    use_amp=True,\n",
    "    end_to_end=False,\n",
    "    report_to='none'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44b7069-27b0-49ea-bc27-c44f94a98e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model_init=model_init,\n",
    "    args=args,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['test'],\n",
    "    column_mapping={\"input\": \"text\", \"labels\": \"label\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5a468b-2796-47c3-8907-c0147ee58dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e799f994",
   "metadata": {},
   "source": [
    "The final step is to compute the model's performance using the `evaluate()` method. The default metric measures 'subset accuracy', which measures the fraction of samples where we predict all 8 labels correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453c11d0-a1e4-49c2-859a-cc70e033b4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = trainer.evaluate()\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "06c1d5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array(dataset['test'][\"labels\"])\n",
    "y_pred = trainer.model.predict(dataset['test'][\"input\"], use_labels=False).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "fd3bb759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# inputs = tokenizer(['hello'], return_tensors='pt')\n",
    "# \n",
    "# with torch.no_grad():\n",
    "#     embeddings = trainer.model.model_body(inputs.to('mps'))\n",
    "#     logits, probs = trainer.model.model_head(embeddings['sentence_embedding'], temperature=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0385856",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import  Counter\n",
    "Counter([id2label[i] for labs in dataset['train'][\"labels\"] for i, l in enumerate(labs) if l == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa71a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "scores = {}\n",
    "for l, a, b in zip(trainer.model.labels, y_true.T, y_pred.T):\n",
    "    p, r, f1, _ = precision_recall_fscore_support(a, b, average='binary', zero_division=0.0)\n",
    "    scores[l] = {'f1': f1, 'precision': p, 'recall': r, 'support': np.sum(a)}\n",
    "pd.DataFrame(scores).T\n",
    "\n",
    "# {f'{m}_{l}': v for l, s in scores.items() for m, v in s.items()} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "db4419e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = trainer.model.model_body.tokenizer(dataset['test']['input'][:16], return_tensors='pt', padding=True)\n",
    "with torch.no_grad():\n",
    "    embeddings = trainer.model.model_body(inputs.to('mps'))\n",
    "    outputs = trainer.model.model_head.linear(embeddings['sentence_embedding'])\n",
    "    # logits, probs = trainer.model.model_head(embeddings['sentence_embedding'], temperature=1.0)\n",
    "    # logits = logits.cpu().numpy()\n",
    "    # probs = probs.cpu().numpy()\n",
    "\n",
    "threshold = np.ones(probs.shape)/2\n",
    "cuts = probs - threshold\n",
    "idx = np.abs(cuts).mean(axis=1).argmin()\n",
    "\n",
    "probs[idx], y_pred[idx], y_true[idx], dataset['test']['input'][idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baacc0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IDEA: focus sentence embedding model on mention\n",
    "inputs = embedder.tokenizer(dataset['test']['input'][:1], return_tensors='pt', padding=True)\n",
    "\n",
    "import torch\n",
    "with torch.no_grad():\n",
    "    features = embedder[0](features=inputs)\n",
    "\n",
    "token_embeddings = features[\"token_embeddings\"]\n",
    "attention_mask = (\n",
    "    features[\"attention_mask\"]\n",
    "    if \"attention_mask\" in features\n",
    "    else torch.ones(token_embeddings.shape[:-1], device=token_embeddings.device, dtype=torch.int64)\n",
    ")\n",
    "\n",
    "mask = features['input_ids'] == embedder.tokenizer.sep_token_id\n",
    "mask = mask.cumsum(dim=1) == 1\n",
    "# convert mask to same type as attention_mask\n",
    "mask = mask.to(attention_mask.dtype)\n",
    "attention_mask = mask\n",
    "\n",
    "# note: this is what happens in the SentenceTransformer model under the hood\n",
    "input_mask_expanded = (\n",
    "    attention_mask.unsqueeze(-1).expand(token_embeddings.size()).to(token_embeddings.dtype)\n",
    ")\n",
    "sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "\n",
    "sum_mask = input_mask_expanded.sum(1)\n",
    "\n",
    "sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "\n",
    "output_vector = sum_embeddings / sum_mask\n",
    "\n",
    "output_vector.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bbf7a1",
   "metadata": {},
   "source": [
    "## Stance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2331d061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: let's focus on non-economic attributes for now\n",
    "df = pd.read_csv(fp, sep='\\t')\n",
    "df = df[df.q_id == 'stance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65468692",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['label']].value_counts(sort=False)\n",
    "# NOTE: extreme label class imbalance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9646657f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['text', 'mention', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3e3a7813",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {'Neutral': 0, 'Positive': 1, 'Negative': 2}\n",
    "id2label = {i: l for l, i in label2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "25be0ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.label = df.label.map(label2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1f8070",
   "metadata": {},
   "source": [
    "### split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cb35f8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trn, tst = train_test_split(range(len(df)), test_size=0.5, stratify=df.label, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7288b916",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"sentence-transformers/paraphrase-mpnet-base-v2\"\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5b2db338",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['input'] = df.text + tokenizer.sep_token + df.mention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9d1c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length_ = max(tokenizer(df.input.to_list(), truncation=False, padding=False, return_length=True).length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f370126d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['input', 'label']\n",
    "dataset = datasets.DatasetDict({\n",
    "    'train': datasets.Dataset.from_pandas(df.iloc[trn][cols], preserve_index=False),\n",
    "    'test': datasets.Dataset.from_pandas(df.iloc[tst][cols], preserve_index=False)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862270bc",
   "metadata": {},
   "source": [
    "### Fine-tuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "86100868",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir='setfit',\n",
    "    batch_size=(32, 4),\n",
    "    max_length=max_length_,\n",
    "    num_epochs=(1, 8),\n",
    "    max_steps=10,\n",
    "    body_learning_rate=(2e-5, 1e-5),\n",
    "    head_learning_rate=1e-2,\n",
    "    end_to_end=False,\n",
    "    samples_per_label=2, # default but can be increased for TripletLoss\n",
    "    loss=CosineSimilarityLoss, # note: could use TripletLoss\n",
    "    use_amp=True,\n",
    "    report_to='none'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f70173a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = get_class_weights(np.array(dataset['train']['label']))\n",
    "dict(zip(id2label.values(), class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209d033d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model_init=lambda: model_init(\n",
    "        model_name=MODEL,\n",
    "        id2label=id2label,\n",
    "        class_weights=class_weights,\n",
    "        device='mps'\n",
    "    ),\n",
    "    args=args,\n",
    "    metric=lambda p, t: compute_metrics_multiclass(p, t, id2label),\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['test'],\n",
    "    column_mapping={'input': 'text', 'label': 'label'},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77da4911",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797eebb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = trainer.evaluate()\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b128eb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = trainer.model.model_body.tokenizer(dataset['test']['input'][:16], return_tensors='pt', padding=True)\n",
    "with torch.no_grad():\n",
    "    embeddings = trainer.model.model_body(inputs.to('mps'))\n",
    "    # outputs = trainer.model.model_head.linear(embeddings['sentence_embedding']).cpu().numpy()\n",
    "    logits, probs = trainer.model.model_head(embeddings['sentence_embedding'], temperature=1.0)\n",
    "    logits = logits.cpu().numpy()\n",
    "    probs = probs.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0a727e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517e7b1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41707298",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = trainer.model.predict_proba(dataset['test']['input'], as_numpy=True)\n",
    "labs = np.array(dataset['test']['label'])\n",
    "pred_probs = probs[range(len(labs)), labs]\n",
    "\n",
    "# plot boxplot of predicted probabilities by true label\n",
    "plt.boxplot(\n",
    "    [pred_probs[labs==i] for i in id2label.keys()],\n",
    "    labels=id2label.values()\n",
    ")\n",
    "plt.ylim(0, 1)\n",
    "# draw a line at 0.5\n",
    "plt.axhline(0.5, color='r', linestyle='--')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "galtan_group_appeals",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
