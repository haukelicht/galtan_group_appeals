{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hlicht/miniforge3/envs/galtan_group_appeals/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "import datasets\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from setfit import SetFitHead, SetFitModel\n",
    "\n",
    "from setfit import TrainingArguments, Trainer\n",
    "from sentence_transformers.losses import CosineSimilarityLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "import copy\n",
    "\n",
    "import torch.nn as nn\n",
    "from sentence_transformers.models import Pooling\n",
    "from sentence_transformers.quantization import quantize_embeddings\n",
    "\n",
    "from sentence_transformers.util import batch_to_device, truncate_embeddings\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import regex\n",
    "\n",
    "from torch import Tensor\n",
    "from typing import Union, List, Tuple, Dict, Literal\n",
    "\n",
    "from tqdm.autonotebook import trange\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class SpanEmbeddingPooling(Pooling):\n",
    "    \"\"\"\n",
    "    Subclass of sentence_transformers.models.Pooling.Pooling Performs pooling (max or mean) on the token embeddings of \n",
    "    tokens in the span by using the span_mask to mask out embeddings of tokens that are not part of the span.\n",
    "\n",
    "    Using pooling, it generates from a variable sized sentence a fixed sized span embedding. \n",
    "\n",
    "    Args:\n",
    "        word_embedding_dimension: Dimensions for the word embeddings\n",
    "        pooling_mode: Either \"max\", \"mean\",\n",
    "            \"mean_sqrt_len_tokens\", or \"weightedmean\". If set,\n",
    "            overwrites the other pooling_mode_* settings\n",
    "        pooling_mode_cls_token: Use the first token (CLS token) as text\n",
    "            representations \n",
    "            IMPORTANT: not supported and only kept for consistency\n",
    "        pooling_mode_max_tokens: Use max in each dimension over all\n",
    "            tokens.\n",
    "        pooling_mode_mean_tokens: Perform mean-pooling\n",
    "        pooling_mode_mean_sqrt_len_tokens: Perform mean-pooling, but\n",
    "            divide by sqrt(input_length).\n",
    "        pooling_mode_weightedmean_tokens: Perform (position) weighted\n",
    "            mean pooling. See `SGPT: GPT Sentence Embeddings for\n",
    "            Semantic Search <https://arxiv.org/abs/2202.08904>`_.\n",
    "        pooling_mode_lasttoken: Perform last token pooling. See `SGPT:\n",
    "            GPT Sentence Embeddings for Semantic Search\n",
    "            <https://arxiv.org/abs/2202.08904>`_ and `Text and Code\n",
    "            Embeddings by Contrastive Pre-Training\n",
    "            <https://arxiv.org/abs/2201.10005>`_.\n",
    "            IMPORTANT: not supported and only kept for consistency\n",
    "    \"\"\"\n",
    "\n",
    "    POOLING_MODES = (\n",
    "        \"max\",\n",
    "        \"mean\",\n",
    "        \"mean_sqrt_len_tokens\",\n",
    "        \"weightedmean\",\n",
    "    )\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        if kwargs.get('pooling_mode_cls_token', False):\n",
    "            raise NotImplementedError(\"pooling_mode_cls_token is not supported for SpanEmbeddingPooling\")\n",
    "        if kwargs.get('pooling_mode_last_token', False):\n",
    "            raise NotImplementedError(\"pooling_mode_last_token is not supported for SpanEmbeddingPooling\")\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"SpanEmbeddingPooling({self.get_config_dict()})\"\n",
    "\n",
    "    @staticmethod\n",
    "    def load(input_path) -> Pooling:\n",
    "        with open(os.path.join(input_path, \"config.json\")) as fIn:\n",
    "            config = json.load(fIn)\n",
    "\n",
    "        return SpanEmbeddingPooling(**config)\n",
    "    \n",
    "    def forward(self, features: dict[str, Tensor]) -> dict[str, Tensor]:\n",
    "        features[\"attention_mask\"] = features[\"span_mask\"].to(features[\"attention_mask\"].device)\n",
    "        # del features[\"span_mask\"]\n",
    "        output = super().forward(features)\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "def _process_span_inputs(texts: List[Tuple[str, Union[Tuple[int, int], str]]]):\n",
    "    \"\"\"\n",
    "    Helper function to process the input texts and spans into a list of tuples of texts and span character positions.\n",
    "\n",
    "    Args:\n",
    "        texts (List[Tuple[str, Union[Tuple[int, int], str]]]): A list of tuples of texts to be tokenized and the span (positions) they contain.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[str], List[Tuple[int, int]]]: A tuple of lists of texts and span character positions\n",
    "    \"\"\"\n",
    "    sentences, spans = [], []\n",
    "    for i, (t, s) in enumerate(texts):\n",
    "        sentences.append(t)\n",
    "        if isinstance(s, str):\n",
    "            m = regex.search(s, t)\n",
    "            if m is None:\n",
    "                raise ValueError(f\"Could not find '{s}' in '{t}'.\")\n",
    "            spans.append(m.span())\n",
    "        else:\n",
    "            spans.append(s)\n",
    "    return sentences, spans\n",
    "\n",
    "\n",
    "class SentenceTransformerForSpanEmbedding(SentenceTransformer):\n",
    "    \"\"\"\n",
    "    SentenceTransformer model for span embedding.\n",
    "\n",
    "    Args: check ``?sentence_transformers.SentenceTransformer``\n",
    "\n",
    "    Example:\n",
    "        ::\n",
    "\n",
    "            model = SentenceTransformerForSpanEmbedding('all-mpnet-base-v2')\n",
    "            sentences = [\n",
    "                (\"The weather is lovely today.\", \"weather\"),\n",
    "                (\"It's so sunny outside!\", \"outside\"),\n",
    "                (\"He drove to the stadium.\", \"He drove\")\n",
    "            ]\n",
    "            embeddings = model.encode(sentences)\n",
    "            print(embeddings.shape)\n",
    "            # (3, 768)\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "    def _load_module_class_from_ref(\n",
    "            self,\n",
    "            class_ref: str,\n",
    "            *args,\n",
    "            **kwargs\n",
    "    ) -> nn.Module:\n",
    "        # use SpanEmbeddingPooling instead of Pooling for pooling module\n",
    "        if class_ref == \"sentence_transformers.models.Pooling\":\n",
    "            return SpanEmbeddingPooling  \n",
    "        # otherwise, use the default implementation\n",
    "        return super()._load_module_class_from_ref(class_ref, *args, **kwargs)\n",
    "\n",
    "    def tokenize(\n",
    "        self,\n",
    "        texts: List[Tuple[str, Union[Tuple[int, int], str]]]\n",
    "    ) -> Dict[str, Tensor]:\n",
    "        \"\"\"\n",
    "        Tokenizes the texts.\n",
    "\n",
    "        Args:\n",
    "            texts (List[Tuple[str, Union[Tuple[int, int], str]]]): A list of tuples of texts to be tokenized \n",
    "            and the span (positions) they contain.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, Tensor]: A dictionary of tensors with the tokenized texts. Common keys are \"input_ids\",\n",
    "            \"attention_mask\", \"token_type_ids\", and \"span_mask\".\n",
    "        \"\"\"\n",
    "        sentences, spans = _process_span_inputs(texts)\n",
    "        \n",
    "        # NOTE: super's tokenize() usually calls self._first_module().tokenize(), which applies some extra preprocessing\n",
    "        #        to allow for differnt input formats. We mimic this here but add `return_offsets_mapping=True` to get the\n",
    "        #        character locations of the tokens.\n",
    "        features = self._first_module().tokenizer(\n",
    "            sentences,\n",
    "            padding=True,\n",
    "            truncation=\"longest_first\",\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=self._first_module().tokenizer.max_len_single_sentence,\n",
    "            return_offsets_mapping=True, # <== added to get tokens character locations\n",
    "        )\n",
    "\n",
    "        # iterate over tokenized inputs and flag locations of spans \n",
    "        # NOTE: this is added logic relative to super's implementation\n",
    "        span_mask = torch.zeros(features['input_ids'].shape, dtype=features[\"attention_mask\"].dtype)\n",
    "        for i, span in enumerate(spans):\n",
    "            inside = False\n",
    "            for t, (om, am) in enumerate(zip(features['offset_mapping'][i], features['attention_mask'][i])):\n",
    "                if am == 0:\n",
    "                    break\n",
    "                if span[0] in range(*om) and not inside:\n",
    "                    span_mask[i][t] = 1\n",
    "                    inside = True\n",
    "                elif span[1]-1 in range(*om) and inside:\n",
    "                    span_mask[i][t] = 1\n",
    "                    inside = False\n",
    "                elif inside:\n",
    "                    span_mask[i][t] = 1\n",
    "        features.update({'span_mask': span_mask.to(features[\"attention_mask\"].device)})\n",
    "        del features['offset_mapping']\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def encode(\n",
    "        self,\n",
    "        sentences: Tuple[str, Union[Tuple[int, int], str]] | List[Tuple[str, Union[Tuple[int, int], str]]],\n",
    "        prompt_name: str | None = None,\n",
    "        prompt: str | None = None,\n",
    "        batch_size: int = 32,\n",
    "        show_progress_bar: bool | None = None,\n",
    "        output_value: Literal[\"sentence_embedding\", \"token_embeddings\"] | None = \"sentence_embedding\",\n",
    "        precision: Literal[\"float32\", \"int8\", \"uint8\", \"binary\", \"ubinary\"] = \"float32\",\n",
    "        convert_to_numpy: bool = True,\n",
    "        convert_to_tensor: bool = False,\n",
    "        device: str = None,\n",
    "        normalize_embeddings: bool = False,\n",
    "        **kwargs,\n",
    "    ) -> list[Tensor] | np.ndarray | Tensor:\n",
    "        \"\"\"\n",
    "        Computes sentence embeddings.\n",
    "\n",
    "        Args:\n",
    "            sentences (Tuple[str, Union[Tuple[int, int], str]] | List[Tuple[str, Union[Tuple[int, int], str]]]):\n",
    "                The tuple(s) of span(s) in sentence(s) to embed.\n",
    "            prompt_name (Optional[str], optional): The name of the prompt to use for encoding. Must be a key in the `prompts` dictionary,\n",
    "                which is either set in the constructor or loaded from the model configuration. For example if\n",
    "                ``prompt_name`` is \"query\" and the ``prompts`` is {\"query\": \"query: \", ...}, then the sentence \"What\n",
    "                is the capital of France?\" will be encoded as \"query: What is the capital of France?\" because the sentence\n",
    "                is appended to the prompt. If ``prompt`` is also set, this argument is ignored. Defaults to None.\n",
    "            prompt (Optional[str], optional): The prompt to use for encoding. For example, if the prompt is \"query: \", then the\n",
    "                sentence \"What is the capital of France?\" will be encoded as \"query: What is the capital of France?\"\n",
    "                because the sentence is appended to the prompt. If ``prompt`` is set, ``prompt_name`` is ignored. Defaults to None.\n",
    "            batch_size (int, optional): The batch size used for the computation. Defaults to 32.\n",
    "            show_progress_bar (bool, optional): Whether to output a progress bar when encode sentences. Defaults to None.\n",
    "            output_value (Optional[Literal[\"sentence_embedding\", \"token_embeddings\"]], optional): The type of embeddings to return:\n",
    "                \"sentence_embedding\" to get sentence embeddings, \"token_embeddings\" to get wordpiece token embeddings, and `None`,\n",
    "                to get all output values. Defaults to \"sentence_embedding\".\n",
    "            precision (Literal[\"float32\", \"int8\", \"uint8\", \"binary\", \"ubinary\"], optional): The precision to use for the embeddings.\n",
    "                Can be \"float32\", \"int8\", \"uint8\", \"binary\", or \"ubinary\". All non-float32 precisions are quantized embeddings.\n",
    "                Quantized embeddings are smaller in size and faster to compute, but may have a lower accuracy. They are useful for\n",
    "                reducing the size of the embeddings of a corpus for semantic search, among other tasks. Defaults to \"float32\".\n",
    "            convert_to_numpy (bool, optional): Whether the output should be a list of numpy vectors. If False, it is a list of PyTorch tensors.\n",
    "                Defaults to True.\n",
    "            convert_to_tensor (bool, optional): Whether the output should be one large tensor. Overwrites `convert_to_numpy`.\n",
    "                Defaults to False.\n",
    "            device (str, optional): Which :class:`torch.device` to use for the computation. Defaults to None.\n",
    "            normalize_embeddings (bool, optional): Whether to normalize returned vectors to have length 1. In that case,\n",
    "                the faster dot-product (util.dot_score) instead of cosine similarity can be used. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            Union[List[Tensor], ndarray, Tensor]: By default, a 2d numpy array with shape [num_inputs, output_dimension] is returned.\n",
    "            If only one string input is provided, then the output is a 1d array with shape [output_dimension]. If ``convert_to_tensor``,\n",
    "            a torch Tensor is returned instead. If ``self.truncate_dim <= output_dimension`` then output_dimension is ``self.truncate_dim``.\n",
    "\n",
    "        Example:\n",
    "            ::\n",
    "\n",
    "                model = SentenceTransformerForSpanEmbedding('all-mpnet-base-v2')\n",
    "\n",
    "                sentences = [\n",
    "                    (\"The weather is lovely today.\", \"weather)\n",
    "                    (\"It's so sunny outside!\", \"outside\")\n",
    "                    (\"He drove to the stadium.\", \"He drove\")\n",
    "                ]\n",
    "                embeddings = model.encode(sentences)\n",
    "                print(embeddings.shape)\n",
    "                # (3, 768)\n",
    "        \"\"\"\n",
    "        if self.device.type == \"hpu\" and not self.is_hpu_graph_enabled:\n",
    "            # import habana_frameworks.torch as ht\n",
    "            # \n",
    "            # ht.hpu.wrap_in_hpu_graph(self, disable_tensor_cache=True)\n",
    "            # self.is_hpu_graph_enabled = True\n",
    "            NotImplementedError(\"HPU is not supported in this version\")\n",
    "\n",
    "        self.eval()\n",
    "        if show_progress_bar is None:\n",
    "            show_progress_bar = logger.getEffectiveLevel() in (logging.INFO, logging.DEBUG)\n",
    "\n",
    "        if convert_to_tensor:\n",
    "            convert_to_numpy = False\n",
    "\n",
    "        if output_value != \"sentence_embedding\":\n",
    "            convert_to_tensor = False\n",
    "            convert_to_numpy = False\n",
    "\n",
    "        single_input = False\n",
    "        if isinstance(sentences, Tuple) or not hasattr(\n",
    "            sentences[0], \"__len__\"\n",
    "        ):  # Cast an individual sentence to a list with length 1\n",
    "            sentences = [sentences]\n",
    "            single_input = True\n",
    "\n",
    "        if prompt is None:\n",
    "            if prompt_name is not None:\n",
    "                try:\n",
    "                    prompt = self.prompts[prompt_name]\n",
    "                except KeyError:\n",
    "                    raise ValueError(\n",
    "                        f\"Prompt name '{prompt_name}' not found in the configured prompts dictionary with keys {list(self.prompts.keys())!r}.\"\n",
    "                    )\n",
    "            elif self.default_prompt_name is not None:\n",
    "                prompt = self.prompts.get(self.default_prompt_name, None)\n",
    "        else:\n",
    "            if prompt_name is not None:\n",
    "                logger.warning(\n",
    "                    \"Encode with either a `prompt`, a `prompt_name`, or neither, but not both. \"\n",
    "                    \"Ignoring the `prompt_name` in favor of `prompt`.\"\n",
    "                )\n",
    "\n",
    "        extra_features = {}\n",
    "        if prompt is not None:\n",
    "            prompt_len = len(prompt)\n",
    "            for i, input in enumerate(sentences):\n",
    "                span = input[1]\n",
    "                if isinstance(span, tuple):\n",
    "                    span[0] += prompt_len\n",
    "                    span[1] += prompt_len\n",
    "                    input[1] = span\n",
    "                sentences[i] = (prompt + input[0], span)\n",
    "\n",
    "            # Some models (e.g. INSTRUCTOR, GRIT) require removing the prompt before pooling\n",
    "            # Tracking the prompt length allow us to remove the prompt during pooling\n",
    "            tokenized_prompt = self.tokenize([prompt])\n",
    "            if \"input_ids\" in tokenized_prompt:\n",
    "                extra_features[\"prompt_length\"] = tokenized_prompt[\"input_ids\"].shape[-1] - 1\n",
    "\n",
    "        if device is None:\n",
    "            device = self.device\n",
    "\n",
    "        self.to(device)\n",
    "\n",
    "        all_embeddings = []\n",
    "        length_sorted_idx = np.argsort([-len(input[0]) for input in sentences])\n",
    "        sentences_sorted = [sentences[idx] for idx in length_sorted_idx]\n",
    "\n",
    "        for start_index in trange(0, len(sentences), batch_size, desc=\"Batches\", disable=not show_progress_bar):\n",
    "            sentences_batch = sentences_sorted[start_index : start_index + batch_size]\n",
    "            features = self.tokenize(sentences_batch)\n",
    "            if self.device.type == \"hpu\":\n",
    "                if \"input_ids\" in features:\n",
    "                    curr_tokenize_len = features[\"input_ids\"].shape\n",
    "                    additional_pad_len = 2 ** math.ceil(math.log2(curr_tokenize_len[1])) - curr_tokenize_len[1]\n",
    "                    features[\"input_ids\"] = torch.cat(\n",
    "                        (\n",
    "                            features[\"input_ids\"],\n",
    "                            torch.ones((curr_tokenize_len[0], additional_pad_len), dtype=torch.int8),\n",
    "                        ),\n",
    "                        -1,\n",
    "                    )\n",
    "                    features[\"attention_mask\"] = torch.cat(\n",
    "                        (\n",
    "                            features[\"attention_mask\"],\n",
    "                            torch.zeros((curr_tokenize_len[0], additional_pad_len), dtype=torch.int8),\n",
    "                        ),\n",
    "                        -1,\n",
    "                    )\n",
    "                    features[\"span_mask\"] = torch.cat(\n",
    "                        (\n",
    "                            features[\"span_mask\"],\n",
    "                            torch.zeros((curr_tokenize_len[0], additional_pad_len), dtype=torch.int8),\n",
    "                        ),\n",
    "                        -1,\n",
    "                    )\n",
    "                    if \"token_type_ids\" in features:\n",
    "                        features[\"token_type_ids\"] = torch.cat(\n",
    "                            (\n",
    "                                features[\"token_type_ids\"],\n",
    "                                torch.zeros((curr_tokenize_len[0], additional_pad_len), dtype=torch.int8),\n",
    "                            ),\n",
    "                            -1,\n",
    "                        )\n",
    "\n",
    "            features = batch_to_device(features, device)\n",
    "            features.update(extra_features)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                out_features = self.forward(features, **kwargs)\n",
    "                if self.device.type == \"hpu\":\n",
    "                    out_features = copy.deepcopy(out_features)\n",
    "\n",
    "                out_features[\"sentence_embedding\"] = truncate_embeddings(\n",
    "                    out_features[\"sentence_embedding\"], self.truncate_dim\n",
    "                )\n",
    "\n",
    "                if output_value == \"token_embeddings\":\n",
    "                    # TODO: maybe make NotImplementedError\n",
    "                    embeddings = []\n",
    "                    for token_emb, attention in zip(out_features[output_value], out_features[\"attention_mask\"]):\n",
    "                        last_mask_id = len(attention) - 1\n",
    "                        while last_mask_id > 0 and attention[last_mask_id].item() == 0:\n",
    "                            last_mask_id -= 1\n",
    "\n",
    "                        embeddings.append(token_emb[0 : last_mask_id + 1])\n",
    "                elif output_value is None:  # Return all outputs\n",
    "                    embeddings = []\n",
    "                    for sent_idx in range(len(out_features[\"sentence_embedding\"])):\n",
    "                        row = {name: out_features[name][sent_idx] for name in out_features}\n",
    "                        embeddings.append(row)\n",
    "                else:  # Sentence embeddings\n",
    "                    embeddings = out_features[output_value]\n",
    "                    embeddings = embeddings.detach()\n",
    "                    if normalize_embeddings:\n",
    "                        embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n",
    "\n",
    "                    # fixes for #522 and #487 to avoid oom problems on gpu with large datasets\n",
    "                    if convert_to_numpy:\n",
    "                        embeddings = embeddings.cpu()\n",
    "\n",
    "                all_embeddings.extend(embeddings)\n",
    "\n",
    "        all_embeddings = [all_embeddings[idx] for idx in np.argsort(length_sorted_idx)]\n",
    "\n",
    "        if precision and precision != \"float32\":\n",
    "            all_embeddings = quantize_embeddings(all_embeddings, precision=precision)\n",
    "\n",
    "        if convert_to_tensor:\n",
    "            if len(all_embeddings):\n",
    "                if isinstance(all_embeddings, np.ndarray):\n",
    "                    all_embeddings = torch.from_numpy(all_embeddings)\n",
    "                else:\n",
    "                    all_embeddings = torch.stack(all_embeddings)\n",
    "            else:\n",
    "                all_embeddings = torch.Tensor()\n",
    "        elif convert_to_numpy:\n",
    "            if not isinstance(all_embeddings, np.ndarray):\n",
    "                if all_embeddings and all_embeddings[0].dtype == torch.bfloat16:\n",
    "                    all_embeddings = np.asarray([emb.float().numpy() for emb in all_embeddings])\n",
    "                else:\n",
    "                    all_embeddings = np.asarray([emb.numpy() for emb in all_embeddings])\n",
    "        elif isinstance(all_embeddings, np.ndarray):\n",
    "            all_embeddings = [torch.from_numpy(embedding) for embedding in all_embeddings]\n",
    "\n",
    "        if single_input:\n",
    "            all_embeddings = all_embeddings[0]\n",
    "\n",
    "        return all_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = 'all-mpnet-base-v2'\n",
    "model = SentenceTransformerForSpanEmbedding(MODEL, device='mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    ('This sentence mentions a group.', 'a group'),\n",
    "    ('This sentence mentions another group.', 'another group'),\n",
    "    ('Multiple groups are mentioned in this sentence.', 'Multiple groups'),\n",
    "]\n",
    "inputs = model.tokenize(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'span_mask', 'token_embeddings', 'sentence_embedding'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output = model.forward(inputs.to(model.device))\n",
    "    # print(output)\n",
    "output.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 768)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encode(texts).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerBase\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "\n",
    "from typing import Dict, List\n",
    "TokenizerOutput = Dict[str, List[int]]\n",
    "\n",
    "class SetFitDatasetForSpanClassification(TorchDataset):\n",
    "    \"\"\"SetFitDatasetForSpanClassification\n",
    "\n",
    "    A dataset for training the differentiable head on span classification.\n",
    "\n",
    "    Args:\n",
    "        x (`List[Tuple[str, Tuple[int, int]]]`):\n",
    "            A list of input data as tuples of texts and span start and end character positions that will be fed into `SetFitModel`.\n",
    "        y (`Union[List[int], List[List[int]]]`):\n",
    "            A list of input data's labels. Can be a nested list for multi-label classification.\n",
    "        tokenizer (`PreTrainedTokenizerBase`):\n",
    "            The tokenizer from `SetFitModel`'s body.\n",
    "        max_length (`int`, defaults to `32`):\n",
    "            The maximum token length a tokenizer can generate.\n",
    "            Will pad or truncate tokens when the number of tokens for a text is either smaller or larger than this value.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        x: List[Tuple[str, Tuple[int, int]]],\n",
    "        y: Union[List[int], List[List[int]]],\n",
    "        tokenizer: \"PreTrainedTokenizerBase\",\n",
    "        max_length: int = 32,\n",
    "    ) -> None:\n",
    "        assert len(x) == len(y)\n",
    "\n",
    "        # TODO (maybe): use _process_inputs to extract text and span from input\n",
    "        self.sentences, self.spans = _process_span_inputs(x)\n",
    "        self.y = y\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[TokenizerOutput, Union[int, List[int]]]:\n",
    "        label = self.y[idx]\n",
    "        \n",
    "        feature = self.tokenizer(\n",
    "            self.sentences[idx],\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=\"attention_mask\" in self.tokenizer.model_input_names,\n",
    "            return_token_type_ids=\"token_type_ids\" in self.tokenizer.model_input_names,\n",
    "            return_offsets_mapping=True, # <== added to get tokens character locations\n",
    "        )\n",
    "        # iterate over tokenized inputs and flag locations of spans\n",
    "        span = self.spans[idx]\n",
    "        span_mask = [0]*len(feature['input_ids'])\n",
    "        inside = False\n",
    "        for t, (om, am) in enumerate(zip(feature['offset_mapping'], feature['attention_mask'])):\n",
    "            if am == 0:\n",
    "                break\n",
    "            if span[0] in range(*om) and not inside:\n",
    "                span_mask[t] = 1\n",
    "                inside = True\n",
    "            elif span[1]-1 in range(*om) and inside:\n",
    "                span_mask[t] = 1\n",
    "                inside = False\n",
    "            elif inside:\n",
    "                span_mask[t] = 1\n",
    "        feature.update({'span_mask': span_mask})\n",
    "        \n",
    "        del feature['offset_mapping']\n",
    "\n",
    "        return feature, label\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        features = {input_name: [] for input_name in self.tokenizer.model_input_names + ['span_mask']}\n",
    "\n",
    "        labels = []\n",
    "        for feature, label in batch:\n",
    "            features[\"input_ids\"].append(feature[\"input_ids\"])\n",
    "            if \"attention_mask\" in features:\n",
    "                features[\"attention_mask\"].append(feature[\"attention_mask\"])\n",
    "            if \"token_type_ids\" in features:\n",
    "                features[\"token_type_ids\"].append(feature[\"token_type_ids\"])\n",
    "            if \"span_mask\" in features:\n",
    "                features[\"span_mask\"].append(feature[\"span_mask\"])\n",
    "            labels.append(label)\n",
    "\n",
    "        # convert to tensors\n",
    "        features = {k: torch.Tensor(v).int() for k, v in features.items()}\n",
    "        labels = torch.Tensor(labels)\n",
    "        labels = labels.long() if len(labels.size()) == 1 else labels.float()\n",
    "        return features, labels\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from typing import Optional\n",
    "\n",
    "class SetFitModelForSpanClassification(SetFitModel):\n",
    "    def _prepare_dataloader(\n",
    "            self,\n",
    "            x_train: List[str],\n",
    "            y_train: Union[List[int], List[List[int]]],\n",
    "            batch_size: Optional[int] = None,\n",
    "            max_length: Optional[int] = None,\n",
    "            shuffle: bool = True,\n",
    "        ) -> DataLoader:\n",
    "            max_acceptable_length = self.model_body.get_max_seq_length()\n",
    "            if max_length is None:\n",
    "                max_length = max_acceptable_length\n",
    "                logger.warning(\n",
    "                    f\"The `max_length` is `None`. Using the maximum acceptable length according to the current model body: {max_length}.\"\n",
    "                )\n",
    "\n",
    "            if max_length > max_acceptable_length:\n",
    "                logger.warning(\n",
    "                    (\n",
    "                        f\"The specified `max_length`: {max_length} is greater than the maximum length of the current model body: {max_acceptable_length}. \"\n",
    "                        f\"Using {max_acceptable_length} instead.\"\n",
    "                    )\n",
    "                )\n",
    "                max_length = max_acceptable_length\n",
    "\n",
    "            dataset = SetFitDatasetForSpanClassification(\n",
    "                x_train,\n",
    "                y_train,\n",
    "                tokenizer=self.model_body.tokenizer,\n",
    "                max_length=max_length,\n",
    "            )\n",
    "            dataloader = DataLoader(\n",
    "                dataset,\n",
    "                batch_size=batch_size,\n",
    "                collate_fn=dataset.collate_fn,\n",
    "                shuffle=shuffle,\n",
    "                pin_memory=True,\n",
    "            )\n",
    "\n",
    "            return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import losses\n",
    "from setfit.sampler import ContrastiveDataset, shuffle_combinations\n",
    "from torch.utils.data import IterableDataset\n",
    "from datasets import Dataset\n",
    "from setfit import Trainer\n",
    "from setfit.losses import SupConLoss\n",
    "\n",
    "from typing import Optional, Iterable\n",
    "\n",
    "class ContrastiveDatasetForSpanEmbedding(ContrastiveDataset):\n",
    "    \n",
    "    def __init__(\n",
    "            self, \n",
    "            sentences: List[Tuple[str, Tuple[int, int]]], \n",
    "            labels: List[Union[int, float]],\n",
    "            multilabel: bool,\n",
    "            num_iterations: Optional[None] = None,\n",
    "            sampling_strategy: str = \"oversampling\",\n",
    "            max_pairs: int = -1,\n",
    "        ):\n",
    "        IterableDataset.__init__(self)\n",
    "        self.pos_index = 0\n",
    "        self.neg_index = 0\n",
    "        self.pos_pairs = []\n",
    "        self.neg_pairs = []\n",
    "        self.sentences = [t for t, _ in sentences]\n",
    "        self.spans = [s for _, s in sentences]\n",
    "        self.labels = labels\n",
    "        self.sentence_labels = list(zip(self.sentences, self.spans, self.labels))\n",
    "        self.max_pos_or_neg = -1 if max_pairs == -1 else max_pairs // 2\n",
    "\n",
    "        if multilabel:\n",
    "            self.generate_multilabel_pairs()\n",
    "        else:\n",
    "            self.generate_pairs()\n",
    "\n",
    "        if num_iterations is not None and num_iterations > 0:\n",
    "            self.len_pos_pairs = num_iterations * len(self.sentences)\n",
    "            self.len_neg_pairs = num_iterations * len(self.sentences)\n",
    "\n",
    "        elif sampling_strategy == \"unique\":\n",
    "            self.len_pos_pairs = len(self.pos_pairs)\n",
    "            self.len_neg_pairs = len(self.neg_pairs)\n",
    "\n",
    "        elif sampling_strategy == \"undersampling\":\n",
    "            self.len_pos_pairs = min(len(self.pos_pairs), len(self.neg_pairs))\n",
    "            self.len_neg_pairs = min(len(self.pos_pairs), len(self.neg_pairs))\n",
    "\n",
    "        elif sampling_strategy == \"oversampling\":\n",
    "            self.len_pos_pairs = max(len(self.pos_pairs), len(self.neg_pairs))\n",
    "            self.len_neg_pairs = max(len(self.pos_pairs), len(self.neg_pairs))\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Invalid sampling strategy. Must be one of 'unique', 'oversampling', or 'undersampling'.\")\n",
    "\n",
    "    \n",
    "    def generate_pairs(self) -> None:\n",
    "        for (_text, _span, _label), (text, span, label) in shuffle_combinations(self.sentence_labels):\n",
    "            is_positive = _label == label\n",
    "            is_positive_full = self.max_pos_or_neg != -1 and len(self.pos_pairs) >= self.max_pos_or_neg\n",
    "            is_negative_full = self.max_pos_or_neg != -1 and len(self.neg_pairs) >= self.max_pos_or_neg\n",
    "\n",
    "            if is_positive:\n",
    "                if not is_positive_full:\n",
    "                    self.pos_pairs.append({\"sentence_1\": _text, \"span_1\": _span, \"sentence_2\": text, \"span_2\": span, \"label\": 1.0})\n",
    "            elif not is_negative_full:\n",
    "                self.neg_pairs.append({\"sentence_1\": _text, \"span_1\": _span, \"sentence_2\": text, \"span_2\": span, \"label\": 0.0})\n",
    "\n",
    "            if is_positive_full and is_negative_full:\n",
    "                break\n",
    "    \n",
    "    def generate_multilabel_pairs(self) -> None:\n",
    "        for (_text, _span, _label), (text, span, label) in shuffle_combinations(self.sentence_labels):\n",
    "            # logical_and checks if labels are both set for each class\n",
    "            is_positive = any(np.logical_and(_label, label))\n",
    "            is_positive_full = self.max_pos_or_neg != -1 and len(self.pos_pairs) >= self.max_pos_or_neg\n",
    "            is_negative_full = self.max_pos_or_neg != -1 and len(self.neg_pairs) >= self.max_pos_or_neg\n",
    "\n",
    "            if is_positive:\n",
    "                if not is_positive_full:\n",
    "                    self.pos_pairs.append({\"sentence_1\": _text, \"span_1\": _span, \"sentence_2\": text, \"span_2\": span, \"label\": 1.0})\n",
    "            elif not is_negative_full:\n",
    "                self.neg_pairs.append({\"sentence_1\": _text, \"span_1\": _span, \"sentence_2\": text, \"span_2\": span, \"label\": 0.0})\n",
    "\n",
    "            if is_positive_full and is_negative_full:\n",
    "                break\n",
    "\n",
    "from setfit.trainer import ColumnMappingMixin, BCSentenceTransformersTrainer\n",
    "\n",
    "class SpanColumnMappingMixin(ColumnMappingMixin):\n",
    "    _REQUIRED_COLUMNS = {\"text\", \"span\", \"label\"}\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from sentence_transformers.data_collator import SentenceTransformerDataCollator\n",
    "from typing import Any\n",
    "\n",
    "@dataclass\n",
    "class SentenceTransformerDataCollatorForSpanClassification(SentenceTransformerDataCollator):\n",
    "\n",
    "    required_features: list[str] = field(default_factory=lambda: [\"sentence_1\", \"span_1\", \"sentence_2\", \"span_2\", \"label\"])\n",
    "    \n",
    "    def __call__(self, features: list[dict[str, Any]]) -> dict[str, torch.Tensor]:\n",
    "        column_names = list(features[0].keys())\n",
    "        \n",
    "        # TODO: implement this sanity check\n",
    "        # if tuple(column_names) not in self.required_features:\n",
    "        #     raise ValueError(\n",
    "        #         f\"Column names must be {self.required_features}. Got {column_names}.\"\n",
    "        #     )\n",
    "\n",
    "        # We should always be able to return a loss, label or not:\n",
    "        batch = {}\n",
    "\n",
    "        if \"dataset_name\" in column_names:\n",
    "            column_names.remove(\"dataset_name\")\n",
    "            batch[\"dataset_name\"] = features[0][\"dataset_name\"]\n",
    "\n",
    "        # if tuple(column_names) not in self._warned_columns:\n",
    "        #     self.maybe_warn_about_column_order(column_names)\n",
    "\n",
    "        # Extract the label column if it exists\n",
    "        for label_column in self.valid_label_columns:\n",
    "            if label_column in column_names:\n",
    "                batch[\"label\"] = torch.tensor([row[label_column] for row in features])\n",
    "                column_names.remove(label_column)\n",
    "                break\n",
    "\n",
    "        # # Extract the feature columns\n",
    "        # for column_name in column_names:\n",
    "        #     tokenized = self.tokenize_fn([row[column_name] for row in features])\n",
    "        #     for key, value in tokenized.items():\n",
    "        #         batch[f\"{column_name}_{key}\"] = value\n",
    "        \n",
    "        for idx in [1, 2]:\n",
    "            inputs = [(row[f'sentence_{idx}'], row[f'span_{idx}']) for row in features]\n",
    "            tokenized = self.tokenize_fn(inputs)\n",
    "            for key, value in tokenized.items():\n",
    "                batch[f\"sentence_{idx}_{key}\"] = value\n",
    "        \n",
    "        return batch\n",
    "        \n",
    "\n",
    "\n",
    "from setfit.model_card import ModelCardCallback\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import evaluate\n",
    "\n",
    "class TrainerForSpanClassification(Trainer, SpanColumnMappingMixin):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        \n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "        # capture the callbacks\n",
    "        callbacks = callbacks = kwargs.get(\"callbacks\", None)\n",
    "        callbacks = callbacks + [ModelCardCallback(self)] if callbacks else [ModelCardCallback(self)]\n",
    "\n",
    "        # re-init the sentence transformer trainer (used when calling `self.train_embeddings()``)\n",
    "        self.st_trainer = BCSentenceTransformersTrainer(\n",
    "            setfit_model=self.model,\n",
    "            setfit_args=self.args,\n",
    "            callbacks=callbacks,\n",
    "            data_collator=SentenceTransformerDataCollatorForSpanClassification(tokenize_fn=self.model.model_body.tokenize),\n",
    "        )\n",
    "    \n",
    "    def _dataset_format_inputs(self, dataset: Dataset) -> List[Tuple[str, Tuple[int, int]]]:\n",
    "        texts = dataset['text']\n",
    "        spans = dataset['span']\n",
    "        # spans = [tuple(span) if isinstance(span, list) else span for span in spans]\n",
    "        return list(zip(texts, spans))\n",
    "    \n",
    "    def dataset_to_parameters(self, dataset: Dataset) -> List[Iterable]:        \n",
    "        return [ self._dataset_format_inputs(dataset), dataset['label'] ] \n",
    "\n",
    "    def get_dataset(\n",
    "        self, x: List[Tuple[str, Tuple[int, int]]], y: Union[List[int], List[List[int]]], args: TrainingArguments, max_pairs: int = -1\n",
    "    ) -> Tuple[Dataset, nn.Module, int, int]:\n",
    "        if args.loss in [\n",
    "            losses.BatchAllTripletLoss,\n",
    "            losses.BatchHardTripletLoss,\n",
    "            losses.BatchSemiHardTripletLoss,\n",
    "            losses.BatchHardSoftMarginTripletLoss,\n",
    "            SupConLoss,\n",
    "        ]:\n",
    "            dataset = Dataset.from_dict({\"sentence\": [d[0] for d in x], \"span\": [d[1] for d in x], \"label\": y})\n",
    "\n",
    "            if args.loss is losses.BatchHardSoftMarginTripletLoss:\n",
    "                loss = args.loss(\n",
    "                    model=self.model.model_body,\n",
    "                    distance_metric=args.distance_metric,\n",
    "                )\n",
    "            elif args.loss is SupConLoss:\n",
    "                loss = args.loss(model=self.model.model_body)\n",
    "            else:\n",
    "                loss = args.loss(\n",
    "                    model=self.model.model_body,\n",
    "                    distance_metric=args.distance_metric,\n",
    "                    margin=args.margin,\n",
    "                )\n",
    "        else:\n",
    "            data_sampler = ContrastiveDatasetForSpanEmbedding(\n",
    "                x,\n",
    "                y,\n",
    "                self.model.multi_target_strategy,\n",
    "                args.num_iterations,\n",
    "                args.sampling_strategy,\n",
    "                max_pairs=max_pairs,\n",
    "            )\n",
    "            dataset = Dataset.from_list(list(data_sampler))\n",
    "            loss = args.loss(self.model.model_body)\n",
    "\n",
    "        return dataset, loss\n",
    "    \n",
    "    def evaluate(self, dataset: Optional[Dataset] = None, metric_key_prefix: str = \"test\") -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Computes the metrics for a given classifier.\n",
    "\n",
    "        Args:\n",
    "            dataset (`Dataset`, *optional*):\n",
    "                The dataset to compute the metrics on. If not provided, will use the evaluation dataset passed via\n",
    "                the `eval_dataset` argument at `Trainer` initialization.\n",
    "\n",
    "        Returns:\n",
    "            `Dict[str, float]`: The evaluation metrics.\n",
    "        \"\"\"\n",
    "\n",
    "        if dataset is not None:\n",
    "            self._validate_column_mapping(dataset)\n",
    "            if self.column_mapping is not None:\n",
    "                logger.info(\"Applying column mapping to the evaluation dataset\")\n",
    "                eval_dataset = self._apply_column_mapping(dataset, self.column_mapping)\n",
    "            else:\n",
    "                eval_dataset = dataset\n",
    "        else:\n",
    "            eval_dataset = self.eval_dataset\n",
    "\n",
    "        if eval_dataset is None:\n",
    "            raise ValueError(\"No evaluation dataset provided to `Trainer.evaluate` nor the `Trainer` initialzation.\")\n",
    "\n",
    "        # NOTE: Below is the _only_ line that differs from the parent class\n",
    "        x_test = self._dataset_format_inputs(eval_dataset)\n",
    "        y_test = eval_dataset[\"label\"]\n",
    "\n",
    "        logger.info(\"***** Running evaluation *****\")\n",
    "        y_pred = self.model.predict(x_test, use_labels=False)\n",
    "        if isinstance(y_pred, torch.Tensor):\n",
    "            y_pred = y_pred.cpu()\n",
    "\n",
    "        # Normalize string outputs\n",
    "        if y_test and isinstance(y_test[0], str):\n",
    "            encoder = LabelEncoder()\n",
    "            encoder.fit(list(y_test) + list(y_pred))\n",
    "            y_test = encoder.transform(y_test)\n",
    "            y_pred = encoder.transform(y_pred)\n",
    "\n",
    "        metric_kwargs = self.metric_kwargs or {}\n",
    "        if isinstance(self.metric, str):\n",
    "            metric_config = \"multilabel\" if self.model.multi_target_strategy is not None else None\n",
    "            metric_fn = evaluate.load(self.metric, config_name=metric_config)\n",
    "\n",
    "            results = metric_fn.compute(predictions=y_pred, references=y_test, **metric_kwargs)\n",
    "\n",
    "        elif callable(self.metric):\n",
    "            results = self.metric(y_pred, y_test, **metric_kwargs)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"metric must be a string or a callable\")\n",
    "\n",
    "        if not isinstance(results, dict):\n",
    "            results = {\"metric\": results}\n",
    "        self.model.model_card_data.post_training_eval_results(\n",
    "            {f\"{metric_key_prefix}_{key}\": value for key, value in results.items()}\n",
    "        )\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from setfit import SetFitHead, SetFitModel\n",
    "from utils.setfit import SetFitHeadWithClassWeights\n",
    "\n",
    "from setfit import TrainingArguments, Trainer\n",
    "from sentence_transformers.losses import CosineSimilarityLoss\n",
    "\n",
    "from utils.metrics import *\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Mapping, Optional, Union\n",
    "from numpy._typing import NDArray\n",
    "\n",
    "get_device = lambda: 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "\n",
    "def get_class_weights(x: NDArray, multitarget: bool=False) -> NDArray:\n",
    "    if not multitarget: assert x.ndim == 1, 'if multitarget=False, x.ndim must be 1'\n",
    "    if multitarget: assert x.ndim == 2, 'if multitarget=True, x.ndim must be 2'\n",
    "\n",
    "    if multitarget:\n",
    "        # assume that multitarget feature indicators can only be True/False, i.e., 0/1\n",
    "        w = x.sum()/x.sum(axis=0)\n",
    "        w /= w.sum()\n",
    "        return w\n",
    "    else:\n",
    "        _, cnts = np.unique(x, return_counts=True)\n",
    "        w = sum(cnts)/cnts\n",
    "        w /= w.sum()\n",
    "        return w\n",
    "\n",
    "def model_init(\n",
    "        model_name: str,\n",
    "        id2label: Mapping[int, str], \n",
    "        multitarget_strategy: Optional[str]=None, \n",
    "        use_span_embedding: bool=True, # !!!\n",
    "        class_weights: Optional[NDArray]=None,\n",
    "        device: Optional[Union[str, torch.device]]=None\n",
    "    ) -> \"SetFitModel\":\n",
    "    if class_weights is not None:\n",
    "        assert len(id2label) == len(class_weights), 'len(id2label) must equal len(class_weights)'\n",
    "    \n",
    "    if device is None:\n",
    "        device = get_device()\n",
    "    \n",
    "    body = SentenceTransformerForSpanEmbedding(model_name, device='cpu') if use_span_embedding else SentenceTransformer(model_name, device='cpu')\n",
    "    \n",
    "    head_kwargs = dict(\n",
    "        in_features=body.get_sentence_embedding_dimension(), \n",
    "        out_features=len(id2label),\n",
    "        device='cpu',\n",
    "        multitarget=isinstance(multitarget_strategy, str),\n",
    "    )\n",
    "    if class_weights is not None:\n",
    "        head_kwargs['class_weights'] = class_weights\n",
    "        head = SetFitHeadWithClassWeights(**head_kwargs)\n",
    "    else:\n",
    "        head = SetFitHead(**head_kwargs)\n",
    "    \n",
    "    ModelClass = SetFitModelForSpanClassification if use_span_embedding else SetFitModel\n",
    "    return ModelClass(\n",
    "        model_head=head,\n",
    "        model_body=body,\n",
    "        multitarget_strategy=multitarget_strategy,\n",
    "        labels=list(id2label.values()),\n",
    "        id2label=id2label\n",
    "    ).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"sentence-transformers/paraphrase-mpnet-base-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "data_path = data_path = '../../data/annotations/group_mention_categorization/social-group-mention-categorization-coder-training'\n",
    "fp = os.path.join(data_path, 'parsed', 'consolidated_annotations.tsv')\n",
    "\n",
    "df = pd.read_csv(fp, sep='\\t')\n",
    "df.q_id.unique()\n",
    "\n",
    "# get start and end character positions of the span in the text\n",
    "df['span'] = df.apply(lambda x: regex.search(x.mention, x.text).span(), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Universal attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(fp, sep='\\t')\n",
    "tmp = df.loc[df.q_id == 'universal_attributes', ['text', 'mention', 'span', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "No       244\n",
       "Yes       56\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp[['label']].value_counts(sort=False)\n",
    "# NOTE: extreme label class imbalance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {'No': 0, 'Yes': 1}\n",
    "id2label = {0: 'No', 1: 'Yes'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp.label = tmp.label.map(label2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn, tst = train_test_split(range(len(tmp)), test_size=0.5, stratify=df.label, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols = ['input', 'label']\n",
    "cols = ['text', 'span', 'label']\n",
    "\n",
    "dataset = datasets.DatasetDict({\n",
    "    'train': datasets.Dataset.from_pandas(df.iloc[trn][cols], preserve_index=False),\n",
    "    'test': datasets.Dataset.from_pandas(df.iloc[tst][cols], preserve_index=False)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fine-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir='setfit',\n",
    "    batch_size=(32, 4),\n",
    "    num_epochs=(1, 5),\n",
    "    max_steps=100,\n",
    "    body_learning_rate=(2e-5, 1e-5),\n",
    "    head_learning_rate=1e-2,\n",
    "    end_to_end=True,\n",
    "    samples_per_label=2, # default but can be increased for TripletLoss\n",
    "    loss=CosineSimilarityLoss, # note: could use TripletLoss\n",
    "    use_amp=True,\n",
    "    report_to='none'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'No': 0.18666666666666668, 'Yes': 0.8133333333333334}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights = get_class_weights(np.array(dataset['train']['label']))\n",
    "dict(zip(id2label.values(), class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 150/150 [00:00<00:00, 18715.10 examples/s]\n",
      "Map: 100%|██████████| 150/150 [00:00<00:00, 17056.95 examples/s]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "trainer = TrainerForSpanClassification(\n",
    "    model_init=lambda: model_init(\n",
    "        model_name=MODEL,\n",
    "        use_span_embedding=True,\n",
    "        id2label=id2label,\n",
    "        class_weights=class_weights,\n",
    "        device='mps'\n",
    "    ),\n",
    "    args=args,\n",
    "    metric=compute_metrics_binary,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['test'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num unique pairs = 3200\n",
      "  Batch size = 32\n",
      "  Num epochs = 1\n",
      "  1%|          | 1/100 [00:02<03:41,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.3905, 'grad_norm': 1.5870999097824097, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 50/100 [01:13<01:11,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.209, 'grad_norm': 0.9312437772750854, 'learning_rate': 1.1111111111111113e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [02:29<00:00,  1.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.0361, 'grad_norm': 0.4403105676174164, 'learning_rate': 0.0, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [02:32<00:00,  1.52s/it]\n",
      "The `max_length` is `None`. Using the maximum acceptable length according to the current model body: 512.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 152.107, 'train_samples_per_second': 21.038, 'train_steps_per_second': 0.657, 'train_loss': 0.12432763278484345, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics = trainer.evaluate()\n",
    "# metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array(dataset['test']['label'])\n",
    "inputs = trainer._dataset_format_inputs(dataset['test'])\n",
    "y_pred = trainer.model.predict(inputs, use_labels=False).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_true, y_pred, target_names=id2label.values()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "galtan_group_appeals",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
