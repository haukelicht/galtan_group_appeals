{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "\n",
    "args = SimpleNamespace()\n",
    "\n",
    "args.input_path = './../../data/manifestos/all_manifesto_sentences_translated.tsv'\n",
    "args.id_col = 'sentence_id'\n",
    "args.text_col = 'text_mt_m2m_100_1.2b'\n",
    "args.group_by_document = True\n",
    "\n",
    "args.exclude_ids_in_files = [\n",
    "    # for first uncertainty-based sample (i.e., batch 2), add\n",
    "    '../../data/annotations/group-menion-gold-examples/sample.tsv',\n",
    "    '../../data/annotations/group-menion-coder-training/sample_round1.tsv',\n",
    "    '../../data/annotations/group-menion-coder-training/sample_round2.tsv',\n",
    "    '../../data/annotations/group-mention-annotation-batch-01/sample.tsv',\n",
    "    # for second uncertainty-based sample (i.e., batch 3), add\n",
    "    '../../data/annotations/group-mention-annotation-batch-02/sample.tsv',\n",
    "]\n",
    "\n",
    "# args.model_path = './../../results/classifiers/group-mention-detection_batch-01/best_model'\n",
    "args.model_path = './../../results/classifiers/group-mention-detection_batch-02/best_model'\n",
    "args.seed = 1234\n",
    "\n",
    "# # for first uncertainty-based sample\n",
    "# args.focal_category = None \n",
    "# for second uncertainty-based sample\n",
    "args.focal_category = None \n",
    "\n",
    "\n",
    "# # for first uncertainty-based sample (i.e., batch 2), add\n",
    "# args.sample_size = 2500\n",
    "args.sample_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#args.output_path = '../../data/annotations/group-mention-annotation-batch-02/sample.tsv'\n",
    "args.output_path = '../../data/annotations/group-mention-annotation-batch-03/sample.tsv'\n",
    "\n",
    "os.makedirs(os.path.dirname(args.output_path), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import set_seed, AutoTokenizer, AutoModelForTokenClassification, DataCollatorForTokenClassification\n",
    "set_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "import numpy as np\n",
    "from scipy import special\n",
    "\n",
    "def weighted_entropy(\n",
    "    pk: np.typing.ArrayLike,\n",
    "    w: np.typing.ArrayLike | None = None,\n",
    "    base: float | None = None,\n",
    "    axis: int = 0\n",
    ") -> np.number | np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculate the weighted entropy [1] of given distribution(s).\n",
    "\n",
    "    If only probabilities `pk` are given, the Shannon entropy [2] is calculated as\n",
    "    ``H = -sum(pk * log(pk))``.\n",
    "\n",
    "    If probabilities `pk` and `w` are given, the weighted (Shannon) entropy is calculated as\n",
    "    ``H = -sum(pk * log(pk))``.\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pk : array_like\n",
    "        Defines the distribution. Along each axis-slice \n",
    "        of ``pk``, element ``i`` is the probability of \n",
    "        event ``i``.\n",
    "    w : array_like, optional\n",
    "        Defines the weight attributed to event ``i`` \n",
    "    base : float, optional\n",
    "        The logarithmic base to use, defaults to ``e`` (natural logarithm).\n",
    "    axis : int, optional\n",
    "        The axis along which the weighted entropy is calculated. Default is 0.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    S : {float, array_like}\n",
    "        The calculated entropy.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    Based on [1] Guiaşu ([1971](https://doi.org/10.1016/0034-4877(71)90002-4)) and adapted from scipy.stats.entropy\n",
    "\n",
    "\n",
    "    The relative entropy, ``D(pk|qk)``, quantifies the increase in the average\n",
    "    number of units of information needed per symbol if the encoding is\n",
    "    optimized for the probability distribution `qk` instead of the true\n",
    "    distribution `pk`. Informally, the relative entropy quantifies the expected\n",
    "    excess in surprise experienced if one believes the true distribution is\n",
    "    `qk` when it is actually `pk`.\n",
    "\n",
    "    A related quantity, the cross entropy ``CE(pk, qk)``, satisfies the\n",
    "    equation ``CE(pk, qk) = H(pk) + D(pk|qk)`` and can also be calculated with\n",
    "    the formula ``CE = -sum(pk * log(qk))``. It gives the average\n",
    "    number of units of information needed per symbol if an encoding is\n",
    "    optimized for the probability distribution `qk` when the true distribution\n",
    "    is `pk`. It is not computed directly by `entropy`, but it can be computed\n",
    "    using two calls to the function (see Examples).\n",
    "\n",
    "    See [2]_ for more information.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] Guiaşu, S. (1971), Weighted Entropy. Reports\n",
    "           on Mathematical Physics, 2(3): 165-179.\n",
    "           https://doi.org/10.1016/0034-4877(71)90002-4\n",
    "    .. [2] Shannon, C.E. (1948), A Mathematical Theory of Communication.\n",
    "           Bell System Technical Journal, 27: 379-423.\n",
    "           https://doi.org/10.1002/j.1538-7305.1948.tb01338.x\n",
    "\n",
    "    \"\"\"\n",
    "    if base is not None and base <= 0:\n",
    "        raise ValueError(\"`base` must be a positive number or `None`.\")\n",
    "    if base is None:\n",
    "        base = np.e\n",
    "\n",
    "    pk = np.asarray(pk)\n",
    "    # normalize\n",
    "    with np.errstate(invalid='ignore'):\n",
    "        pk = 1.0*pk / np.sum(pk, axis=axis, keepdims=True)\n",
    "\n",
    "    # construct or check weights\n",
    "    if w is None:\n",
    "        w = np.ones(pk.shape[axis])\n",
    "    else:\n",
    "        assert len(w) == pk.shape[axis], f\"Expected {pk.shape[axis]} weights, got {len(w)}\"\n",
    "        w = np.asarray(w)\n",
    "\n",
    "    # compute inner\n",
    "    s = np.multiply(pk, w)*np.emath.logn(base, pk)\n",
    "\n",
    "    # compute outer\n",
    "    S = -s.sum(axis=axis)\n",
    "\n",
    "    return S\n",
    "\n",
    "# # test\n",
    "# probs = [\n",
    "#     [0.2, 0.8], \n",
    "#     [0.9, 0.1], \n",
    "#     [0.7, 0.3]\n",
    "# ]\n",
    "# weights = [1, 1]\n",
    "# weighted_entropy(probs, w=weights, axis=1), entropy(probs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "from scipy.special import softmax\n",
    "from typing import Union\n",
    "\n",
    "\n",
    "def _compute_prediction_entropy(predictions, dataset, weights: Union[None, np.typing.NDArray]=None):\n",
    "    \"\"\"\n",
    "    Compute the entropy of the predictions for each example in the dataset.\n",
    "\n",
    "    For token classification, we get an (# tokens, # classes)-shaped array of logits (the output of the classification layer).\n",
    "    We compute the softmax of the logits (applied per toekn), and then compute the entropy of the resulting probability distribution.\n",
    "\n",
    "    Inspired by small-text's PredictionEntropy confidence-based active learning query startegy \n",
    "     see https://github.com/webis-de/small-text/blob/c78459e1b60269da1aeaa270e954961cc36d77cb/small_text/query_strategies/strategies.py#L180\n",
    "    \"\"\"\n",
    "    if weights is not None:\n",
    "        assert predictions[0].shape[1] == len(weights), f\"Expected {len(weights)} classes, got {predictions[0].shape[1]}\"\n",
    "    else:\n",
    "        weights = [1] * predictions[0].shape[1]\n",
    "\n",
    "    entropies = []\n",
    "    lengths = dataset['attention_mask'].sum(dim=1)\n",
    "    for p, l in zip(predictions, lengths):\n",
    "        logits = p[:l].numpy()\n",
    "        probs = softmax(logits, axis=1)\n",
    "        # based on https://aclanthology.org/2024.lrec-main.30.pdf\n",
    "        e = weighted_entropy(probs, w=weights, axis=1).max()\n",
    "        entropies.append(e)\n",
    "    return np.array(entropies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.6094379124341005, 0.0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# theoretical entropy limits\n",
    "n_classes = 5\n",
    "entropy([1/n_classes] * n_classes), entropy([1.0] + [0.0]*(n_classes-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5004024235381879, 0.6730116670092565)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creat example of token-level predicted class probabilities ((# tokens, # classes)-shaped array)\n",
    "a = np.array([[0.2, 0.8], [0.9, 0.1]]) # <= high certainty/low entropy\n",
    "b = np.array([[0.4, 0.6], [0.6, 0.4]]) # <= low certainty/high entropy\n",
    "\n",
    "entropy(a, axis=1).max(), entropy(b, axis=1).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def compute_prediction_entropies(\n",
    "    dataset: Dataset,\n",
    "    model: AutoModelForTokenClassification,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    batch_size: int = 32,\n",
    "    **kwargs\n",
    ") -> np.ndarray:\n",
    "    data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "    entropies = []\n",
    "    for batch in tqdm(torch.utils.data.DataLoader(dataset, batch_size=batch_size, collate_fn=data_collator)):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch.to(model.device))\n",
    "        entropies.append(_compute_prediction_entropy(outputs.logits.cpu(), batch, **kwargs))\n",
    "    entropies = np.concatenate(entropies)\n",
    "    return entropies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load and prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(args.input_path, sep='\\t', usecols=[args.id_col, args.text_col])\n",
    "df.rename(columns={args.text_col: 'text'}, inplace=True)\n",
    "df = df[~df.text.isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### remove previously seen/annotated examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude = []\n",
    "for fp in args.exclude_ids_in_files:\n",
    "    tmp = pd.read_csv(fp, sep='\\t')\n",
    "    if args.id_col not in tmp.columns:\n",
    "        tmp.rename(columns={tmp.columns[0]: args.id_col}, inplace=True)\n",
    "    exclude.append(tmp[args.id_col])\n",
    "exclude = pd.concat(exclude, ignore_index=True, sort=False).to_list()\n",
    "\n",
    "df = df[~df[args.id_col].isin(exclude)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "429548"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.sample(100, random_state=args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2173037d808c4a77bc4dfd27d908671d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/429548 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = Dataset.from_pandas(df, preserve_index=False)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.model_path, use_fasr=True, truncation=True)\n",
    "tokenize = lambda examples: tokenizer(examples['text'], padding=False, truncation=True)\n",
    "\n",
    "dataset = dataset.map(tokenize, batched=True)\n",
    "dataset = dataset.remove_columns([c for c in dataset.column_names if c not in ['input_ids']])\n",
    "dataset.set_format(type='torch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate labeling uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(args.model_path, device_map='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'O',\n",
       " 1: 'I-social group',\n",
       " 2: 'I-organizational group',\n",
       " 3: 'B-social group',\n",
       " 4: 'B-organizational group'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for second uncertainty-based sample, upweigh 'organizational group' type\n",
    "w = [1, 1, 2, 1, 2] # [1]*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4b5af1bf0ff48b29b39b0e33ebfd72b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6712 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "entropies = compute_prediction_entropies(dataset, model, tokenizer, batch_size=64, weights=w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['entropy'] = entropies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample sentences from manifestos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: The sample size 1000 is not divisible by the number of groups (494). Setting the sample size to 988 (2 samples per group).\n"
     ]
    }
   ],
   "source": [
    "# if args.group_by_document:\n",
    "df['manifesto_id'] = df[args.id_col].str.extract(r'^(\\d+_\\d+)')\n",
    "\n",
    "# get the number of groups in the df\n",
    "n_groups = df.manifesto_id.nunique()\n",
    "# compute the sample size per group\n",
    "per_group_sample_size = args.sample_size // n_groups\n",
    "\n",
    "if per_group_sample_size*n_groups != args.sample_size:\n",
    "    print(\n",
    "        f\"Warning: The sample size {args.sample_size} is not divisible by the number of groups ({n_groups}).\",\n",
    "        f\"Setting the sample size to {per_group_sample_size*n_groups} ({per_group_sample_size} samples per group).\"\n",
    "    )\n",
    "\n",
    "# get the `per_group_sample_size` with the highest entropy within group\n",
    "sample = df.sort_values('entropy', ascending=False).groupby('manifesto_id').head(per_group_sample_size)\n",
    "\n",
    "# reshuffle the sample\n",
    "sample = sample.sample(frac=1, random_state=args.seed).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PearsonRResult(statistic=-0.3066503503678775, pvalue=0.0)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# correlate the entropy with the length of the text\n",
    "from scipy.stats import pearsonr\n",
    "pearsonr(df['entropy'], df['text'].str.len())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write sample to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample[['manifesto_id', 'sentence_id', 'text']].to_csv(args.output_path, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample['label'] = [[]] * len(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sample[['sentence_id', 'text', 'label']].to_dict(orient='records')\n",
    "from utils.io import write_jsonlines\n",
    "write_jsonlines(lines, args.output_path.replace('.tsv', '.manifest'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "parl_speech_actors",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
