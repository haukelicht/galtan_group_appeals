{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e03d5358",
   "metadata": {},
   "source": [
    "# placeholder {.hidden}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f648db",
   "metadata": {},
   "source": [
    "# Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7f0161",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "import os\n",
    "from pathlib import Path\n",
    "import gc\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "base_path = Path.cwd().parent.parent\n",
    "\n",
    "import sys\n",
    "sys.path.append(str(base_path / 'paper'))\n",
    "\n",
    "from reporting import *\n",
    "\n",
    "from IPython.display import HTML\n",
    "latex_table = lambda x, *args, **kwargs: HTML(x.to_html(**kwargs))\n",
    "\n",
    "# Define base directory (paper/secs -> go up two levels to project root)\n",
    "data_path = base_path / 'data'\n",
    "labeled_path = data_path / 'labeled'\n",
    "intermediate_path = data_path / 'intermediate'\n",
    "manifestos_path = data_path / 'manifestos'\n",
    "annotations_path = data_path / 'annotations' # / 'group_mention_categorization'\n",
    "results_path = base_path / 'results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cad5cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| cache: true\n",
    "fp = manifestos_path / 'all_manifesto_sentences.tsv'\n",
    "sentences_df = pd.read_csv(fp, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57716f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_df.sentence_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc5bcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| cache: true\n",
    "fp = labeled_path / 'labeled_mentions_with__party_metadata.pkl'\n",
    "df = pd.read_pickle(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba156de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.country_iso3c.nunique()\n",
    "df.year.describe().loc[[\"min\", \"max\"]]\n",
    "df.manifesto_id.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2568580a",
   "metadata": {},
   "source": [
    "To demonstrate the scalability of our approach, we apply automated text analysis methods to, first, extract social group mentions from party manifestos, and second, classify these mentions according to the attribute categories they feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75b3e86",
   "metadata": {},
   "source": [
    "## Case selection and data\n",
    "\n",
    "We focus our application on the social group mentions of populist radical-right (PRR) and Green parties and have compiled a corpus of the party manifestos these parties in 36 Western countries between 1966 and 2021.\n",
    "The contrasting ideological profiles of the PRR and Green party families should provide for wide range of group references and therefore a  broad coverage of attribute categories. <!-- Further, this focus complements existing work on mainstream parties. -->\n",
    "However, to allow for comparisons to mainstream parties, we have also included the party manifestos of mainstream center-left/social democratic and center-right/conservative parties in four selected countries (Germany, Sweden, United Kingdom, and the United States).\n",
    "@fig-cases_overview shows and overview of the cases included in our application.\n",
    "\n",
    "We have obtained the texts of party manifestos from secondary sources[^fn:manifesto_sources] and filled gaps through original data collection whenever possible.\n",
    "We created sentence-level data from raw texts through automatic sentence segmentation[^fn:sentence_segmentation] and machine-translated all sentences into English using the open-weights M2M model [@fan_beyond_2020]. <!--by Helsinki NLP [@tiedemann_opus-mt_2020; tiedemann_democratizing_2024].-->\n",
    "In total, we processed 495 party manifestos into a corpus comprising 436,984 sentences.\n",
    "\n",
    "[^fn:manifesto_sources]: The _Manifesto Project Dataset_ [@lehmann_manifesto_2023] and PoliDoc [@benoit_challenges_2009].\n",
    "[^fn:sentence_segmentation]: Using the `stanza` library [@qi_stanza_2020]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2e7f84",
   "metadata": {},
   "source": [
    "## Measurement\n",
    "\n",
    "We proceeded in two steps to quantify the attributes used in social group mentions in this corpus.\n",
    "We first identified social group mentions in manifesto sentences, applying the methods described in @licht_detecting_2025.\n",
    "We then labeled the extracted social group mentions by classifying the attributes they contain with a custom multi-label classification approach.\n",
    "In each step, we collected manual annotations for a sample of texts to produce labeled data for (few-shot) supervised machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8044c747",
   "metadata": {},
   "source": [
    "### Social group mention detection\n",
    "\n",
    "Studying what types of attributes political parties emphasize when they mention social groups in their manifestos presupposes data that records the verbatim social groups mentions contained in each manifesto sentence (if any) [@licht_detecting_2025; @horne_using_2025].\n",
    "Given the broad coverage of our case selection, we could neither rely on full manual annotation nor on existing labeled data or pre-trained classifiers.\n",
    "\n",
    "Accordingly, we produced the labeled data necessary for our application following the approach proposed by @licht_detecting_2025.\n",
    "Their approach combines sequence annotation (i.e., marking relevant phrases in sentences) and supervised token classification to extract any verbatim words or phrases used in a sentence to refer to social groups.\n",
    "\n",
    "<!-- ### Annotation, classifier training, and evaluatio -->\n",
    "\n",
    "<!-- TODO: discuss whether to mention social group / organizational group distinction  -->\n",
    "\n",
    "In particular, we proceeded in three annotation rounds, building on an active learning-like logic <!-- @miller_active_2020: https://doi.org/10.1017/pan.2020.4 --> with the goal of maximizing the reliability and generalization of supervised classifier trained on the collected annotations. \n",
    "In the first round, we applied a informativeness-based sampling [@kaufman_selecting_2024] to maximally diversify the selection of examples distributed for annotation, selecting 4,454 sentences for annotation (stratifying by manifesto, i.e., party and election). <!-- see ./code/sampling/sample_group_mention_annotation_batch_01.ipynb -->\n",
    "To manually annotate social group mentions in this sample, we recruited a research assistant (RA) that already had experience with this annotation task from prior projects and proved very reliable.[^fn:mention_detection_ra].\n",
    "To prepare the second annotation round, we used the RA's annotations from round one to train a preliminary token classifier, applied it to the remaining sentences in our corpus, and computed classification uncertainty for each sentence based on the predicted probabilities of the preliminary classifier.\n",
    "We then sampled 2,472 sentences (again, stratifying by manifesto) with high prediction uncertainty for annotation for the second round. <!-- see ./code/mention-detection/sample_most_uncertain.ipynb -->\n",
    "Importantly, this sampling strategy allowed us to progressively focus the human annotation effort on difficult cases. <!-- [cf. @licht_measuring_2025]. -->\n",
    "We repeated this process one more time -- annotation, classifier training, uncertainty-based sampling -- to sample another 987 sentences for annotation in a third round. <!-- see ./code/mention-detection/sample_most_uncertain.ipynb -->\n",
    "\n",
    "\n",
    "[^fn:mention_detection_ra]:\n",
    "    In addition to being qualified for the task through prior experience, they received detailed coding instructions explaining the concept of social group mentions with definitions and examples and we performed two rounds of training with the RA using 231 respectively 238 sentences sampled from our target corpus.\n",
    "    This allowed us to assert the coder's ability to identify social group mentions in the target data and provide them with feedback.\n",
    "    <!--\n",
    "    see \n",
    "    - data/annotations/group_mention_detection/group-menion-coder-training/annotations/sample_round1_emarie.jsonl (N=231)\n",
    "    - data/annotations/group_mention_detection/group-menion-coder-training/annotations/sample_round1_emarie.jsonl (N=238)\n",
    "    -->\n",
    "\n",
    "We combined all sentences annotated in these three rounds, set aside 15% of sentences for evaluation, and trained a token classifier using the same model architecture and training procedure as described in @licht_detecting_2025.\n",
    "The final classifier trained on our data achieved a span-level F1[^fn:seqeval] of 0.927 and a sentence-level F1 of 0.981 in held-out sentences. <!--, at least matching the performance reported by @licht_detecting_2025 on a similar task and dataset.-->\n",
    "\n",
    "[^fn:seqeval]: The span-level F1 is 0.831 when only considering exact span matches [as per the strict `seqeval` metric, cf. @licht_detecting_2025].\n",
    "\n",
    "<!-- We applied the social group mention classifier described in the previous section to all sentences in our corpus to predict which social group mentions each sentence contain (if any). -->\n",
    "<!-- We then sampled group mentions from sentences with at least one predicted social group mention for annotation of the attributes contained in these mentions. -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f88c10b",
   "metadata": {},
   "source": [
    "### Attribute classification\n",
    "\n",
    "To classify the attributes contained in the social group mentions identified in the previous step, we collected multilabel annotations for a sample of mentions extracted by our supervised mention detection classifier.\n",
    "\n",
    "#### Annotation\n",
    "\n",
    "We recruited two RAs to annotate the attributes contained in the social group mentions identified in the previous step.\n",
    "Our coding instrument showed one mention at a time in its respective sentence context, first asked whether the group mention qualifies as \"universal\" group reference, and, if not, prompted the annotator to select all veritical and horiztonal attributes conained in the focal group mention.[^fn:coding_instrument]\n",
    "While a mention's classification as \"universal\" ruled out economic and non-economic attributes classification, non-universal mentions could be labeled with one or more of the available attribute categories, resulting in multi-label annotations [@erlich_multi-label_2022].\n",
    "\n",
    "[^fn:coding_instrument]: Please refere to @sec-coding_instrument for a detailed description of our coding instrument.\n",
    "\n",
    "<!-- `TODO: add a couple of meta sentences that explain the trade-off between intercoder reliability and choosing difficult examples, and the impact of strategic vs. random sampling on label class balance and how our three rounds of annotation reflect this; this will make it all appear less ad hoc; also, it mirrors what we did for group mention annotation.` -->\n",
    "Based on prior empirical work on parties' group focus [e.g., @thau_how_2019; @huber_beyond_2021; @dolinsky_who_2025], we expected that some social attribute categories are much less prevalent in social group mentions than others.\n",
    "We therefore again proceeded in several annotation rounds to hedge the risk of label class imbalance in our group attributes classification annotations and diversify our sample of annotated examples.[^fn:label_class_imbalance]\n",
    "We describe these steps in detail in the Supporting Materials and highlight the main points here.\n",
    "Our first annotation rounds focused on sampling diverse examples, tackling difficult cases, and balancing attributes' prevalence in the annotated data.\n",
    "Inter-annotator agreement (ICA) was overall very high in these rounds and for most attribute categories, except in those rounds that focused on difficult examples and low-prevalence attribute categories for which estimating ICA is difficult (see @tbl-ica_overall and @tbl-ica_cats).\n",
    "To further improve the quality of our annotations, the author team arbitrated cases with disagreeing annotations in each round.\n",
    "Further they implemented a conceptually-driven annotation consolidation round and reviewed cases where an ensemble of classifiers tended to disagree with the then current annotations.\n",
    "This process resulted in consolidated multi-label attribute annotations for 600 mention-in-context examples.\n",
    "\n",
    "[^fn:label_class_imbalance]:\n",
    "    Label class imbalance is a common problem in social science classification tasks that can harm classifiers' predictive performance.\n",
    "    Iteratively sampling texts can be an effective strategy to mitigate this harm. <!-- [cf. @licht_measuring_2025; @erhardt_, @miller_active_2020].-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b1a2a9",
   "metadata": {},
   "source": [
    "#### Automation\n",
    "\n",
    "The collected annotations only covered a small fraction of the 209,351 (predicted) social group mentions in our corpus, however.\n",
    "Classifying which attributes are contained in each of these mentions thus required automating this classification task.\n",
    "This proved practically challenging.\n",
    "The high per-example effort required for full multi-label annotation during human annotation stressed our fixed annotation budget, so we could only annotate a 600 mention-in-context examples.\n",
    "<!-- During annotation, it requires annotators to make multiple binary judgments for each example. -->\n",
    "<!-- This raises the per-example annotation effort and thus creates a trade-off between full multilabel annotation and the total number of examples that can be annotated given a fixed annotation budget. -->\n",
    "Yet, supervised machine learning for multi-label classification from few examples with many label classes and label class imbalance is a difficult problem [cf. @erlich_multi-label_2022].\n",
    "\n",
    "<!-- In a next step, we used the attribute annotations to scale this measurement step to all of the 209,351 (predicted) mention-in-context examples in our corpus. -->\n",
    "<!-- While conceptually appealing to capture attribute intersectionality in group mentions, automating multilabel classification . -->\n",
    "<!-- This is mainly because multilabel annotation requires that annotators make multiple binary judgments for each example which raises the per-example effort and thus limits the total number of examples that can be annotated given a fixed annotation budget. -->\n",
    "\n",
    "The current computational text analysis literature offers two approaches to address this problem: few-shot finetuning with \"small\" transformer encoder-only models and few-shot in-context learning [@brown_language_2020] with large decoder-only language models (LLMs).\n",
    "We opted for the first option because it is more compute-efficient and has been shown to be effective [cf. @tunstall_efficient_2022; @laurer_less_2024; @burnham_political_2025], and leave the second option for future work.\n",
    "\n",
    "Specifically, we opted for the few-shot sentence transformer finetuning (SetFit) approach proposed by @tunstall_efficient_2022.\n",
    "In this framework, the labeled examples in the training set are first used to construct pairs for contrastive embedding model finetuning.\n",
    "In our application, this effectively specializes a general-purpuse sentence embedding model to represent social group mentions.\n",
    "In the second step, the finetuned embeddings serve as input to a classification head and the embedding model plus the classification head are trained end-to-end for multilabel classifiction of the training examples.[^fn:SetFit_advantage]\n",
    "\n",
    "[^fn:SetFit_advantage]:\n",
    "    @tunstall_efficient_2022 demonstrate that this two-pronged finetuning strategy enables impressively effective few-shot learning.\n",
    "    A further advantage of SetFit is that it is very efficient at prediction time in contrast to NLI commonly applied for few-shot supervised classification in political science [@laurer_less_2024; @burnham_political_2025]. <!--[^fn:versus_nli]-->\n",
    "    <!-- [^fn:versus_nli]: NLI relies on sentence pair classification. Each label class is verabalized as a hypothesis. To predict the labels of an example using NLI, its text is combine with each of the hypotheses for binary classifiction (true/false). With 18 label classes (or 33 in Horne et al.), this creates a large computation overhead because 18 inferences are needed to fully label a given input example. Our approach, with two classifiers -- one for economic, one for non-economic attributes -- requires only two prediction passes per example, which results in an about 9-fold speed up (_ceteris paribus_). -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9d666a",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits_path = annotations_path / 'group_mention_categorization' / 'splits' / 'model_selection'\n",
    "\n",
    "tmp = pd.concat({\n",
    "    split_file.parent.name: pd.read_pickle(split_file)\n",
    "    for split_file in splits_path.glob(\"**/train.pkl\")\n",
    "}).reset_index(level=0, names=['fold'])\n",
    "tmp.rename(columns={'economic__education_level': 'economic__education'}, inplace=True)\n",
    "train_set_prevalences = tmp[label_cols].mean().astype(float).rename(index=attribute_category_names_map)\n",
    "train_set_prevalences = train_set_prevalences.to_frame('prevalence').reset_index(names=['category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc37eb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = \"nomic-ai--modernbert-embed-base\"\n",
    "model = \"sentence-transformers--all-mpnet-base-v2\"\n",
    "tasks = [\n",
    "    \"economic_attributes_classification\",\n",
    "    \"noneconomic_attributes_classification\"\n",
    "]\n",
    "\n",
    "discard_these = [\"samples avg\"] # \"macro avg\", \n",
    "metrics = ['f1-score', 'precision', 'recall']\n",
    "\n",
    "idx_map = {\n",
    "    'micro avg': 'micro average', \n",
    "    'weighted avg': 'weighted average',\n",
    "    'macro avg': 'macro average', \n",
    "    **attribute_category_names_map\n",
    "}\n",
    "\n",
    "eval_res = {}\n",
    "eval_res_tabs = {}\n",
    "\n",
    "for task in tasks:\n",
    "#task = tasks[0]\n",
    "    results_dir = results_path / 'classifiers' / task  / \"hp_search\" / \"setfit\" / model / 'mention_text'\n",
    "    res = pd.concat({\n",
    "        fp.parts[-5:-1]: pd.read_json(fp).T.reset_index(level=0, names=\"what\") \n",
    "        for fp in results_dir.glob(\"**/eval_results.json\")\n",
    "    })\n",
    "    res.reset_index(level=[0,1,2,3], names=[\"method\", \"model_name\", \"strategy\", \"fold\"], inplace=True)\n",
    "    res = res.query(\"what not in @discard_these\").copy()\n",
    "    res[\"what\"] = res[\"what\"].replace({'economic__education_level': 'economic__education'})\n",
    "    res[\"category\"] = res[\"what\"].replace(idx_map)\n",
    "    eval_res[task] = res\n",
    "\n",
    "    res_tab = res.groupby(\"category\", observed=True)[metrics].mean()\n",
    "    res_tab.columns = res_tab.columns.str.title()\n",
    "    res_tab = res_tab.reset_index().merge(train_set_prevalences, on=\"category\", how=\"left\").round(3)\n",
    "    res_tab['category'] = pd.Categorical(res_tab['category'], categories=list(idx_map.values()), ordered=True)\n",
    "    res_tab = res_tab.sort_values('category', ascending=True)\n",
    "    res_tab['category'] = res_tab['category'].astype(str).replace({v: rf\"\\quad \\textit{{{v}}}\" for v in attribute_category_names_map.values()})\n",
    "\n",
    "    eval_res_tabs[task] = res_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cd6b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: tbl-attribute_classifier_eval_res\n",
    "#| output: true\n",
    "#| tbl-cap: \"Evaluation results of group mention attribute classifiers. Values report the mean precision, recall, and F1-score averaged across five folds for each attribute category, along with the prevalence of each category in the training set.\"\n",
    "\n",
    "# eval_res_tab = pd.concat({\n",
    "#     fr\"\\textbf{{{d.replace('_attributes_classification', '').replace('non', 'non-')} attributes}}\": \n",
    "#     tab.replace(np.nan, '').set_index('category')\n",
    "#     for d, tab in eval_res_tabs.items()\n",
    "# })\n",
    "# eval_res_tab.index.names = [None] * len(eval_res_tab.index.names)\n",
    "tab = pd.concat([\n",
    "    pd.DataFrame({\"category\": r\"\\textbf{economic attributes}\"}, index=[0]),\n",
    "    eval_res_tabs[tasks[0]],\n",
    "    pd.DataFrame({\"category\": r\"\\textbf{non-economic attributes}\"}, index=[0]),\n",
    "    eval_res_tabs[tasks[1]],\n",
    "])\n",
    "tab.set_index('category', inplace=True)\n",
    "tab = tab.replace(np.nan, '')\n",
    "tab.index.name = None\n",
    "latex_table(tab, escape=False, index=True, multicolumn_cmidrules=[(10, 0, 4)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7ffc17",
   "metadata": {},
   "source": [
    "We adopted the SetFit approach to train two separate multi-label classifiers, one for economic attributes and one for non-economic attributes. <!--, using the same training examples for both classifiers.-->\n",
    "Since our application has not been examined in prior work, we have thoroughly evaluated several implementation choices such as the base embedding model, input formatting strategy, and hyperparameter choices, which we detail in the Supporting Materials.\n",
    "@tbl-attribute_classifier_eval_res summarizes the performance of these classifiers averaged across five held-out test folds.[^fn:attribute_classifier_eval_res]\n",
    "The economic attribute classifier achieves a macro-averaged (micro) F1 of 0.8 (0.785); the non-economic attribute classifier a macro-averaged (micro) F1 of 0.804 (0.829).\n",
    "And while classification reliability varies across attribute categories, it is overall very good, with most categories achieving an F1 of 0.75 or higher.\n",
    "We consider these strong results given the low number of few-shot training examples and strong label class imbalance [see the prevalence column, @erlich_multi-label_2022].\n",
    "In particular, the classifiers prove similarly reliable as trained human annotators (see @tbl-ica_cats) in classifying economic attributes and even more reliable in classifying non-economic attributes.\n",
    "\n",
    "[^fn:attribute_classifier_eval_res]: These are the results of models trained on folds' respective training examples using \"optimal\" hyper-parameters identified using stochastic hyperparameter grid search. Specifically, at the end of each hyper-parameter sweep on a fold's training examples, we selected the hyper-parameters that yielded the best macro-averaged F1 on the fold's validation examples, finetuned the model with these hyper-parameters, and evaluated it in the corresponding fold's test examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716908ef",
   "metadata": {},
   "source": [
    "# APPENDIX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a622aed",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506fbbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: fig-cases_overview\n",
    "#| cache: true\n",
    "#| output: true\n",
    "#| fig-cap: \"Overview of cases included in out corpus. Each square represents a party in a given year, colored by its party family.\"\n",
    "\n",
    "tmp = df[['country_iso3c', 'year', 'party_name', 'party_family']].drop_duplicates()\n",
    "tmp.party_family = tmp.party_family.map(family_map)\n",
    "\n",
    "countries = tmp['country_iso3c'].unique().tolist()\n",
    "\n",
    "heights = tmp.groupby('country_iso3c').aggregate({'party_name': 'nunique'})['party_name']\n",
    "\n",
    "n_col = 1\n",
    "fig, axes = plt.subplots(\n",
    "    len(countries)//n_col, n_col, \n",
    "    figsize=(5, heights.sum()/4), \n",
    "    height_ratios=heights.to_list(),\n",
    "    sharex=True, \n",
    "    gridspec_kw={'hspace': 0.4}\n",
    "    )\n",
    "\n",
    "axes = axes.flatten()\n",
    "\n",
    "scatter_kwargs = dict(\n",
    "    x='year', \n",
    "    y='party_name', \n",
    "    hue='party_family', \n",
    "    marker='s', \n",
    "    palette=all_fam_palette,\n",
    "    s=100, \n",
    "    legend=False\n",
    ")\n",
    "for i, (ctr, subdf) in enumerate(tmp.groupby(\"country_iso3c\")):\n",
    "    ax = axes[i]\n",
    "    sns.scatterplot(data=subdf, ax=ax, **scatter_kwargs)\n",
    "    # ax.set_ylim(1.15, -0.15)\n",
    "    ax.set_ylabel(None)\n",
    "    # add country label as y-axis label (second axis) on right hand side\n",
    "    ax.xaxis.grid(False)\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_edgecolor(None)\n",
    "        spine.set_visible(False)\n",
    "    ax_right = ax.twinx()\n",
    "    ax_right.set_ylabel(ctr, fontweight='bold', rotation=0, labelpad=15)\n",
    "    ax_right.set_yticks([])\n",
    "    # ax_right.spines['right'].set_visible(False)\n",
    "    # increase y-limits so that squares are fully visible\n",
    "    ax.set_ylim(-0.5, len(subdf['party_name'].unique()) - 0.5)\n",
    "    # make plot backgorund light gray\n",
    "    ax.set_facecolor('#f0f0f0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79b669f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(econ_attrs), len(nonecon_attrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97687c2b",
   "metadata": {},
   "source": [
    "\\clearpage\n",
    "\n",
    "# Attribute classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66ef2bf",
   "metadata": {},
   "source": [
    "## Coding instrument {#sec-coding_instrument}\n",
    "\n",
    "Our coding instructions introduce the vertical/horizontal distinction, each of the attribute categories, as well as our \"universal\" group mention category, explain the available coding choices, and give examples.\n",
    "Our coding instrument, implemented as a Qualtrix survey, presents annotators independently with one social group mention at a time (i.e., per page) in their respective sentence context, marking the mention in bold.\n",
    "\n",
    "Below the display of the sentence, the annotator is first asked to indicate whether the highlighted mention is an \"universal\" mention per our definition.\n",
    "If the annotator chose \"Yes\" for this coding dimension, our coding instructions asked them to proceed with the next instance on the next page.\n",
    "\n",
    "If not, the annotator proceeds with coding the attribute categories for the vertical and horizontal attribute dimensions in turn.\n",
    "For each of the dimensions, we tasked the annotator to indicate which of the respective attribute categories was contained in the highlighted social group mention, displaying the attribute categories below each other[^1] in a multiple-choice grid with the answer options \"Yes\" or \"Unsure\" horizontally aligned.[^2]\n",
    "This procedure results in 17 annotations[^3] per mention-in-sentence-context instance and annotator.\n",
    "<!-- TODO: consider how to handle later omission of _class membership_ and _ecology of group_ categories !? -->\n",
    "\n",
    "[^1]: We kept the categories' order fixed across examples to ease the cognitive load at annotation time.\n",
    "[^2]: We omitted options for \"No\" because not choosing \"Yes\" or \"Unsure\" for a given attribute category implied this coding choice.\n",
    "[^3]: 1$\\times$ universal + 5$\\times$ vertical attributes + 11$\\times$ horizontal attributes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c157d4a1",
   "metadata": {},
   "source": [
    "## Annotation\n",
    "\n",
    "In a first round of annotations, we have sampled 300 mentions-in-sentence-context examples from all sentences with at least one predicted social group mention, again using an informativeness-based sampling strategy.\n",
    "@tbl-ica_overall reports the micro inter-annotator agreement (ICA) by attribute dimension from this round and indicated that our coders produced overall reliable annotations.\n",
    "To resolve examples with disagreeing annotations, the authors team reviewed any mention-in-sentence-context example with at least one disagreeing annotation to determine their final labels.<!--[^5]-->\n",
    "\n",
    "\n",
    "<!-- [^5]: This lead to a few revisions of attribute classifications the annotators agreed on in annotations where RAs unanimous coding disagreed with experts' judgments.\n",
    "    We resolved them manually and updated the final annotations accordingly. -->\n",
    "\n",
    "<!-- TODO: mention when we introduced the shared values/mentalities class -->\n",
    "\n",
    "However, as expected, the prevalence of attribute categories in the labels collected in this first round turned out to be very imbalanced and we observed variation in ICA estimates across categories that could only partially be explained by low prevalence (see @tbl-ica_cats).\n",
    "We therefore dedicated a second annotation round to collecting more annotations for difficult examples.\n",
    "To this end, we fine-tuned a multilabel classifier to predict mentions-in-sentence-context examples' binary labels on the universal, vertical, horizontal indicators based on the consolidated annotations collected in the first round.<!--[^6]-->\n",
    "We applied this classifier to the mention-in-sentence-context instances not yet distributed for attribute annotation to obtain predicted probabilities.\n",
    "We then computed classification uncertainty at the example level<!--[^7]--> and selected the 150 most uncertain examples into a second annotation batch.\n",
    "\n",
    "<!-- [^6]: See section XX for the details of our few-shot fine-tuning strategy.\n",
    "    The resulting model showed already strong classification performance for this high-level task considering how few examples we distributed for annotation in the first round.\n",
    "-->\n",
    "\n",
    "<!-- [^7]: Computing classification uncertainty as minimal closeness to one of the three 0.5 classification thresholds. -->\n",
    "\n",
    "@tbl-ica_overall and @tbl-ica_cats show that, due to our focus on difficult examples, ICA was generally lower than in the first round.\n",
    "We therefore used zero-shot in-context learning [@brown_language_2020] to generate large language model (LLM) annotations for the examples in the second-round batch.<!--[^8]-->\n",
    "We then presented our annotators the instances where the LLM disagreed with their judgment and tasked them to (independently) judge which annotation they viewed as more valid while blinding them towards the source of the respective annotations.\n",
    "The author team arbitrated the cases in which our annotators' independent judgments disagreed.\n",
    "All instances from this second round were then consolidated into final labels and added to those of the first round."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70eea7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "## report reliability by round\n",
    "\n",
    "annoation_rounds = {\n",
    "    '1': 'social-group-mention-categorization-coder-training',\n",
    "    '2': 'social-group-mention-categorization-round02',\n",
    "    '3': 'social-group-mention-categorization-round03',\n",
    "}\n",
    "\n",
    "folder = annotations_path / 'group_mention_categorization'\n",
    "ica_raw = pd.concat({\n",
    "    r: pd.read_pickle(folder / f / 'parsed' / 'ica_estimates.pkl') \n",
    "    for r, f in annoation_rounds.items()\n",
    "})\n",
    "ica_raw.reset_index(level=0, names=['round'], inplace=True)\n",
    "ica_raw.loc[ica_raw.label.isna(), 'label'] = ica_raw.loc[ica_raw.label.isna(), 'q_category']\n",
    "ica_raw.loc[ica_raw.q_id=='universal_attributes', 'label'] = 'overall'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a4329e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ica = ica_raw.loc[ica_raw.q_id.str.endswith('_attributes'), ['round', 'q_id', 'label', 'prop_yes', 'krippendorff_alpha']]\n",
    "ica = ica[~ica.label.isna()]\n",
    "\n",
    "cats_map = {i.split('__')[1]: nm for i, nm in attribute_category_names_map.items()}\n",
    "ica['label'] = ica.label.replace({\"education level\": \"education\"})\n",
    "ica['category'] = ica.label.replace(cats_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cdc503",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: tbl-ica_overall\n",
    "#| output: true\n",
    "#| tbl-cap: \"Inter-coder agreement estimates for attribute classifications computed at the level of attribute dimensions: universal, economic, and non-economic. Estimates are based on Krippendorff's σ and the prevalence of 'yes' annotations (prevalence) across all annotated examples in each annotation round.\"\n",
    "ica_overall = ica.loc[ica.label == 'overall', ['round', 'q_id', 'prop_yes', 'krippendorff_alpha']]\n",
    "ica_overall = ica_overall.rename(columns={'prop_yes': 'prevalence', 'krippendorff_alpha': r\"Krippendorff's $\\alpha$\"})\n",
    "ica_overall['q_id'] = ica_overall.q_id.str.removesuffix('_attributes')\n",
    "ica_overall['q_id'] = pd.Categorical(ica_overall['q_id'], categories=['universal', 'economic', 'non-economic'], ordered=True)\n",
    "ica_overall = ica_overall.pivot_table(index='q_id', columns=['round'], observed=True).round(3)\n",
    "ica_overall.index.name = None\n",
    "ica_overall.columns.names = [None, 'annotation round']\n",
    "latex_table(ica_overall, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b2b8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: tbl-ica_cats\n",
    "#| output: true\n",
    "#| tbl-cap: \"Inter-coder agreement estimates for attribute classifications computed at the level of economic and non-economic attribute categories. Estimates are based on Krippendorff's α and values in parentheses indicate the prevalence of 'yes' annotations (prevalence) across all annotated examples in each annotation round.\"\n",
    "ica_cats = ica.loc[ica.label != 'overall', ['round', 'q_id', 'category', 'prop_yes', 'krippendorff_alpha']]\n",
    "ica_cats['category'] = pd.Categorical(ica_cats['category'], categories=list(cats_map.values()), ordered=True)\n",
    "ica_cats['value'] = ica_cats.apply(lambda r: rf\"{r['krippendorff_alpha']:+0.3f} ({r['prop_yes']*100:0.1f}\\%)\", axis=1)\n",
    "ica_cats['q_id'] = ica_cats.q_id.str.removesuffix('_attributes')\n",
    "ica_cats = ica_cats.pivot_table(index=['q_id', 'category'], columns=['round'], values=['value'], aggfunc='first', observed=False).round(3).fillna('').sort_index()\n",
    "ica_cats.columns = pd.Index(ica_cats.columns.get_level_values(1), name='annotation round')\n",
    "ica_cats.index.names = ['dimension', 'category']\n",
    "latex_table(ica_cats, index=True, resize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b300dc",
   "metadata": {},
   "source": [
    "<!-- [^8]: See SM XX for the prompts, model, and hyper-parameters we used. -->\n",
    "In a third round of annotation, we then addressed the problem of label class imbalance that clearly showed in the pooled set of multi-labeled mention-in-sentence-context instances from rounds one and two.\n",
    "In particular, we focused on over-sampling likely examples of so far under-represented attribute categories in the vertical and horizontal attribute dimensions.\n",
    "To identify likely examples for the given label categories, we used the attribute category definitions to define queries and used a pre-trained sentence embedding model to rank so far unannotated mention-in-sentence-context instances based on their cosine similarities to each of these queries.\n",
    "To oversample likely examples of so far underrepresented categories, we defined quotas for each attribute category in inverse proportion to the categories prevalence in the labeled instances from round one and two, using a annotation budget of 200 examples for round 3.\n",
    "We then chose as many of the so far unanottated mention-in-sentence-context instances as the quote prescribed in descending order of their embeddings' similarities to the embedding of the respective attribute category definition query.\n",
    "In total, this resulted in a samole of 180 mention-in-sentence-context examples for annotation in round 3.[^10]\n",
    "\n",
    "[^10]: Deviations from the annotation budget of 200 cases are explained by rounding in the computation of quotas and some mention-in-sentence-context instances that were ranked high for multiple attribute queries.\n",
    "\n",
    "@tbl-ica_overall shows that the attribute annotations of examples in this third round were overall highly reliable, which is likely explained by focusing on identifying *likely* examples of each attribute category in this final round (in contrast to difficult instances in round 2).\n",
    "Further, the consolidated labels from round 3 showed that our over-sampling strategy was effective as it helped to reduce the label class imbalance across the set of vertical and horizontal attribute categories.\n",
    "\n",
    "A last round of annotation focused on conceptual consolidation and was performed by the authors.\n",
    "Specifically, we manually reviewed any annotated examples that were labeled as combining certain attribute categories to determine whether the combination of these categories was conceptually valid and consistent with our attribute definitions.\n",
    "\n",
    "<!-- \n",
    "TODOs\n",
    "\n",
    "\n",
    "- describe concrete review startegies \n",
    "    1. concept-driven review\n",
    "    2. word-wise review\n",
    "\n",
    "- mention classifier ensemble-based review !!!\n",
    "\n",
    "note: see round5/ in code/group_mention_categorization/ \n",
    "\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfd6c2d",
   "metadata": {},
   "source": [
    "## SetFit finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7900560",
   "metadata": {},
   "source": [
    "We have first carefully split the 600 annotated examples into five training, validation and test folds to minimize leakage between the training and evaluation sets.\n",
    "This is generally a crucial step in any supervised machine learning application, but it is particularly important in our application for several reasons.\n",
    "First, simple random sampling does not account for the facts that \n",
    "(a) mentions are embedded in sentences so that the same mention can appear in multiple sentences and \n",
    "(b) some mentions are near duplicates of each other and having these in different splits can lead to overestimation of predictive performance.\n",
    "Second, in the context of few-shot learning, it is particularly important to minimize leakage between the training and evaluation sets because the small number of training examples makes it more likely that a given example in the evaluation set is very similar to one in the training set, which can lead to overestimation of predictive performance.\n",
    "\n",
    "We have then used the training and validation split to examine the average performance of different base embedding models[^fn:embedding_model_selection] and input formatting strategies[^fn:inpiut_formatting_strategies] and select the best-performing model and formatting strategy for each classifier.\n",
    "\n",
    "Finally, we have trained the two classifiers on the full training set using the selected embedding model and input formatting strategy and evaluated their performance on the held-out test set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdf9d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = annotations_path / 'group_mention_categorization' / 'final_annotations.tsv'\n",
    "annotations = pd.read_csv(fp, sep='\\t')\n",
    "annotations.mention_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2492cd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotations[annotations.mention.str.contains(\"lite\")].pivot_table(index=['mention_id', 'mention'], columns='attribute_combination', values='label', aggfunc='first').T"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "galtan_group_appeals",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
