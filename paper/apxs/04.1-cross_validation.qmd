

<!-- put in additional results section or so -->

## Cross-validation our attribute classifications against Thau's group category annotations

### Economic class

```{python}
thau_econ_class_mentions = df_thau_preds.query("group_category=='Economic class'")
thau_econ_class_mentions['labels'] = thau_econ_class_mentions.apply(lambda row: ' + '.join([l for c, l in attribute_category_names_map.items() if row[c]==1]), axis=1)
tmp = thau_econ_class_mentions.\
    groupby('labels').\
    agg({'group_type': 'count', 'mention': lambda x: list(x.sample(min(len(x), 5), random_state=1 ))}).\
    reset_index().\
    rename(columns={'group_type': 'count'}).\
    sort_values('count', ascending=False)
tmp = tmp[tmp['labels']!='']
tmp['n_attributes'] = tmp['labels'].str.count(' \+ ')+1
tmp['intersectional'] = tmp['n_attributes'] > 1
tmp['examples'] = tmp['mention'].apply(lambda ms: '; '.join([f"``{m}''" for m in ms]))
```

Having applied our multi-attribute classification approach to social group mentions categorized according to Thau's group category scheme,
we estimate that about 39% of mentions in his _economic class_ category are intersectional references.
@tbl-thau_econ_class_mentions_single_attr_examples shows that Among the 61% of single-attribute mentions in this subset, most are classified as _employment status_, _income/wealth/economic status_, or _occupation/profession_ instances in our scheme.

```{python}
#| label: tbl-thau_econ_class_mentions_multi_attr_examples
#| output: true
#| tbl-cap: 'Social group mentions in Thau''s _economic class_ that are assigned to two attribute categories according to our scheme and their absolute frequency. *Note:* Table only reports results for the six most prevalent attribute combinations.'

# NOTE: not sure whether we should interprete these numbers as relative prevalence of attribute co-occs
tab = tmp.query("n_attributes == 2").head(6)[['labels', 'count', 'examples']]
tab.columns = ["attribute combination", "$N$", "examples"]
latex_table(tab)
```

However, other social group mentions in this subset of Thau's data are often labeled as combinations among these economic attributes or with other attributes,  often non-economic ones like family, age, or place/location.
@tbl-thau_econ_class_mentions_multi_attr_examples shows, for example, that mentions in Thau's _economic class_ group category are often labeled as intersectional references featurin the attributes _income/wealth/economic status_ and _family_ or _employment status_ and _age_, respectively.

```{python}
#| eval: false
tmp.query("n_attributes > 2").head(4)[['labels', 'count', 'examples']]
```

### Family

```{python}
thau_gender_mentions = df_thau_preds.query("group_category=='Gender'")
thau_gender_mentions['labels'] = thau_gender_mentions.apply(lambda row: ' + '.join([l for c, l in attribute_category_names_map.items() if row[c]==1]), axis=1)
tmp = thau_gender_mentions.\
    groupby('labels').\
    agg({'group_type': 'count', 'mention': lambda x: list(x.sample(min(len(x), 5), random_state=1 ))}).\
    reset_index().\
    rename(columns={'group_type': 'count'}).\
    sort_values('count', ascending=False)
tmp = tmp[tmp['labels']!='']
tmp['n_attributes'] = tmp['labels'].str.count(' \+ ')+1
tmp['intersectional'] = tmp['n_attributes'] > 1
tmp['examples'] = tmp['mention'].apply(lambda ms: '; '.join([f"``{m}''" for m in ms]))
```

```{python}
#| eval: false
cnts = tmp.groupby('intersectional')['count'].sum()
props = cnts / cnts.sum()
props 
```

Thau's _gender_ group category is an example where our scheme is slightly broader because it also includes the aspect of sexual orientation.
Mentions in this subset of Thau's data taht are assigned to only one attribute in our scheme (59%) are typically  categorized as _family_ or _gender/sexuality_
instances (see @tbl-thau_gender_mentions_single_attr_examples).

```{python}
#| label: tbl-thau_gender_mentions_single_attr_examples
#| output: true
#| tbl-cap: 'Social group mentions in Thau''s _gender_ group category that are assigned to only one attribute category according to our scheme and their absolute frequency. *Note:* Table only reports the results for the six most prevalent attribute categories.'
# TODO:  make this a table
tab = tmp.query("n_attributes == 1").head(6)[['labels', 'count', 'examples']]

# NOTE: this reveals some misclassifictions
#  - "women prisoners" => also crime
#  - "boys" => also gender/sexuality
#  - "men who work in the public services" => also gender/sexuality

tab.columns = ["attribute", "$N$", "examples"]
latex_table(tab)
```

```{python}
#| label: tbl-thau_gender_mentions_multi_attr_examples
#| output: true
#| tbl-cap: 'Social group mentions in Thau''s _gender_ group category that are assigned to two attribute categories according to our scheme and their absolute frequency. *Note:* Table only reports the results for the six most prevalent attribute combinations.'
# TODO:  make this a table
tab = tmp.query("n_attributes == 2").head(6)[['labels', 'count', 'examples']]
tab.columns = ["attribute combination", "$N$", "examples"]
latex_table(tab)
```

Yet, despite our gender category is already broader than Thau's, we still find that about 41% of the group mentions classified into the _gender_ category are intersectional according to our annotations.
@tbl-thau_gender_mentions_multi_attr_examples shows that the most common combinations are _family_ and _gender/sexuality_ and _occupation/profession_ + _gender/sexuality_, respectively.
This is in parts driven by how we treat gendered references to familial roles, like father, mother, husband, wive, etc., which we consider intersectional.
Other instances where we find intersectionality are not explained by this difference in coding appraoches, however, such as mentions that feature gender/sexuality alongside occpuation/profession ("service women"),  employment status ("women part-time workers"), or nationality ("British women married to foreign husbands").

### "Other"

Another intersting point of comparison to Thau's group category classifications is mentions in his _other_ category.
Overall, about 23% of sopcial group mentions have been classified into this category by his annotators.

```{python}
#| eval: false
(df_thau_preds.query("group_type=='Social group'").group_category=='Other').mean()
```

```{python}
thau_other_mentions = df_thau_preds.query("group_category=='Other'")
thau_other_mentions['labels'] = thau_other_mentions.apply(lambda row: '; '.join([l for c, l in attribute_category_names_map.items() if row[c]==1]), axis=1)
tmp = thau_other_mentions.\
    groupby('labels').\
    agg({'group_type': 'count', 'mention': lambda x: list(x.sample(min(len(x), 5), random_state=1 ))}).\
    reset_index().\
    rename(columns={'group_type': 'count'}).\
    sort_values('count', ascending=False)
tmp['n_attributes'] = tmp['labels'].str.count('; ')+1
tmp['any_attributes'] = tmp['labels']!=''
tmp.loc[~tmp['any_attributes'], 'n_attributes'] = 0
tmp['intersectional'] = tmp['n_attributes'] > 1
tmp['examples'] = tmp['mention'].apply(lambda ms: '; '.join([f"``{m}''" for m in ms]))
```

```{python}
#| eval: false
cnts = tmp.groupby(['any_attributes', 'intersectional'])['count'].sum()
props = cnts / cnts.sum()
props 
```

```{python}
#| eval: false
cnts = tmp.query('any_attributes').groupby('intersectional')['count'].sum()
props = cnts / cnts.sum()
props 
```

Our multilabel attribute classification approach assings 81% of mentions in Thau's _other_ category to at least one attribute in our scheme.
Our approach thus makes 4 out of 5 mentions in Thau's general _other_ group analytically accesible by labeling them as featuring one (69%) or more attributes (31%) in our scheme.

```{python}
#| eval: false
tmp.query("n_attributes == 1").head(7)
```

```{python}
#| eval: false
tmp.query("n_attributes > 1").head(10)
```

## Cross-validation our attribute classifications against Horne et al.'s group category annotations

```{python}
vectorizer = CountVectorizer(
    stop_words=stopwords.words('english'),
    ngram_range=(1, 3), 
    max_df = 0.8
)

idxs = horne_predictions_econ_occprof_attribute_sample[occupation_cats].any(axis=1)
fw = compute_fighting_words(
    l1=horne_predictions_econ_occprof_attribute_sample.loc[ idxs, 'text'].to_list(),
    l2=horne_predictions_econ_occprof_attribute_sample.loc[~idxs, 'text'].to_list(),
    cv=vectorizer,
)
fw_ours_vs_horne_occupation = pd.DataFrame(fw, columns=['word', 'score']).sort_values('score', ascending=False)
```

```{python}
#| label: fig-ours_vs_horne_occupation_fighting_words
#| output: true
#| fig-cap: Most distinctive words for mentions labeled as featuring occupation/profession as an attribute by our classifier depending on whether by Horne et al.'S classifier has classified them into (at least) one of their occupuation/profession-related categories (left) or not (right). Values plotted are $z$-scores from "fighting words" on sample of 5000 occupation/profession mentions. Values above Â±1.96 (vertical dashed line) can be considered significantly distinctive.

# Get top 20 lowest (most negative) and highest (most positive) scores
top_negative = fw_ours_vs_horne_occupation.nsmallest(20, 'score').sort_values('score', ascending=False)
top_positive = fw_ours_vs_horne_occupation.nlargest(20, 'score').sort_values('score', ascending=True)

# Create two-column layout
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(11, 4), sharey=False)

# Left plot: positive scores (distinctive for classified by Horne et al.)
ttl = r"mentions categorized by both as occupation/profession-related"
ax1.axvline(x=1.96, color='black', linestyle='--', linewidth=0.8, zorder=1)
ax1.barh(range(len(top_positive)), top_positive['score'], color='#1b9e77', zorder=2)
ax1.set_yticks(range(len(top_positive)))
ax1.set_yticklabels(top_positive['word'])
ax1.set_xlabel('z-score', fontsize=11)
ax1.set_title(ttl, fontweight='bold', fontsize=12)
ax1.axvline(x=0, color='black', linestyle='-', linewidth=0.8)
ax1.yaxis.tick_right()
ax1.yaxis.set_label_position('right')
plt.setp(ax1.get_yticklabels(), ha='left')
ax1.set_xlim(0, 10)
ax1.invert_xaxis()  # Invert to show negative values extending left

# Right plot: negative scores (distinctive for NOT classified by Horne et al.)
ttl = r"mentions categorized only by ours as occupation/profession-related"
ax2.axvline(x=-1.96, color='black', linestyle='--', linewidth=0.8, zorder=1)
ax2.barh(range(len(top_negative)), top_negative['score'], color='#d95f02', zorder=2)
ax2.set_yticks(range(len(top_negative)))
ax2.set_yticklabels(top_negative['word'])
ax2.set_xlabel('z-score', fontsize=11)
ax2.set_title(ttl, fontweight='bold', fontsize=12)
ax2.set_xlim(-10, 0)
ax2.invert_xaxis()  # Invert to show negative values extending left

plt.tight_layout()
plt.show()
```

```{python}
#| label: tbl-ours_vs_horne_occupation_fighting_words_examples
#| output: true
#| tbl-cap: 'Examples of social group mentions labeled as featuring occupation/profession as an attribute by our classifier that were not assigned to any of Horne et al.''s occupation-related group categories by their classifier. Values computed by summing "fighting words" scores as weights of mentions'' tokens, normalized by number of tokens.'
fw_lookup = {r['word']: r['score'] for r in fw_ours_vs_horne_occupation.to_dict(orient='records')}
fw_vals = np.array([fw_lookup[f] for f in vectorizer.get_feature_names_out()])
analyzer = vectorizer.build_analyzer()

# vectorize mentions 
mentions = horne_predictions_econ_occprof_attribute_sample[~idxs].reset_index(drop=True)
mentions_texts = mentions['text']
X_mentions = vectorizer.transform(mentions_texts.tolist())
# binarize
X_mentions[X_mentions>0] = 1
# apply z-score values to each row in `X_mentions` as weights
X_mentions_scores = X_mentions @ fw_vals[:, np.newaxis]
# normalize for mention length
X_mentions_scores /= X_mentions.sum(axis=1)

mention_scores = X_mentions_scores[:, 0]
rank = mention_scores.argsort()#[::-1]

n_ = 20
tab = horne_predictions_econ_occprof_attribute_sample.loc[~idxs, ['text', *label_cols_horne]].iloc[rank]
tab['score'] = mention_scores[rank]
tab = tab[tab['score']<-1.96]
tab['text_norm'] = tab['text'].apply(lambda x: ' '.join(analyzer(x)).strip())
tab = tab.drop_duplicates('text_norm')# .head(n_).reset_index(drop=True)
tab = tab.sample(n_, weights=tab['score'].abs(), random_state=42)
tab.loc[:, 'horne'] = tab.iloc[:, 1:].apply(lambda row: '; '.join([l for l in label_cols_horne if row[l]==1]), axis=1)
tab = tab[['text', 'score', 'horne']].sort_values('score')
tab.columns = ["Mention", "$z$-score", "Horne et al. classification"]
# TODO: make latex table
latex_table(tab)
```

```{python}
vectorizer = CountVectorizer(
    stop_words=stopwords.words('english'),
    ngram_range=(1, 3), 
    max_df = 0.8
)

idxs = horne_predictions_nonecon_gender_attribute_sample[gender_sexuality_cats].any(axis=1)
fw = compute_fighting_words(
    l1=horne_predictions_nonecon_gender_attribute_sample.loc[ idxs, 'text'].to_list(),
    l2=horne_predictions_nonecon_gender_attribute_sample.loc[~idxs, 'text'].to_list(),
    cv=vectorizer,
)
fw_ours_vs_horne_gender = pd.DataFrame(fw, columns=['word', 'score']).sort_values('score', ascending=False)
```

```{python}
#| label: fig-ours_vs_horne_gender_fighting_words
#| output: true
#| fig-cap: Top 20 most distinctive words for gender/sexuality mentions classified by Horne et al. (right) vs. not classified (left)

# Get top 20 lowest (most negative) and highest (most positive) scores
top_negative = fw_ours_vs_horne_gender.nsmallest(20, 'score').sort_values('score', ascending=False)
top_positive = fw_ours_vs_horne_gender.nlargest(20, 'score').sort_values('score', ascending=True)

# Create two-column layout
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(11, 4), sharey=False)

# Left plot: positive scores (distinctive for classified by Horne et al.)
ttl = r"mentions categorized by both as gender/sexuality-related"
ax1.axvline(x=1.96, color='black', linestyle='--', linewidth=0.8, zorder=1)
ax1.barh(range(len(top_positive)), top_positive['score'], color='#1b9e77', zorder=2)
ax1.set_yticks(range(len(top_positive)))
ax1.set_yticklabels(top_positive['word'])
ax1.set_xlabel('Score (z-score)', fontsize=11)
ax1.set_title(ttl, fontweight='bold', fontsize=12)
ax1.axvline(x=0, color='black', linestyle='-', linewidth=0.8)
ax1.yaxis.tick_right()
ax1.yaxis.set_label_position('right')
plt.setp(ax1.get_yticklabels(), ha='left')
ax1.set_xlim(0, 25)
ax1.invert_xaxis()  # Invert to show negative values extending left

# Right plot: negative scores (distinctive for NOT classified by Horne et al.)
ttl = r"mentions categorized only by ours as gender/sexuality-related"
ax2.axvline(x=-1.96, color='black', linestyle='--', linewidth=0.8, zorder=1)
ax2.barh(range(len(top_negative)), top_negative['score'], color='#d95f02', zorder=2)
ax2.set_yticks(range(len(top_negative)))
ax2.set_yticklabels(top_negative['word'])
ax2.set_xlabel('Score (z-score)', fontsize=11)
ax2.set_title(ttl, fontweight='bold', fontsize=12)
ax2.set_xlim(-25, 0)
ax2.invert_xaxis()  # Invert to show negative values extending left

plt.tight_layout()
plt.show()
```

```{python}
#| label: tbl-ours_vs_horne_gender_fighting_words_examples
#| output: true
#| tbl-cap: 'Examples of social group mentions labeled as featuring gender/sexuality as an attribute by our classifier that were not assigned to any of Horne et al.''s Men, Women, or LGBTQI categories by their classifier. Values computed by summing "fighting words" scores as weights of mentions'' tokens, normalized by number of tokens.'
fw_lookup = {r['word']: r['score'] for r in fw_ours_vs_horne_gender.to_dict(orient='records')}
fw_vals = np.array([fw_lookup[f] for f in vectorizer.get_feature_names_out()])
analyzer = vectorizer.build_analyzer()

# vectorize mentions 
mentions = horne_predictions_nonecon_gender_attribute_sample[~idxs].reset_index(drop=True)
mentions_texts = mentions['text']
X_mentions = vectorizer.transform(mentions_texts.tolist())
# binarize
X_mentions[X_mentions>0] = 1
# apply z-score values to each row in `X_mentions` as weights
X_mentions_scores = X_mentions @ fw_vals[:, np.newaxis]
# normalize for mention length
X_mentions_scores /= X_mentions.sum(axis=1)

mention_scores = X_mentions_scores[:, 0]
rank = mention_scores.argsort()#[::-1]

n_ = 20
tab = horne_predictions_nonecon_gender_attribute_sample.loc[~idxs, ['text', *label_cols_horne]].iloc[rank]
tab['score'] = mention_scores[rank]
tab = tab[tab['score']<-1.64]
tab['text_norm'] = tab['text'].apply(lambda x: ' '.join(analyzer(x)).strip())
tab = tab.drop_duplicates('text_norm')#.head(n_).reset_index(drop=True)
tab = tab.sample(n_, weights=tab['score'].abs(), random_state=1)
tab.loc[:, 'horne'] = tab.iloc[:, 1:].apply(lambda row: '; '.join([l for l in label_cols_horne if row[l]==1]), axis=1)
tab[['text', 'score', 'horne']].sort_values('score')
tab = tab[['text', 'score', 'horne']].sort_values('score')
tab.columns = ["Mention", "$z$-score", "Horne et al. classification"]
# TODO: make latex table
latex_table(tab, column_format='p{3in} l l')
```

