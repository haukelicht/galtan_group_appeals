{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a306570",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"  # to enable deterministic behavior with CuBLAS\n",
    "# NOTE: to avoid error\n",
    "#   RuntimeError: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` \n",
    "#   or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it\n",
    "#   uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an \n",
    "#   environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or \n",
    "#   CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to \n",
    "#   https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility\n",
    "\n",
    "from transformers import set_seed\n",
    "set_seed(42, deterministic=True) # for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2f9ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(precision=4, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82dc5f1",
   "metadata": {},
   "source": [
    "## Single-label classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eeca72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a example classification dataset from hf hub\n",
    "from datasets import load_dataset\n",
    "train_dataset = load_dataset(\"ag_news\", split=\"train[:600]\")\n",
    "val_dataset = load_dataset(\"ag_news\", split=\"test[:200]\")\n",
    "test_dataset = load_dataset(\"ag_news\", split=\"test[200:400]\")\n",
    "\n",
    "num_classes = len(set(train_dataset[\"label\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ae7159",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.trainer_utils import PredictionOutput\n",
    "from sklearn.metrics import f1_score\n",
    "def compute_metrics(p: PredictionOutput):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    labels = p.label_ids\n",
    "    acc = np.sum(preds == labels) / len(labels)\n",
    "    f1 = f1_score(labels, preds, average=\"macro\")\n",
    "    return {\"accuracy\": acc, \"macro_f1\": f1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24875592",
   "metadata": {},
   "source": [
    "### using the `SetfitModel`'s `fit()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d726e593",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from setfit.modeling import SetFitHead\n",
    "# from src.finetuning.setfit_extensions.class_weights_head import compute_class_weights, SetFitHeadWithClassWeights\n",
    "from src.finetuning.setfit_extensions.early_stopping import (\n",
    "    SetFitModelWithEarlyStopping,\n",
    "    EarlyStoppingTrainingArguments\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29e5bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "body = SentenceTransformer(model_id, model_kwargs={\"device_map\": \"auto\"})\n",
    "\n",
    "head = SetFitHead(\n",
    "    in_features=body.get_sentence_embedding_dimension(),\n",
    "    out_features=num_classes,\n",
    "    device=body.device,\n",
    ")\n",
    "\n",
    "model = SetFitModelWithEarlyStopping(\n",
    "    model_body=body,\n",
    "    model_head=head,\n",
    "    normalize_embeddings=True,\n",
    ")\n",
    "model.to(body.device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a1fbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = EarlyStoppingTrainingArguments()\n",
    "args.max_length = body.tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c8143e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    x_train=train_dataset[\"text\"], y_train=train_dataset[\"label\"],\n",
    "    x_eval=val_dataset[\"text\"], y_eval=val_dataset[\"label\"],\n",
    "    \n",
    "    num_epochs=10,\n",
    "    batch_size=16,\n",
    "    body_learning_rate=args.body_classifier_learning_rate,\n",
    "    head_learning_rate=args.head_learning_rate,\n",
    "    l2_weight=args.l2_weight,\n",
    "    \n",
    "    max_length=body.tokenizer.model_max_length,\n",
    "    \n",
    "    show_progress_bar=True,\n",
    "    end_to_end=True,\n",
    "    \n",
    "    # added early stopping arguments\n",
    "    compute_metrics=compute_metrics,\n",
    "    metric_for_best_model=\"macro_f1\", # NOTE: must match one of the keys returned by `compute_metrics`\n",
    "    early_stopping_patience=2,\n",
    "    early_stopping_threshold=0.03,\n",
    "    greater_is_better=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ee583a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# free GPU\n",
    "model.to(\"cpu\");\n",
    "del model\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "# print(torch.cuda.memory_summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea8a983",
   "metadata": {},
   "source": [
    "### with custom early-stopping trainer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e531d162",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from setfit.modeling import SetFitHead\n",
    "from src.finetuning.setfit_extensions.class_weights_head import (\n",
    "    compute_class_weights,\n",
    "    SetFitHeadWithClassWeights\n",
    ")\n",
    "from src.finetuning.setfit_extensions.early_stopping import (\n",
    "    SetFitModelWithEarlyStopping, \n",
    "    EarlyStoppingTrainingArguments,\n",
    "    EarlyStoppingCallback,\n",
    "    EarlyStoppingTrainer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ce84e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init(\n",
    "        model_name: str=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        num_classes: int=2, \n",
    "        class_weights: np._typing.NDArray=None,\n",
    "        **kwargs\n",
    "    ) -> SetFitModelWithEarlyStopping:\n",
    "    \n",
    "    model_kwargs={\"device_map\": \"auto\", **kwargs}\n",
    "    body = SentenceTransformer(model_name, model_kwargs=model_kwargs, trust_remote_code=True)\n",
    "    \n",
    "    head_kwargs = dict(\n",
    "        in_features=body.get_sentence_embedding_dimension(),\n",
    "        out_features=num_classes,\n",
    "        device=body.device,\n",
    "    )\n",
    "    head = SetFitHeadWithClassWeights(**head_kwargs, class_weights=class_weights) if class_weights is not None else SetFitHead(**head_kwargs)\n",
    "    \n",
    "    return SetFitModelWithEarlyStopping(\n",
    "        model_body=body,\n",
    "        model_head=head.to(body.device),\n",
    "        normalize_embeddings=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed451fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"sentence-transformers/all-MiniLM-L6-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e53afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = EarlyStoppingTrainingArguments(\n",
    "    num_epochs=(1, 15),\n",
    "    # sentence transformer (embedding) finetuning arts\n",
    "    eval_strategy=\"steps\", # NOTE: currently no effect on (early stopping in) classification head training\n",
    "    eval_steps=25, # NOTE: overwrites 0 epochs above for sentence transformer finetuning\n",
    "    max_steps=200,\n",
    "    eval_max_steps=200,\n",
    "    # early stopping config\n",
    "    metric_for_best_model=(\"embedding_loss\", \"f1\"),\n",
    "    greater_is_better=(False, True),\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=2, # NOTE: currently no effect on (early stopping in) classification head training\n",
    "    # misc\n",
    "    end_to_end=True,\n",
    ")\n",
    "\n",
    "training_callbacks = [\n",
    "    EarlyStoppingCallback(early_stopping_patience=2, early_stopping_threshold=0.03), # for sentence transformer finetuning\n",
    "    EarlyStoppingCallback(early_stopping_patience=4, early_stopping_threshold=0.02), # for classifier finetuning\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcecbb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute class weights (inversely proportional to class frequencies)\n",
    "class_weights = compute_class_weights(train_dataset[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69f9ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize Trainer\n",
    "trainer = EarlyStoppingTrainer(\n",
    "    model_init=lambda : model_init(\n",
    "        model_name=model_id,\n",
    "        num_classes=num_classes,\n",
    "        class_weights=class_weights,\n",
    "    ),\n",
    "    metric=\"f1\",\n",
    "    metric_kwargs={\"average\": \"macro\"},\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    callbacks=training_callbacks,\n",
    ")\n",
    "# fix max_length issue\n",
    "trainer._args.max_length = trainer.st_trainer.model.tokenizer.model_max_length\n",
    "\n",
    "# set seeds for reproducibility\n",
    "trainer._args.seed = 42\n",
    "trainer.st_trainer.args.seed = 42\n",
    "trainer.st_trainer.args.data_seed = 42\n",
    "trainer.st_trainer.args.full_determinism = True\n",
    "\n",
    "# don't report to wandb or other experiment trackers\n",
    "trainer._args.report_to = 'none'\n",
    "trainer.st_trainer.args.report_to = 'none'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d290afcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864c420c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify best model loaded\n",
    "trainer.evaluate(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f86b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval\n",
    "trainer.evaluate(test_dataset, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00ead68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "preds = trainer.model.predict(test_dataset[\"text\"], as_numpy=True)\n",
    "print(classification_report(test_dataset['label'], preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e4d6e7",
   "metadata": {},
   "source": [
    "## Multi-label classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1513424",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "\n",
    "dataset_id = 'acloudfan/toxicity-multi-label-classifier'\n",
    "\n",
    "train_dataset = load_dataset(dataset_id, split=\"train\")\n",
    "val_dataset = load_dataset(dataset_id, split=\"validation\")\n",
    "test_dataset = load_dataset(dataset_id, split=\"test\")\n",
    "\n",
    "label_cols = ['toxic', 'threat', 'insult', 'identity_hate']\n",
    "for col in label_cols:\n",
    "    print(col, dict(Counter(train_dataset[col])), sep=\": \")\n",
    "\n",
    "num_classes = len(label_cols)\n",
    "id2label = dict(enumerate(label_cols))\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "# convert to multi-label format\n",
    "def format_dataset_multi_label(example):\n",
    "    example['label'] = [example[label_col] for label_col in label2id.keys()]\n",
    "    return example\n",
    "\n",
    "train_dataset = train_dataset.map(format_dataset_multi_label, batched=False)\n",
    "train_dataset = train_dataset.rename_column(\"comment_text\", \"text\")\n",
    "train_dataset = train_dataset.remove_columns(label_cols)\n",
    "\n",
    "val_dataset = val_dataset.map(format_dataset_multi_label, batched=False)\n",
    "val_dataset = val_dataset.rename_column(\"comment_text\", \"text\")\n",
    "val_dataset = val_dataset.remove_columns(label_cols)\n",
    "\n",
    "test_dataset = test_dataset.map(format_dataset_multi_label, batched=False)\n",
    "test_dataset = test_dataset.rename_column(\"comment_text\", \"text\")\n",
    "test_dataset = test_dataset.remove_columns(label_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44892bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "\n",
    "def compute_metrics_multilabel(p):\n",
    "    \"\"\"\n",
    "    Compute evaluation metrics for multi-label classification.\n",
    "    eval_pred: transformers.trainer_utils.PredictionOutput\n",
    "               (contains .predictions and .label_ids)\n",
    "    \"\"\"\n",
    "    # unpack\n",
    "    logits, labels = p.predictions, p.label_ids\n",
    "    # apply sigmoid to get probabilities in [0,1]\n",
    "    #probs = 1 / (1 + np.exp(-logits))\n",
    "    probs = torch.sigmoid(torch.tensor(logits)).numpy()\n",
    "    # threshold at 0.5 for binary decisions\n",
    "    preds = (probs > 0.5).astype(int)\n",
    "\n",
    "    # compute metrics\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    f1_macro = f1_score(labels, preds, average=\"macro\", zero_division=0.0)\n",
    "    f1_micro = f1_score(labels, preds, average=\"micro\", zero_division=0.0)\n",
    "    precision_macro = precision_score(labels, preds, average=\"macro\", zero_division=0.0)\n",
    "    recall_macro = recall_score(labels, preds, average=\"macro\", zero_division=0.0)\n",
    "\n",
    "    # optional: subset accuracy (exact match ratio)\n",
    "    subset_acc = (labels == preds).all(axis=1).mean()\n",
    "\n",
    "    return {\n",
    "        \"f1_macro\": f1_macro,\n",
    "        # \"f1_micro\": f1_micro,\n",
    "        # \"precision_macro\": precision_macro,\n",
    "        # \"recall_macro\": recall_macro,\n",
    "        \"accuracy\": accuracy,\n",
    "        # \"subset_accuracy\": subset_acc,\n",
    "    }\n",
    "\n",
    "# test:\n",
    "# y_true = train_dataset['label'][:10]\n",
    "# # simulate some predictions by sampling (shape (10, num_classes)) uniformly from 0-1\n",
    "# np.random.seed(42)\n",
    "# y_pred = np.random.rand(10, num_classes)\n",
    "# p = PredictionOutput(predictions=y_pred, label_ids=y_true, metrics=None)\n",
    "# compute_metrics_multilabel(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0341f480",
   "metadata": {},
   "source": [
    "### simple `fit` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a198bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "body = SentenceTransformer(model_id, model_kwargs={\"device_map\": \"auto\"})\n",
    "\n",
    "head = SetFitHead(\n",
    "    in_features=body.get_sentence_embedding_dimension(),\n",
    "    out_features=num_classes,\n",
    "    device=body.device,\n",
    "    multitarget=True,\n",
    ")\n",
    "\n",
    "model = SetFitModelWithEarlyStopping(\n",
    "    model_body=body,\n",
    "    model_head=head,\n",
    "    multi_target_strategy=\"one-vs-rest\",\n",
    "    normalize_embeddings=True,\n",
    ")\n",
    "model.to(body.device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf086c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = EarlyStoppingTrainingArguments()\n",
    "args.max_length = body.tokenizer.model_max_length\n",
    "\n",
    "model.fit(\n",
    "    x_train=train_dataset[\"text\"], y_train=train_dataset[\"label\"],\n",
    "    x_eval=val_dataset[\"text\"], y_eval=val_dataset[\"label\"],\n",
    "    \n",
    "    num_epochs=30,\n",
    "    batch_size=16,\n",
    "    body_learning_rate=args.body_classifier_learning_rate,\n",
    "    head_learning_rate=args.head_learning_rate,\n",
    "    l2_weight=args.l2_weight,\n",
    "    \n",
    "    max_length=args.max_length,\n",
    "    \n",
    "    show_progress_bar=True,\n",
    "    end_to_end=True,\n",
    "    \n",
    "    # added early stopping arguments\n",
    "    compute_metrics=compute_metrics_multilabel,\n",
    "    metric_for_best_model=\"f1_macro\", # NOTE: must match one of the keys returned by `compute_metrics`\n",
    "    early_stopping_patience=4,\n",
    "    early_stopping_threshold=0.03,\n",
    "    greater_is_better=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195f460f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify best model loaded\n",
    "logits = model.predict_logits(val_dataset[\"text\"], as_numpy=True)\n",
    "p = PredictionOutput(predictions=logits, label_ids=np.array(val_dataset[\"label\"]), metrics={})\n",
    "compute_metrics_multilabel(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2f5f43",
   "metadata": {},
   "source": [
    "#### with trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e78b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import AutoConfig\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from setfit.modeling import SetFitHead\n",
    "from src.finetuning.setfit_extensions.class_weights_head import (\n",
    "    compute_class_weights,\n",
    "    SetFitHeadWithClassWeights\n",
    ")\n",
    "from src.finetuning.setfit_extensions.early_stopping import (\n",
    "    SetFitModelWithEarlyStopping, \n",
    "    EarlyStoppingTrainingArguments,\n",
    "    EarlyStoppingCallback,\n",
    "    EarlyStoppingTrainer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9021bee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "def multiclass_model_init(\n",
    "        model_name: str=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        num_classes: int=2, \n",
    "        multi_target_strategy: Literal[\"one-vs-rest\", \"multi-output\"]=\"one-vs-rest\",\n",
    "        class_weights: np._typing.NDArray=None,\n",
    "        **kwargs\n",
    "    ) -> SetFitModelWithEarlyStopping:\n",
    "    \n",
    "    model_kwargs={\"device_map\": \"auto\", **kwargs}\n",
    "    body = SentenceTransformer(model_name, model_kwargs=model_kwargs, trust_remote_code=True)\n",
    "    \n",
    "    head_kwargs = dict(\n",
    "        in_features=body.get_sentence_embedding_dimension(),\n",
    "        out_features=num_classes,\n",
    "        device=body.device,\n",
    "        multitarget=(multi_target_strategy is not None),\n",
    "    )\n",
    "    head = SetFitHeadWithClassWeights(**head_kwargs, class_weights=class_weights) if class_weights is not None else SetFitHead(**head_kwargs)\n",
    "    \n",
    "    return SetFitModelWithEarlyStopping(\n",
    "        model_body=body,\n",
    "        model_head=head.to(body.device),\n",
    "        multi_target_strategy=multi_target_strategy,\n",
    "        normalize_embeddings=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f33a766",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"sentence-transformers/all-MiniLM-L6-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa901764",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = EarlyStoppingTrainingArguments(\n",
    "    num_epochs=(1, 15),\n",
    "    # sentence transformer (embedding) finetuning arts\n",
    "    eval_strategy=\"steps\", # NOTE: currently no effect on (early stopping in) classification head training\n",
    "    eval_steps=25, # NOTE: overwrites 0 epochs above for sentence transformer finetuning\n",
    "    max_steps=200,\n",
    "    eval_max_steps=200,\n",
    "    # train end to end\n",
    "    end_to_end=True,\n",
    "    # early stopping config\n",
    "    metric_for_best_model=(\"embedding_loss\", \"f1\"),\n",
    "    greater_is_better=(False, True),\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=2, # NOTE: currently no effect on (early stopping in) classification head training\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c5c4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute class weights (inversely proportional to class frequencies)\n",
    "class_weights = compute_class_weights(train_dataset[\"label\"], multitarget=True)\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48270cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_callbacks = [\n",
    "    EarlyStoppingCallback(early_stopping_patience=2, early_stopping_threshold=0.03), # for sentence transformer finetuning\n",
    "    EarlyStoppingCallback(early_stopping_patience=4, early_stopping_threshold=0.02), # for classifier finetuning\n",
    "]\n",
    "\n",
    "# initialize Trainer\n",
    "trainer = EarlyStoppingTrainer(\n",
    "    model_init=lambda : multiclass_model_init(\n",
    "        model_name=model_id,\n",
    "        num_classes=num_classes,\n",
    "        # class_weights=class_weights,\n",
    "    ),\n",
    "    args=training_args,\n",
    "    metric=\"f1\",\n",
    "    metric_kwargs={\"average\": \"macro\"},\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    callbacks=training_callbacks,\n",
    "    # compute_metrics=compute_metrics_multilabel,\n",
    ")\n",
    "# fix max_length issue\n",
    "trainer._args.max_length = trainer.st_trainer.model.tokenizer.model_max_length\n",
    "\n",
    "# set seeds for reproducibility\n",
    "trainer._args.seed = 42\n",
    "trainer.st_trainer.args.seed = 42\n",
    "trainer.st_trainer.args.data_seed = 42\n",
    "trainer.st_trainer.args.full_determinism = True\n",
    "\n",
    "# don't report to wandb or other experiment trackers\n",
    "trainer._args.report_to = 'none'\n",
    "trainer.st_trainer.args.report_to = 'none'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3a1ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a156db1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate(val_dataset, metric_key_prefix=\"eval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69150ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "preds = trainer.model.predict(test_dataset['text'], as_numpy=True)\n",
    "print(classification_report(test_dataset['label'], preds, zero_division=0, target_names=label_cols))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "galtan_group_appeals",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
