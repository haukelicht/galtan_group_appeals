{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a306570",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"  # to enable deterministic behavior with CuBLAS\n",
    "# NOTE: to avoid error\n",
    "#   RuntimeError: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` \n",
    "#   or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it\n",
    "#   uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an \n",
    "#   environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or \n",
    "#   CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to \n",
    "#   https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility\n",
    "\n",
    "from transformers import set_seed\n",
    "set_seed(42, deterministic=True) # for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b2f9ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from setfit.modeling import SetFitHead\n",
    "# from src.finetuning.setfit_extensions.class_weights_head import compute_class_weights, SetFitHeadWithClassWeights\n",
    "from src.finetuning.setfit_extensions.early_stopping import (\n",
    "    SetFitModelWithEarlyStopping,\n",
    "    EarlyStoppingTrainingArguments,\n",
    ")\n",
    "from transformers.trainer_utils import PredictionOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7eeca72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a example classification dataset from hf hub\n",
    "from datasets import load_dataset\n",
    "train_dataset = load_dataset(\"ag_news\", split=\"train[:600]\")\n",
    "val_dataset = load_dataset(\"ag_news\", split=\"test[:200]\")\n",
    "test_dataset = load_dataset(\"ag_news\", split=\"test[200:400]\")\n",
    "\n",
    "num_classes = len(set(train_dataset[\"label\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24875592",
   "metadata": {},
   "source": [
    "### using the `SetfitModel`'s `fit()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a29e5bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "body = SentenceTransformer(model_id, model_kwargs={\"device_map\": \"auto\"})\n",
    "\n",
    "head = SetFitHead(\n",
    "    in_features=body.get_sentence_embedding_dimension(),\n",
    "    out_features=num_classes,\n",
    "    device=body.device,\n",
    ")\n",
    "\n",
    "model = SetFitModelWithEarlyStopping(\n",
    "    model_body=body,\n",
    "    model_head=head,\n",
    "    normalize_embeddings=True,\n",
    ")\n",
    "model.to(body.device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08a1fbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = EarlyStoppingTrainingArguments()\n",
    "args.max_length = body.tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36ae7159",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "def compute_metrics(p: PredictionOutput):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    labels = p.label_ids\n",
    "    acc = np.sum(preds == labels) / len(labels)\n",
    "    f1 = f1_score(labels, preds, average=\"macro\")\n",
    "    return {\"accuracy\": acc, \"macro_f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65c8143e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  10%|█         | 1/10 [00:00<00:04,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'training loss': 0.997852025847686, 'validation loss': 1.087493910239293, 'accuracy': 0.48, 'macro_f1': 0.4134369245670616}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  20%|██        | 2/10 [00:00<00:03,  2.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'training loss': 0.6535947197361996, 'validation loss': 0.9029704882548406, 'accuracy': 0.635, 'macro_f1': 0.6035682708096501}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  30%|███       | 3/10 [00:01<00:02,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'training loss': 0.5147908455447147, 'validation loss': 0.7719288881008441, 'accuracy': 0.695, 'macro_f1': 0.6850539109126752}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  40%|████      | 4/10 [00:01<00:02,  2.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'training loss': 0.4385863715096524, 'validation loss': 0.7151274589391855, 'accuracy': 0.73, 'macro_f1': 0.7132838589981447}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  50%|█████     | 5/10 [00:01<00:01,  2.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'training loss': 0.3909744169366987, 'validation loss': 0.6981672988488123, 'accuracy': 0.74, 'macro_f1': 0.7209796641915848}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  60%|██████    | 6/10 [00:02<00:01,  2.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'training loss': 0.3556148684338519, 'validation loss': 0.7059849294332358, 'accuracy': 0.75, 'macro_f1': 0.7294087721878985}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  60%|██████    | 6/10 [00:02<00:01,  2.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'training loss': 0.34054943017269435, 'validation loss': 0.6692329622231997, 'accuracy': 0.76, 'macro_f1': 0.741553673243814}\n",
      "Early stopping triggered after 7 epochs\n",
      "Loading best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    x_train=train_dataset[\"text\"], y_train=train_dataset[\"label\"],\n",
    "    x_eval=val_dataset[\"text\"], y_eval=val_dataset[\"label\"],\n",
    "    \n",
    "    num_epochs=10,\n",
    "    batch_size=16,\n",
    "    body_learning_rate=args.body_classifier_learning_rate,\n",
    "    head_learning_rate=args.head_learning_rate,\n",
    "    l2_weight=args.l2_weight,\n",
    "    \n",
    "    max_length=body.tokenizer.model_max_length,\n",
    "    \n",
    "    show_progress_bar=True,\n",
    "    end_to_end=False, # !!! fine-tune only head\n",
    "    \n",
    "    # added early stopping arguments\n",
    "    compute_metrics=compute_metrics,\n",
    "    metric_for_best_model=\"macro_f1\", # NOTE: must match one of the keys returned by `compute_metrics`\n",
    "    early_stopping_patience=2,\n",
    "    early_stopping_threshold=0.03,\n",
    "    greater_is_better=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42ee583a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  65536 KiB | 235349 KiB | 262097 MiB | 262033 MiB |\n",
      "|       from large pool |  65536 KiB | 219863 KiB | 247782 MiB | 247718 MiB |\n",
      "|       from small pool |      0 KiB |  16570 KiB |  14314 MiB |  14314 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  65536 KiB | 235349 KiB | 262097 MiB | 262033 MiB |\n",
      "|       from large pool |  65536 KiB | 219863 KiB | 247782 MiB | 247718 MiB |\n",
      "|       from small pool |      0 KiB |  16570 KiB |  14314 MiB |  14314 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |  65536 KiB | 235344 KiB | 261781 MiB | 261717 MiB |\n",
      "|       from large pool |  65536 KiB | 219863 KiB | 247468 MiB | 247404 MiB |\n",
      "|       from small pool |      0 KiB |  16567 KiB |  14312 MiB |  14312 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |  65536 KiB | 290816 KiB | 290816 KiB | 225280 KiB |\n",
      "|       from large pool |  65536 KiB | 270336 KiB | 270336 KiB | 204800 KiB |\n",
      "|       from small pool |      0 KiB |  20480 KiB |  20480 KiB |  20480 KiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |      0 B   |  93357 KiB | 172230 MiB | 172230 MiB |\n",
      "|       from large pool |      0 B   |  87808 KiB | 157903 MiB | 157903 MiB |\n",
      "|       from small pool |      0 B   |   8507 KiB |  14326 MiB |  14326 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       2    |     146    |   65708    |   65706    |\n",
      "|       from large pool |       2    |      25    |   29908    |   29906    |\n",
      "|       from small pool |       0    |     123    |   35800    |   35800    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       2    |     146    |   65708    |   65706    |\n",
      "|       from large pool |       2    |      25    |   29908    |   29906    |\n",
      "|       from small pool |       0    |     123    |   35800    |   35800    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       2    |      18    |      18    |      16    |\n",
      "|       from large pool |       2    |       8    |       8    |       6    |\n",
      "|       from small pool |       0    |      10    |      10    |      10    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       0    |      21    |   39863    |   39863    |\n",
      "|       from large pool |       0    |       8    |   15196    |   15196    |\n",
      "|       from small pool |       0    |      16    |   24667    |   24667    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# free GPU\n",
    "model.to(\"cpu\");\n",
    "del model\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "print(torch.cuda.memory_summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea8a983",
   "metadata": {},
   "source": [
    "### with custom early-stopping trainer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e531d162",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import AutoConfig\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from setfit.modeling import SetFitHead\n",
    "from src.finetuning.setfit_extensions.class_weights_head import (\n",
    "    compute_class_weights,\n",
    "    SetFitHeadWithClassWeights\n",
    ")\n",
    "from src.finetuning.setfit_extensions.early_stopping import (\n",
    "    SetFitModelWithEarlyStopping, \n",
    "    EarlyStoppingTrainingArguments,\n",
    "    EarlyStoppingCallback,\n",
    "    EarlyStoppingTrainer\n",
    ")\n",
    "\n",
    "def model_init(\n",
    "        model_name: str=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        num_classes: int=2, \n",
    "        class_weights: np._typing.NDArray=None,\n",
    "        **kwargs\n",
    "    ) -> SetFitModelWithEarlyStopping:\n",
    "    \n",
    "    model_kwargs={\"device_map\": \"auto\", **kwargs}\n",
    "    body = SentenceTransformer(model_name, model_kwargs=model_kwargs, trust_remote_code=True)\n",
    "    \n",
    "    # TODO: support multi-label classification\n",
    "    head_kwargs = dict(\n",
    "        in_features=body.get_sentence_embedding_dimension(),\n",
    "        out_features=num_classes,\n",
    "        device=body.device,\n",
    "    )\n",
    "    head = SetFitHeadWithClassWeights(**head_kwargs, class_weights=class_weights) if class_weights is not None else SetFitHead(**head_kwargs)\n",
    "    \n",
    "    return SetFitModelWithEarlyStopping(\n",
    "        model_body=body,\n",
    "        model_head=head.to(body.device),\n",
    "        normalize_embeddings=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ed451fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_id, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7e53afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = EarlyStoppingTrainingArguments(\n",
    "    num_epochs=(1, 15),\n",
    "    # sentence transformer (embedding) finetuning arts\n",
    "    eval_strategy=\"steps\", # NOTE: currently no effect on (early stopping in) classification head training\n",
    "    eval_steps=25, # NOTE: overwrites 0 epochs above for sentence transformer finetuning\n",
    "    max_steps=200,\n",
    "    eval_max_steps=200,\n",
    "    # early stopping config\n",
    "    metric_for_best_model=(\"embedding_loss\", \"macro_f1\"),\n",
    "    greater_is_better=(False, True),\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=2, # NOTE: currently no effect on (early stopping in) classification head training\n",
    ")\n",
    "\n",
    "training_callbacks = [\n",
    "    EarlyStoppingCallback(early_stopping_patience=2, early_stopping_threshold=0.03), # for sentence transformer finetuning\n",
    "    EarlyStoppingCallback(early_stopping_patience=4, early_stopping_threshold=0.02), # for classifier finetuning\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fcecbb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute class weights (inversely proportional to class frequencies)\n",
    "class_weights = compute_class_weights(train_dataset[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e69f9ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize Trainer\n",
    "trainer = EarlyStoppingTrainer(\n",
    "    model_init=lambda : model_init(\n",
    "        model_name=model_id,\n",
    "        num_classes=num_classes,\n",
    "        class_weights=class_weights,\n",
    "    ),\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    callbacks=training_callbacks,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "# fix max_length issue\n",
    "trainer._args.max_length = trainer.st_trainer.model.tokenizer.model_max_length\n",
    "\n",
    "# set seeds for reproducibility\n",
    "trainer._args.seed = 42\n",
    "trainer.st_trainer.args.seed = 42\n",
    "trainer.st_trainer.args.data_seed = 42\n",
    "trainer.st_trainer.args.full_determinism = True\n",
    "\n",
    "# don't report to wandb or other experiment trackers\n",
    "trainer._args.report_to = 'none'\n",
    "trainer.st_trainer.args.report_to = 'none'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d290afcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num unique pairs = 3200\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Num epochs = 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/200 00:18 < 00:11, 6.80 it/s, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.667300</td>\n",
       "      <td>0.232995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.264900</td>\n",
       "      <td>0.215451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.264900</td>\n",
       "      <td>0.239145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.143800</td>\n",
       "      <td>0.224630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.143800</td>\n",
       "      <td>0.233633</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   7%|▋         | 1/15 [00:01<00:23,  1.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'training loss': 0.5792229017739494, 'validation loss': 0.611130109205842, 'accuracy': 0.795, 'macro_f1': 0.7751903735632184}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  13%|█▎        | 2/15 [00:03<00:21,  1.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'training loss': 0.35001990529398125, 'validation loss': 0.5642310957238078, 'accuracy': 0.82, 'macro_f1': 0.7934924319869581}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  20%|██        | 3/15 [00:04<00:19,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'training loss': 0.2867766917310655, 'validation loss': 0.5155245288088918, 'accuracy': 0.81, 'macro_f1': 0.7901657210375367}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  27%|██▋       | 4/15 [00:06<00:18,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'training loss': 0.23860107495139043, 'validation loss': 0.6621418678294867, 'accuracy': 0.78, 'macro_f1': 0.7540520662287904}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  27%|██▋       | 4/15 [00:08<00:22,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'training loss': 0.21036491377900043, 'validation loss': 0.6237878849543631, 'accuracy': 0.8, 'macro_f1': 0.7743435281075662}\n",
      "Early stopping triggered after 5 epochs\n",
      "Loading best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "43f86b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval\n",
    "probs = trainer.model.predict_proba(test_dataset['text'], as_numpy=True)\n",
    "preds = probs.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e00ead68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.79      0.73        42\n",
      "           1       0.95      0.81      0.88        70\n",
      "           2       0.70      0.72      0.71        43\n",
      "           3       0.73      0.78      0.75        45\n",
      "\n",
      "    accuracy                           0.78       200\n",
      "   macro avg       0.77      0.77      0.77       200\n",
      "weighted avg       0.79      0.78      0.78       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_dataset['label'], preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e4d6e7",
   "metadata": {},
   "source": [
    "### with multi-label classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b1513424",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = 'acloudfan/toxicity-multi-label-classifier'\n",
    "\n",
    "from datasets import load_dataset\n",
    "train_dataset = load_dataset(dataset_id, split=\"train\")\n",
    "val_dataset = load_dataset(dataset_id, split=\"validation\")\n",
    "test_dataset = load_dataset(dataset_id, split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0616740a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 58, 0: 31})\n",
      "Counter({0: 69, 1: 20})\n",
      "Counter({0: 63, 1: 26})\n",
      "Counter({0: 69, 1: 20})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "label_cols = ['toxic', 'threat', 'insult', 'identity_hate']\n",
    "for col in label_cols:\n",
    "    print(Counter(train_dataset[col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "03d94963",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = dict(enumerate(label_cols))\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "# convert to multi-label format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fb537f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to multi-label format\n",
    "def format_dataset_multi_label(example):\n",
    "    example['label'] = [example[label_col] for label_col in label2id.keys()]\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "32cfcd78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5285163c128c46b09c9b41b9b128c153",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/35 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = train_dataset.map(format_dataset_multi_label, batched=False)\n",
    "train_dataset = train_dataset.rename_column(\"comment_text\", \"text\")\n",
    "train_dataset = train_dataset.remove_columns(label_cols)\n",
    "\n",
    "val_dataset = val_dataset.map(format_dataset_multi_label, batched=False)\n",
    "val_dataset = val_dataset.rename_column(\"comment_text\", \"text\")\n",
    "val_dataset = val_dataset.remove_columns(label_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "912de8f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 89\n",
       "})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c5a198bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "body = SentenceTransformer(model_id, model_kwargs={\"device_map\": \"auto\"})\n",
    "\n",
    "head = SetFitHead(\n",
    "    in_features=body.get_sentence_embedding_dimension(),\n",
    "    \n",
    "    out_features=num_classes,\n",
    "    device=body.device,\n",
    ")\n",
    "\n",
    "model = SetFitModelWithEarlyStopping(\n",
    "    model_body=body,\n",
    "    model_head=head,\n",
    "    multi_target_strategy=\"one-vs-rest\",\n",
    "    normalize_embeddings=True,\n",
    ")\n",
    "model.to(body.device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fd2422bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = EarlyStoppingTrainingArguments()\n",
    "args.max_length = body.tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f44892bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "\n",
    "def compute_metrics_multilabel(p):\n",
    "    \"\"\"\n",
    "    Compute evaluation metrics for multi-label classification.\n",
    "    eval_pred: transformers.trainer_utils.PredictionOutput\n",
    "               (contains .predictions and .label_ids)\n",
    "    \"\"\"\n",
    "    # unpack\n",
    "    logits, labels = p.predictions, p.label_ids\n",
    "    # apply sigmoid to get probabilities in [0,1]\n",
    "    probs = 1 / (1 + np.exp(-logits))\n",
    "    # threshold at 0.5 for binary decisions\n",
    "    preds = (probs > 0.5).astype(int)\n",
    "\n",
    "    # compute metrics\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    f1_macro = f1_score(labels, preds, average=\"macro\", zero_division=0)\n",
    "    f1_micro = f1_score(labels, preds, average=\"micro\", zero_division=0)\n",
    "    precision_macro = precision_score(labels, preds, average=\"macro\", zero_division=0)\n",
    "    recall_macro = recall_score(labels, preds, average=\"macro\", zero_division=0)\n",
    "\n",
    "    # optional: subset accuracy (exact match ratio)\n",
    "    subset_acc = (labels == preds).all(axis=1).mean()\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"subset_accuracy\": subset_acc,\n",
    "        \"f1_macro\": f1_macro,\n",
    "        \"f1_micro\": f1_micro,\n",
    "        \"precision_macro\": precision_macro,\n",
    "        \"recall_macro\": recall_macro,\n",
    "    }\n",
    "\n",
    "# test:\n",
    "# y_true = train_dataset['label'][:10]\n",
    "# # simulate some predictions by sampling (shape (10, num_classes)) uniformly from 0-1\n",
    "# np.random.seed(42)\n",
    "# y_pred = np.random.rand(10, num_classes)\n",
    "# p = PredictionOutput(predictions=y_pred, label_ids=y_true, metrics=None)\n",
    "# compute_metrics_multilabel(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "cf086c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  10%|█         | 1/10 [00:00<00:01,  5.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'training loss': 1.4056835174560547, 'validation loss': 1.6588252385457356, 'accuracy': 0.3142857142857143, 'subset_accuracy': 0.3142857142857143, 'f1_macro': 0.6158771929824561, 'f1_micro': 0.7068965517241379, 'precision_macro': 0.6293290043290043, 'recall_macro': 0.6833964646464646}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  20%|██        | 2/10 [00:00<00:01,  5.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'training loss': 1.3107593655586243, 'validation loss': 1.7910070419311523, 'accuracy': 0.34285714285714286, 'subset_accuracy': 0.34285714285714286, 'f1_macro': 0.6670193841246473, 'f1_micro': 0.7368421052631579, 'precision_macro': 0.6842532467532467, 'recall_macro': 0.7111742424242424}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  30%|███       | 3/10 [00:00<00:01,  6.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'training loss': 1.2375029027462006, 'validation loss': 1.581046462059021, 'accuracy': 0.37142857142857144, 'subset_accuracy': 0.37142857142857144, 'f1_macro': 0.7113844393592678, 'f1_micro': 0.7521367521367521, 'precision_macro': 0.6918290043290044, 'recall_macro': 0.766729797979798}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  40%|████      | 4/10 [00:00<00:00,  6.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'training loss': 1.1650748252868652, 'validation loss': 1.883272687594096, 'accuracy': 0.37142857142857144, 'subset_accuracy': 0.37142857142857144, 'f1_macro': 0.7113844393592678, 'f1_micro': 0.7521367521367521, 'precision_macro': 0.6918290043290044, 'recall_macro': 0.766729797979798}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  40%|████      | 4/10 [00:00<00:01,  5.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'training loss': 1.1123599310715993, 'validation loss': 1.6481937567392986, 'accuracy': 0.37142857142857144, 'subset_accuracy': 0.37142857142857144, 'f1_macro': 0.6875749155497439, 'f1_micro': 0.7413793103448276, 'precision_macro': 0.683495670995671, 'recall_macro': 0.7389520202020201}\n",
      "Early stopping triggered after 5 epochs\n",
      "Loading best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    x_train=train_dataset[\"text\"], y_train=train_dataset[\"label\"],\n",
    "    x_eval=val_dataset[\"text\"], y_eval=val_dataset[\"label\"],\n",
    "    \n",
    "    num_epochs=10,\n",
    "    batch_size=16,\n",
    "    body_learning_rate=args.body_classifier_learning_rate,\n",
    "    head_learning_rate=args.head_learning_rate,\n",
    "    l2_weight=args.l2_weight,\n",
    "    \n",
    "    max_length=body.tokenizer.model_max_length,\n",
    "    \n",
    "    show_progress_bar=True,\n",
    "    end_to_end=True, # !!! fine-tune only head\n",
    "    \n",
    "    # added early stopping arguments\n",
    "    compute_metrics=compute_metrics_multilabel,\n",
    "    metric_for_best_model=\"f1_macro\", # NOTE: must match one of the keys returned by `compute_metrics`\n",
    "    early_stopping_patience=2,\n",
    "    early_stopping_threshold=0.03,\n",
    "    greater_is_better=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "galtan_group_appeals",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
