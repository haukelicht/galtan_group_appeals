{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a306570",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"  # to enable deterministic behavior with CuBLAS\n",
    "# NOTE: to avoid error\n",
    "#   RuntimeError: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` \n",
    "#   or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it\n",
    "#   uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an \n",
    "#   environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or \n",
    "#   CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to \n",
    "#   https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility\n",
    "\n",
    "from transformers import set_seed\n",
    "set_seed(42, deterministic=True) # for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6ca4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "from src.finetuning.setfit_extensions.class_weights_head import compute_class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc2b8c8",
   "metadata": {},
   "source": [
    "###  prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eeca72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a example classification dataset from hf hub\n",
    "from datasets import load_dataset, Dataset\n",
    "from datasets.utils.logging import disable_progress_bar\n",
    "disable_progress_bar()\n",
    "\n",
    "dataset_id = \"jakartaresearch/semeval-absa\"\n",
    "train_dataset = load_dataset(dataset_id, \"restaurant\", split=\"train[:1000]\")\n",
    "val_dataset = load_dataset(dataset_id, \"restaurant\", split=\"validation[:200]\")\n",
    "test_dataset = load_dataset(dataset_id, \"restaurant\", split=\"validation[200:400]\")\n",
    "\n",
    "def to_span_classification_format(example, allowed_labels=None):\n",
    "    out = []\n",
    "    aspects = example['aspects']\n",
    "    for f, t, lab in zip(aspects['from'], aspects['to'], aspects['polarity']):\n",
    "        if allowed_labels is None or lab in allowed_labels:\n",
    "            out.append({'text': example['text'], 'span': (f, t), 'label': lab})\n",
    "    return out\n",
    "\n",
    "def dataset_to_span_classification_format(dataset, allowed_labels=None):\n",
    "    return Dataset.from_list([example for examples in dataset.to_list() for example in to_span_classification_format(examples, allowed_labels)])\n",
    "\n",
    "allowed_labels = ['negative', 'neutral', 'positive']\n",
    "train_dataset = dataset_to_span_classification_format(train_dataset, allowed_labels)\n",
    "val_dataset = dataset_to_span_classification_format(val_dataset, allowed_labels)\n",
    "test_dataset = dataset_to_span_classification_format(test_dataset, allowed_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9108df77",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(set(train_dataset[\"label\"]))\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0ed772",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {label: i for i, label in enumerate(sorted(set(train_dataset[\"label\"])))}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c856aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply label2id mapping to labels\n",
    "def apply_label_mapping(x, mapping):\n",
    "    x['label'] = mapping[x['label']]\n",
    "    return x\n",
    "train_dataset= train_dataset.map(lambda x: apply_label_mapping(x, label2id), batched=False)\n",
    "val_dataset= val_dataset.map(lambda x: apply_label_mapping(x, label2id), batched=False)\n",
    "test_dataset= test_dataset.map(lambda x: apply_label_mapping(x, label2id), batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694a9c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = compute_class_weights(train_dataset['label'])\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ae7159",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.trainer_utils import PredictionOutput\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def compute_metrics(p: PredictionOutput):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    labels = p.label_ids\n",
    "    acc = np.sum(preds == labels) / len(labels)\n",
    "    f1 = f1_score(labels, preds, average=\"macro\")\n",
    "    return {\"accuracy\": acc, \"macro_f1\": f1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24875592",
   "metadata": {},
   "source": [
    "### using the `SetfitModel`'s `fit()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2000e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from setfit.modeling import SetFitHead\n",
    "from src.finetuning.setfit_extensions.class_weights_head import SetFitHeadWithClassWeights\n",
    "from src.finetuning.setfit_extensions.span_embedding import (\n",
    "    SetFitModelForSpanClassification,\n",
    "    SentenceTransformerForSpanEmbedding,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d25a9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: needto manually convert to tuple for span, as datasets library converts tuples to lists\n",
    "def dataset_to_xy_inputs(dataset):\n",
    "    X = [(example['text'], tuple(example['span'])) for example in dataset]  # NOTE: need to convert span to tuple\n",
    "    y = dataset['label'] # [label2id[example['label']] for example in dataset]\n",
    "    return X, y\n",
    "\n",
    "x_train, y_train = dataset_to_xy_inputs(train_dataset)\n",
    "x_eval, y_eval = dataset_to_xy_inputs(val_dataset)\n",
    "x_test, y_test = dataset_to_xy_inputs(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29e5bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42, deterministic=True)\n",
    "\n",
    "model_id = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "body = SentenceTransformerForSpanEmbedding(model_id, model_kwargs={\"device_map\": \"auto\"})\n",
    "\n",
    "head = SetFitHeadWithClassWeights(\n",
    "    in_features=body.get_sentence_embedding_dimension(),\n",
    "    out_features=num_classes,\n",
    "    device=body.device,\n",
    "    class_weights=class_weights\n",
    ")\n",
    "\n",
    "model = SetFitModelForSpanClassification(\n",
    "    model_body=body,\n",
    "    model_head=head,\n",
    "    normalize_embeddings=True,\n",
    ")\n",
    "model.to(body.device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a1fbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from setfit import TrainingArguments\n",
    "# from src.finetuning.setfit_extensions.span_embedding import SetFitTrainerForSpanClassification\n",
    "args = TrainingArguments()\n",
    "args.max_length = body.tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c8143e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    x_train=x_train, y_train=y_train,\n",
    "    x_eval=x_eval, y_eval=y_eval,\n",
    "    \n",
    "    num_epochs=15,\n",
    "    batch_size=16,\n",
    "    body_learning_rate=args.body_classifier_learning_rate,\n",
    "    head_learning_rate=args.head_learning_rate,\n",
    "    l2_weight=args.l2_weight,\n",
    "    \n",
    "    max_length=body.tokenizer.model_max_length,\n",
    "    \n",
    "    show_progress_bar=True,\n",
    "    end_to_end=True, # !!! fine-tune also the body (important for adapting sentence embedding model for span embedding)\n",
    "    \n",
    "    # added early stopping arguments\n",
    "    compute_metrics=compute_metrics,\n",
    "    metric_for_best_model=\"macro_f1\", # NOTE: must match one of the keys returned by `compute_metrics`\n",
    "    early_stopping_patience=3,\n",
    "    early_stopping_threshold=0.02,\n",
    "    greater_is_better=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922e00b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(inputs=x_test, as_numpy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f6736e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify that it accepts different input formats\n",
    "y_pred2 = model.predict(texts=test_dataset['text'], spans=test_dataset['span'], as_numpy=True)\n",
    "\n",
    "span_texts = [ex['text'][slice(*ex['span'])] for ex in test_dataset]\n",
    "y_pred3 = model.predict(texts=test_dataset['text'], span_texts=span_texts, as_numpy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737e779c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all(y_pred == y_pred2), all(y_pred == y_pred3), all(y_pred2 == y_pred3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d193685",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred, target_names=[id2label[i] for i in range(num_classes)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ee583a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# free GPU\n",
    "model.to(\"cpu\");\n",
    "del model\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "# print(torch.cuda.memory_summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea8a983",
   "metadata": {},
   "source": [
    "### with custom early-stopping trainer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b585248",
   "metadata": {},
   "outputs": [],
   "source": [
    "from setfit.modeling import SetFitHead\n",
    "from src.finetuning.setfit_extensions.class_weights_head import SetFitHeadWithClassWeights\n",
    "from src.finetuning.setfit_extensions.span_embedding import (\n",
    "    SentenceTransformerForSpanEmbedding,\n",
    "    SetFitModelForSpanClassification,\n",
    "    SetFitTrainerForSpanClassification,\n",
    ")\n",
    "from src.finetuning.setfit_extensions.early_stopping import (\n",
    "    EarlyStoppingTrainingArguments,\n",
    "    EarlyStoppingCallback,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e531d162",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init(\n",
    "        model_name: str=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        num_classes: int=2, \n",
    "        class_weights: np._typing.NDArray=None,\n",
    "        **kwargs\n",
    "    ) -> SetFitModelForSpanClassification:\n",
    "    \n",
    "    model_kwargs={\"device_map\": \"auto\", **kwargs}\n",
    "    body = SentenceTransformerForSpanEmbedding(model_name, model_kwargs=model_kwargs, trust_remote_code=True)\n",
    "    \n",
    "    # TODO: support multi-label classification\n",
    "    head_kwargs = dict(\n",
    "        in_features=body.get_sentence_embedding_dimension(),\n",
    "        out_features=num_classes,\n",
    "        device=body.device,\n",
    "    )\n",
    "    if class_weights is not None:\n",
    "        head_kwargs['class_weights'] = class_weights\n",
    "        head = SetFitHeadWithClassWeights(**head_kwargs)\n",
    "    else:\n",
    "        head = SetFitHead(**head_kwargs)\n",
    "    \n",
    "    return SetFitModelForSpanClassification(\n",
    "        model_body=body,\n",
    "        model_head=head.to(body.device),\n",
    "        normalize_embeddings=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed451fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"sentence-transformers/all-MiniLM-L6-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e53afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = EarlyStoppingTrainingArguments(\n",
    "    num_epochs=(1, 15),\n",
    "    # sentence transformer (embedding) finetuning arts\n",
    "    eval_strategy=\"steps\", # NOTE: currently no effect on (early stopping in) classification head training\n",
    "    eval_steps=25, # NOTE: overwrites 0 epochs above for sentence transformer finetuning\n",
    "    max_steps=50,\n",
    "    eval_max_steps=200,\n",
    "    # early stopping config\n",
    "    metric_for_best_model=(\"embedding_loss\", \"f1\"),\n",
    "    greater_is_better=(False, True),\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=2, # NOTE: currently no effect on (early stopping in) classification head training\n",
    "    # misc\n",
    "    end_to_end=True,\n",
    ")\n",
    "\n",
    "training_callbacks = [\n",
    "    EarlyStoppingCallback(early_stopping_patience=2, early_stopping_threshold=0.03), # for sentence transformer finetuning\n",
    "    EarlyStoppingCallback(early_stopping_patience=4, early_stopping_threshold=0.02), # for classifier finetuning\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69f9ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize Trainer\n",
    "trainer = SetFitTrainerForSpanClassification(\n",
    "    model_init=lambda : model_init(\n",
    "        model_name=model_id,\n",
    "        num_classes=num_classes,\n",
    "        class_weights=class_weights,\n",
    "    ),\n",
    "    metric=\"f1\",\n",
    "    metric_kwargs={\"average\": \"macro\"},\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    callbacks=training_callbacks,\n",
    "    # compute_metrics=compute_metrics,\n",
    ")\n",
    "# fix max_length issue\n",
    "trainer._args.max_length = trainer.st_trainer.model.tokenizer.model_max_length\n",
    "\n",
    "# set seeds for reproducibility\n",
    "trainer._args.seed = 42\n",
    "trainer.st_trainer.args.seed = 42\n",
    "trainer.st_trainer.args.data_seed = 42\n",
    "trainer.st_trainer.args.full_determinism = True\n",
    "\n",
    "# don't report to wandb or other experiment trackers\n",
    "trainer._args.report_to = 'none'\n",
    "trainer.st_trainer.args.report_to = 'none'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d290afcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a30316d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify best model loaded\n",
    "probs = trainer.model.predict_proba(texts=val_dataset['text'], spans=val_dataset['span'], as_numpy=True)\n",
    "preds = probs.argmax(axis=1)\n",
    "p = PredictionOutput(predictions=probs, label_ids=np.array(val_dataset['label']), metrics={})\n",
    "compute_metrics(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f86b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate on test set\n",
    "probs = trainer.model.predict_proba(texts=test_dataset['text'], spans=test_dataset['span'], as_numpy=True)\n",
    "preds = probs.argmax(axis=1)\n",
    "p = PredictionOutput(predictions=probs, label_ids=np.array(test_dataset['label']), metrics={})\n",
    "compute_metrics(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c777e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "preds = trainer.model.predict(texts=test_dataset['text'], spans=test_dataset['span'], as_numpy=True)\n",
    "print(classification_report(test_dataset['label'], preds, target_names=list(label2id.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1ad96c",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# NOTE: only shows F1 because passing metric=\"f1\" to Trainer\n",
    "trainer.evaluate(test_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "galtan_group_appeals",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
